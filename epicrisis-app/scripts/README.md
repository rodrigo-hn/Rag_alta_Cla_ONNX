# Scripts de An√°lisis de M√©tricas

Scripts para extraer y comparar m√©tricas de performance de modelos LLM.

---

## üìã Scripts Disponibles

### 1. `extract_metrics.sh`

Extrae m√©tricas de un archivo de log.

**Uso:**
```bash
./scripts/extract_metrics.sh [ruta_al_log]
```

**Ejemplos:**
```bash
# Analizar log de hoy
./scripts/extract_metrics.sh backend/logs/flow-$(date +%Y-%m-%d).log

# Analizar log espec√≠fico
./scripts/extract_metrics.sh backend/logs/flow-2025-12-29.log

# Sin par√°metro: usa log de hoy por defecto
./scripts/extract_metrics.sh
```

**M√©tricas extra√≠das:**
- Total de generaciones
- Tokens por segundo (promedio, m√≠nimo, m√°ximo, mediana)
- Tiempo de inferencia (promedio, m√≠nimo, m√°ximo)
- Tiempo total (promedio)
- Tokens procesados (input, output, total)
- M√©tricas RAG (si existen)

---

### 2. `compare_models.sh`

Compara m√©tricas entre dos modelos diferentes.

**Uso:**
```bash
./scripts/compare_models.sh <log_modelo_1> <log_modelo_2>
```

**Ejemplo:**
```bash
# Comparar TinyLlama vs Llama-3
./scripts/compare_models.sh \
  backend/logs/tinyllama_results.log \
  backend/logs/llama3_results.log
```

**M√©tricas comparadas:**
- Tokens por segundo
- Tiempo de inferencia
- Tiempo total
- Diferencia porcentual entre modelos

---

## üöÄ Flujo de Trabajo Recomendado

### 1. Ejecutar Benchmarks

```bash
# Preparar datos de prueba
cat > test_data.json <<EOF
{
  "clinicalData": {
    "motivo_ingreso": "Dolor tor√°cico",
    "diagnostico_ingreso": [
      {"codigo": "I20.0", "nombre": "Angina inestable"}
    ],
    ...
  }
}
EOF

# Ejecutar 10 requests con Modelo A
for i in {1..10}; do
  curl -X POST http://localhost:3000/api/generate-epicrisis \
    -H "Content-Type: application/json" \
    -d @test_data.json
  sleep 3
done

# Guardar logs en archivo espec√≠fico
cp backend/logs/flow-$(date +%Y-%m-%d).log backend/logs/model_a.log
```

### 2. Cambiar Modelo

```bash
# Actualizar configuraci√≥n para usar Modelo B
# Editar backend/.env o backend/src/services/llmService.ts

# Reiniciar servidor
cd backend && npm run dev
```

### 3. Ejecutar Benchmarks con Modelo B

```bash
# Repetir 10 requests
for i in {1..10}; do
  curl -X POST http://localhost:3000/api/generate-epicrisis \
    -H "Content-Type: application/json" \
    -d @test_data.json
  sleep 3
done

# Guardar logs
cp backend/logs/flow-$(date +%Y-%m-%d).log backend/logs/model_b.log
```

### 4. Comparar Resultados

```bash
# Extraer m√©tricas individuales
./scripts/extract_metrics.sh backend/logs/model_a.log > model_a_metrics.txt
./scripts/extract_metrics.sh backend/logs/model_b.log > model_b_metrics.txt

# Comparar lado a lado
./scripts/compare_models.sh backend/logs/model_a.log backend/logs/model_b.log
```

---

## üìä Ejemplo de Salida

### extract_metrics.sh

```
================================================
üìä M√âTRICAS DE PERFORMANCE
================================================
Archivo: backend/logs/flow-2025-12-29.log
Fecha: Sun Dec 29 16:45:23 CLT 2025

Total de generaciones: 10

--- TOKENS POR SEGUNDO ---
  Promedio: 62.35 t/s
  M√≠nimo: 58.12 t/s
  M√°ximo: 67.89 t/s
  Mediana: 61.45 t/s

--- TIEMPO DE INFERENCIA ---
  Promedio: 2145ms
  M√≠nimo: 1987ms
  M√°ximo: 2301ms

--- TIEMPO TOTAL ---
  Promedio: 2153ms

--- TOKENS PROCESADOS ---
  Promedio input: 864 tokens
  Promedio output: 128 tokens
  Promedio total: 992 tokens

================================================
‚úÖ An√°lisis completado
================================================
```

### compare_models.sh

```
================================================
üî¨ COMPARACI√ìN DE MODELOS LLM
================================================

Modelo 1: backend/logs/tinyllama.log
Modelo 2: backend/logs/llama3.log
Fecha: Sun Dec 29 16:50:15 CLT 2025

--- TOKENS POR SEGUNDO ---
Modelo               Promedio    M√≠nimo    M√°ximo   Samples
-------------------- ---------- ---------- ---------- ----------
Modelo 1               62.35 t/s  58.12 t/s  67.89 t/s         10
Modelo 2               38.67 t/s  35.21 t/s  42.11 t/s         10
üìä Modelo 1 es 61.24% m√°s r√°pido

--- TIEMPO DE INFERENCIA ---
Modelo               Promedio    M√≠nimo    M√°ximo   Samples
-------------------- ---------- ---------- ---------- ----------
Modelo 1              2145ms     1987ms     2301ms         10
Modelo 2              3456ms     3201ms     3789ms         10

--- TIEMPO TOTAL ---
Modelo               Promedio    M√≠nimo    M√°ximo   Samples
-------------------- ---------- ---------- ---------- ----------
Modelo 1              2153ms     1995ms     2310ms         10
Modelo 2              3468ms     3215ms     3801ms         10

================================================
‚úÖ Comparaci√≥n completada
================================================
```

---

## üîß Troubleshooting

### Error: "division by zero"

Esto ocurre cuando no hay datos en el log. Soluciones:

```bash
# Verificar que el archivo existe y tiene contenido
ls -lh backend/logs/flow-*.log

# Ver si hay m√©tricas
grep "LLM_METRICS" backend/logs/flow-*.log

# Ejecutar al menos una generaci√≥n primero
curl -X POST http://localhost:3000/api/generate-epicrisis \
  -H "Content-Type: application/json" \
  -d @test_data.json
```

### Error: "Archivo no encontrado"

```bash
# Crear directorio de logs si no existe
mkdir -p backend/logs

# Verificar que el servidor est√° corriendo
ps aux | grep node
```

### Los scripts no tienen permisos de ejecuci√≥n

```bash
chmod +x scripts/*.sh
```

---

## üìù Personalizaci√≥n

### Agregar nuevas m√©tricas

Editar `extract_metrics.sh` y agregar:

```bash
# Nueva m√©trica: Longitud de output
echo "--- LONGITUD DE OUTPUT ---"
grep "output_length" "$LOG_FILE" | \
  grep -o '"output_length":[0-9]*' | \
  cut -d: -f2 | \
  awk '{sum+=$1; count++} END {if(count>0) print "  Promedio: " sum/count " caracteres"}'
```

### Exportar a CSV

```bash
# Modificar extract_metrics.sh para generar CSV
echo "timestamp,tokens_per_second,inference_ms,total_ms" > metrics.csv

grep "GENERACI√ìN COMPLETADA" backend/logs/flow-*.log | \
  while read line; do
    # Extraer campos y escribir a CSV
    echo "$timestamp,$tps,$inf,$total" >> metrics.csv
  done
```

---

## üéØ Mejores Pr√°cticas

1. **Consistencia**: Usar los mismos datos de prueba para todos los modelos
2. **Warmup**: Ejecutar 2-3 requests de calentamiento antes de medir
3. **Samples**: Usar al menos 10 samples por modelo
4. **Condiciones**: Misma m√°quina, mismo estado del sistema
5. **Aislamiento**: Cerrar otras aplicaciones durante benchmarks
6. **Documentar**: Registrar versi√≥n de modelo, configuraci√≥n, hardware

---

## üìö Referencias

- `LLM_PERFORMANCE_METRICS.md` - Gu√≠a completa de m√©tricas
- `PERFORMANCE_LOGGING_README.md` - Resumen del sistema
- `LOGGING_SYSTEM.md` - Sistema general de logging

---

**√öltima actualizaci√≥n:** 2025-12-29
