# ============================================
# CONFIGURACION GLOBAL - EPICRISIS APP
# ============================================
# Este archivo documenta las variables de entorno disponibles
# para configurar la aplicacion en modo remoto o local.
#
# MODO REMOTO: Usa el backend Node.js con modelos GGUF
# MODO LOCAL:  Usa modelos ONNX directamente en el navegador
#
# ============================================

# ============================================
# MODO DE OPERACION
# ============================================

# Modo principal de la aplicacion:
# - 'remote': Backend API con modelos GGUF (default)
# - 'local': Modelos ONNX en navegador (100% privado)
GENERATION_MODE=remote

# ============================================
# CONFIGURACION MODO REMOTO (Backend)
# ============================================
# Ver backend/.env.example para configuracion completa

# Puerto del servidor backend
PORT=3000

# Tipo de modelo en backend: local, openai, anthropic
MODEL_TYPE=local

# Ruta al modelo LLM local (GGUF)
LLM_MODEL_PATH=./models/llm/tinyllama-1.1b-chat-q4/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# ============================================
# CONFIGURACION MODO LOCAL (Frontend ONNX)
# ============================================
# Ver frontend/.env.example para configuracion completa

# Modelo LLM ONNX por defecto
# Opciones:
# - onnx-community/Llama-3.2-1B-Instruct (1.1GB, recomendado)
# - onnx-community/Phi-3.5-mini-instruct-onnx-web (2.2GB)
# - onnx-community/Ministral-3B-Instruct-2412-ONNX (2.4GB)
# - HuggingFaceTB/SmolLM2-360M-Instruct (200MB, rapido)
DEFAULT_LOCAL_MODEL=onnx-community/Llama-3.2-1B-Instruct

# Cargar modelos automaticamente al iniciar
AUTO_LOAD_MODELS=false

# Preferir WebGPU sobre WASM
PREFER_WEBGPU=true

# ============================================
# CONFIGURACION DE GENERACION
# ============================================

# Preset de generacion:
# - resumen_alta: Epicrisis (temp=0.1, tokens=600)
# - rag: Preguntas RAG (temp=0.2, tokens=512)
# - extraction: Extraccion precisa (temp=0.1, tokens=256)
GENERATION_CONFIG=resumen_alta

# Override de parametros
MAX_NEW_TOKENS=600
TEMPERATURE=0.1

# ============================================
# DEBUG
# ============================================

DEBUG_MODE=true

# ============================================
# COMPARACION DE MODOS
# ============================================
#
# | Caracteristica      | REMOTO (Backend)    | LOCAL (ONNX Browser) |
# |---------------------|---------------------|----------------------|
# | Privacidad          | Datos en servidor   | 100% en navegador    |
# | Velocidad           | Depende de HW       | Depende de GPU/CPU   |
# | Modelos soportados  | GGUF (7B+)          | ONNX (0.5B-3B)       |
# | Calidad             | Mayor (modelos +)   | Menor (modelos -)    |
# | Requisitos          | Node.js + modelos   | Solo navegador       |
# | Offline             | Si (con modelos)    | Si (despues de cache)|
#
# ============================================
# RECOMENDACIONES
# ============================================
#
# DESARROLLO:
#   GENERATION_MODE=remote
#   MODEL_TYPE=local
#   LLM_MODEL_PATH=./models/llm/tinyllama (rapido)
#
# PRODUCCION SERVIDOR:
#   GENERATION_MODE=remote
#   MODEL_TYPE=local
#   LLM_MODEL_PATH=./models/llm/mistral-7b (calidad)
#
# PRIVACIDAD MAXIMA:
#   GENERATION_MODE=local
#   DEFAULT_LOCAL_MODEL=onnx-community/Llama-3.2-1B-Instruct
#
# DEMO/PRUEBAS RAPIDAS:
#   GENERATION_MODE=local
#   DEFAULT_LOCAL_MODEL=HuggingFaceTB/SmolLM2-360M-Instruct
#
