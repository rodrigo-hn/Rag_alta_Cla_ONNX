{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-0.5B para Epicrisis\n",
    "\n",
    "Este notebook realiza fine-tuning del modelo Qwen2.5-0.5B-Instruct para generar epicrisis medicas.\n",
    "\n",
    "**Requisitos:**\n",
    "- GPU T4 o superior (disponible en Colab gratuito)\n",
    "- ~8GB VRAM\n",
    "\n",
    "**Dataset:**\n",
    "- ~1200 ejemplos de epicrisis en formato ChatML\n",
    "- 90% train / 10% validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuracion del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU disponible\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl wandb\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport json\nimport os\nfrom pathlib import Path\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuracion del modelo y entrenamiento\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\nOUTPUT_DIR = \"./epicrisis-finetuned\"\nDATASET_DIR = \"./datasets\"\n\n# Hiperparametros CONSERVADORES para evitar corromper el modelo\n# El problema anterior fue que el modelo aprendio a generar prompts/JSON\n# en lugar de solo respuestas narrativas\n\nEPOCHS = 3  # 3 epochs es razonable con DataCollator correcto\nBATCH_SIZE = 2\nGRADIENT_ACCUMULATION = 4  # Effective batch size = 8\nLEARNING_RATE = 2e-5  # Standard para LoRA fine-tuning\nMAX_SEQ_LENGTH = 1024\n\n# LoRA - configuracion conservadora\nLORA_RANK = 16  # r=16 es bueno para tareas de generacion\nLORA_ALPHA = 32  # alpha = 2*r es una buena regla\nLORA_DROPOUT = 0.05\n\n# System instruction (igual que en la app)\nSYSTEM_INSTRUCTION = (\n    \"Genera una epicrisis narrativa en UN SOLO PARRAFO. \"\n    \"USA SOLO la informacion del JSON, NO inventes datos. \"\n    \"IMPORTANTE: Incluye TODOS los codigos entre parentesis: \"\n    \"diagnostico de ingreso con codigo CIE-10 (ej: I20.0), \"\n    \"procedimientos con codigo K (ej: K492, K493), \"\n    \"medicacion con dosis y codigo ATC (ej: B01AC06). \"\n    \"Estructura: dx ingreso -> procedimientos -> evolucion -> dx alta -> medicacion alta. \"\n    \"Abreviaturas: DA=descendente anterior, CD=coronaria derecha, CX=circunfleja, \"\n    \"SDST=supradesnivel ST, IAM=infarto agudo miocardio.\"\n)\n\nprint(\"=\"*60)\nprint(\"CONFIGURACION DE FINE-TUNING\")\nprint(\"=\"*60)\nprint(f\"  Modelo: {MODEL_NAME}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Max seq length: {MAX_SEQ_LENGTH}\")\nprint(f\"  LoRA rank: {LORA_RANK}\")\nprint(f\"  LoRA alpha: {LORA_ALPHA}\")\nprint(f\"  LoRA dropout: {LORA_DROPOUT}\")\nprint(\"=\"*60)\nprint(\"\\nNOTA: Se usara DataCollatorForCompletionOnlyLM para\")\nprint(\"      entrenar SOLO en las respuestas del assistant.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subir y preparar datasets\n",
    "\n",
    "Sube los archivos JSONL del dataset o ejecuta la celda siguiente para crear datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Subir archivos del dataset unificado\n# Sube los archivos train.jsonl y validation.jsonl de la carpeta unified_data/\nfrom google.colab import files\n\nos.makedirs(DATASET_DIR, exist_ok=True)\n\nprint(\"=\"*60)\nprint(\"IMPORTANTE: Sube los archivos del dataset unificado:\")\nprint(\"  - unified_data/train.jsonl (1071 ejemplos)\")\nprint(\"  - unified_data/validation.jsonl (120 ejemplos)\")\nprint(\"=\"*60)\nprint()\n\nuploaded = files.upload()\n\nfor filename, content in uploaded.items():\n    # Guardar en el directorio de datasets\n    filepath = f\"{DATASET_DIR}/{filename}\"\n    with open(filepath, \"wb\") as f:\n        f.write(content)\n    \n    # Contar lineas\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        lines = sum(1 for _ in f)\n    \n    print(f\"Guardado: {filepath} ({lines} ejemplos)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificar archivos subidos\nprint(\"Archivos en el directorio de datasets:\")\nfor f in Path(DATASET_DIR).glob(\"*.jsonl\"):\n    with open(f, \"r\", encoding=\"utf-8\") as file:\n        lines = sum(1 for _ in file)\n    print(f\"  - {f.name}: {lines} ejemplos\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargar datasets unificados\n# Los archivos ya estan en formato ChatML (campo \"text\")\n\ndef load_unified_datasets(dataset_dir):\n    \"\"\"\n    Carga los datasets unificados (train.jsonl y validation.jsonl).\n    Estos archivos ya tienen el formato ChatML en el campo \"text\".\n    \"\"\"\n    train_path = Path(dataset_dir) / \"train.jsonl\"\n    valid_path = Path(dataset_dir) / \"validation.jsonl\"\n    \n    train_examples = []\n    valid_examples = []\n    \n    # Cargar train\n    if train_path.exists():\n        with open(train_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                if line.strip():\n                    example = json.loads(line)\n                    train_examples.append(example)\n        print(f\"Train: {len(train_examples)} ejemplos\")\n    else:\n        raise FileNotFoundError(f\"No se encontro {train_path}\")\n    \n    # Cargar validation\n    if valid_path.exists():\n        with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                if line.strip():\n                    example = json.loads(line)\n                    valid_examples.append(example)\n        print(f\"Validation: {len(valid_examples)} ejemplos\")\n    else:\n        raise FileNotFoundError(f\"No se encontro {valid_path}\")\n    \n    # Crear DatasetDict\n    dataset = DatasetDict({\n        \"train\": Dataset.from_list(train_examples),\n        \"validation\": Dataset.from_list(valid_examples)\n    })\n    \n    return dataset\n\n# Cargar datasets\ndataset = load_unified_datasets(DATASET_DIR)\nprint(f\"\\nDataset cargado: {dataset}\")\n\n# Verificar que tiene el campo \"text\"\nprint(f\"\\nCampos disponibles: {dataset['train'].column_names}\")\nif \"text\" in dataset['train'].column_names:\n    print(\"✓ Campo 'text' encontrado (formato ChatML)\")\nelse:\n    print(\"✗ ERROR: No se encontro el campo 'text'\")\n    \n# IMPORTANTE: Verificar longitud de los ejemplos\nlengths = [len(ex[\"text\"]) for ex in dataset[\"train\"]]\nprint(f\"\\nEstadisticas de longitud (caracteres):\")\nprint(f\"  Min: {min(lengths)}\")\nprint(f\"  Max: {max(lengths)}\")\nprint(f\"  Promedio: {sum(lengths)/len(lengths):.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ver ejemplos del dataset para verificar el formato\nprint(\"=\"*60)\nprint(\"VERIFICANDO FORMATO DEL DATASET\")\nprint(\"=\"*60)\n\n# Ver el primer ejemplo completo\nexample = dataset[\"train\"][0][\"text\"]\nprint(\"\\nEjemplo 1 (completo):\")\nprint(\"-\"*60)\nprint(example)\nprint(\"-\"*60)\n\n# Verificar que el formato es correcto\nprint(\"\\n\\nVerificaciones:\")\nprint(f\"1. Contiene '<|im_start|>system': {'<|im_start|>system' in example}\")\nprint(f\"2. Contiene '<|im_start|>user': {'<|im_start|>user' in example}\")\nprint(f\"3. Contiene '<|im_start|>assistant': {'<|im_start|>assistant' in example}\")\nprint(f\"4. Contiene '<|im_end|>': {'<|im_end|>' in example}\")\n\n# Contar tokens especiales\nprint(f\"\\n5. Numero de '<|im_start|>': {example.count('<|im_start|>')}\")\nprint(f\"6. Numero de '<|im_end|>': {example.count('<|im_end|>')}\")\n\n# Extraer y mostrar el output del assistant\nif \"<|im_start|>assistant\\n\" in example:\n    assistant_part = example.split(\"<|im_start|>assistant\\n\")[1]\n    if \"<|im_end|>\" in assistant_part:\n        assistant_text = assistant_part.split(\"<|im_end|}\")[0]\n    else:\n        assistant_text = assistant_part\n    print(f\"\\n7. Output del assistant (primeros 500 chars):\")\n    print(\"-\"*60)\n    print(assistant_text[:500])\n\n# Verificar todos los ejemplos tienen el formato correcto\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFICANDO TODOS LOS EJEMPLOS\")\nprint(\"=\"*60)\n\nerrors = 0\nfor i, ex in enumerate(dataset[\"train\"]):\n    text = ex[\"text\"]\n    if \"<|im_start|>assistant\\n\" not in text:\n        print(f\"  ERROR en ejemplo {i}: falta '<|im_start|>assistant\\\\n'\")\n        errors += 1\n    if not text.strip().endswith(\"<|im_end|>\"):\n        print(f\"  ERROR en ejemplo {i}: no termina con '<|im_end|>'\")\n        errors += 1\n\nif errors == 0:\n    print(f\"✓ Todos los {len(dataset['train'])} ejemplos tienen formato correcto\")\nelse:\n    print(f\"✗ Se encontraron {errors} errores\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datasets unificados para referencia\n",
    "dataset[\"train\"].to_json(f\"{DATASET_DIR}/unified_train.jsonl\")\n",
    "dataset[\"validation\"].to_json(f\"{DATASET_DIR}/unified_validation.jsonl\")\n",
    "print(\"Datasets unificados guardados en:\")\n",
    "print(f\"  - {DATASET_DIR}/unified_train.jsonl\")\n",
    "print(f\"  - {DATASET_DIR}/unified_validation.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cargar modelo y tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuracion de cuantizacion (4-bit para ahorrar memoria)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Cargar tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# IMPORTANTE: Configurar pad_token correctamente para Qwen\n# Qwen usa <|endoftext|> como eos_token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Verificar tokens especiales de ChatML\nprint(f\"Tokenizer cargado: {MODEL_NAME}\")\nprint(f\"Vocab size: {tokenizer.vocab_size}\")\nprint(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\nprint(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n\n# Verificar que los tokens de ChatML existen\nchatml_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\nprint(\"\\nTokens ChatML:\")\nfor token in chatml_tokens:\n    token_id = tokenizer.convert_tokens_to_ids(token)\n    if token_id == tokenizer.unk_token_id:\n        print(f\"  {token}: ⚠️ NO EXISTE (mapped to UNK)\")\n    else:\n        print(f\"  {token}: ID {token_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Preparar para entrenamiento con cuantizacion\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Modelo cargado: {MODEL_NAME}\")\n",
    "print(f\"Parametros: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configurar LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Mostrar parametros entrenables\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configurar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuracion de entrenamiento\n# Los hiperparametros se configuran en SFTConfig en la siguiente celda\n\nprint(\"Configuracion de entrenamiento:\")\nprint(f\"  - Epochs: {EPOCHS}\")\nprint(f\"  - Batch size: {BATCH_SIZE}\")\nprint(f\"  - Gradient accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"  - Learning rate: {LEARNING_RATE}\")\nprint(f\"  - Max sequence length: {MAX_SEQ_LENGTH}\")\nprint(f\"  - LoRA rank: {LORA_RANK}\")\nprint(f\"  - LoRA alpha: {LORA_ALPHA}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Crear trainer para TRL >= 0.27.0\nimport trl\nprint(f\"TRL version: {trl.__version__}\")\n\nfrom trl import SFTConfig, SFTTrainer\n\n# Buscar DataCollatorForCompletionOnlyLM en diferentes ubicaciones de TRL\nDataCollatorForCompletionOnlyLM = None\n\n# Intentar diferentes ubicaciones\ntry:\n    from trl import DataCollatorForCompletionOnlyLM\n    print(\"Importado desde trl\")\nexcept ImportError:\n    pass\n\nif DataCollatorForCompletionOnlyLM is None:\n    try:\n        from trl.trainer import DataCollatorForCompletionOnlyLM\n        print(\"Importado desde trl.trainer\")\n    except ImportError:\n        pass\n\nif DataCollatorForCompletionOnlyLM is None:\n    try:\n        from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n        print(\"Importado desde trl.trainer.utils\")\n    except ImportError:\n        pass\n\nif DataCollatorForCompletionOnlyLM is None:\n    try:\n        from trl.data_utils import DataCollatorForCompletionOnlyLM\n        print(\"Importado desde trl.data_utils\")\n    except ImportError:\n        pass\n\n# Si no encontramos el collator, implementamos uno simple\nif DataCollatorForCompletionOnlyLM is None:\n    print(\"DataCollatorForCompletionOnlyLM no disponible en TRL 0.27.0\")\n    print(\"Usando implementacion alternativa con completion_only_collator...\")\n    \n    from transformers import DataCollatorForLanguageModeling\n    from dataclasses import dataclass\n    from typing import Any, Dict, List\n    import torch\n    \n    @dataclass\n    class CompletionOnlyDataCollator:\n        \"\"\"\n        Data collator que solo calcula loss en la parte del assistant.\n        \"\"\"\n        tokenizer: Any\n        response_template: str = \"<|im_start|>assistant\\n\"\n        mlm: bool = False\n        \n        def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n            # Tokenizar si es necesario\n            if isinstance(examples[0], dict) and \"text\" in examples[0]:\n                texts = [ex[\"text\"] for ex in examples]\n                batch = self.tokenizer(\n                    texts,\n                    padding=True,\n                    truncation=True,\n                    max_length=1024,\n                    return_tensors=\"pt\",\n                )\n            else:\n                batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n            \n            # Crear labels (copiar input_ids)\n            labels = batch[\"input_ids\"].clone()\n            \n            # Tokenizar el response_template\n            response_token_ids = self.tokenizer.encode(\n                self.response_template, \n                add_special_tokens=False\n            )\n            \n            # Para cada ejemplo, enmascarar todo antes del response_template\n            for i in range(labels.shape[0]):\n                input_ids = batch[\"input_ids\"][i].tolist()\n                \n                # Buscar donde empieza la respuesta del assistant\n                response_start = None\n                for j in range(len(input_ids) - len(response_token_ids) + 1):\n                    if input_ids[j:j+len(response_token_ids)] == response_token_ids:\n                        response_start = j + len(response_token_ids)\n                        break\n                \n                # Si encontramos el template, enmascarar todo antes\n                if response_start is not None:\n                    labels[i, :response_start] = -100\n                \n                # Tambien enmascarar padding\n                labels[i, batch[\"attention_mask\"][i] == 0] = -100\n            \n            batch[\"labels\"] = labels\n            return batch\n    \n    DataCollatorForCompletionOnlyLM = CompletionOnlyDataCollator\n\n# Crear el collator\nresponse_template = \"<|im_start|>assistant\\n\"\ncollator = DataCollatorForCompletionOnlyLM(\n    tokenizer=tokenizer,\n    response_template=response_template,\n)\n\nprint(f\"\\nResponse template: '{response_template}'\")\nprint(f\"Response template tokens: {tokenizer.encode(response_template, add_special_tokens=False)}\")\n\nsft_config = SFTConfig(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    eval_strategy=\"steps\",\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    max_grad_norm=0.5,\n    # Parametros especificos de SFT (TRL 0.27+)\n    max_length=MAX_SEQ_LENGTH,\n    dataset_text_field=\"text\",\n    packing=False,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    processing_class=tokenizer,\n    data_collator=collator,\n)\n\nprint(\"\\nTrainer configurado\")\nprint(\"  - Solo entrena en las respuestas del assistant\")\nprint(\"  - Evita que el modelo 'aprenda' a generar prompts\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo final\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
    "\n",
    "print(f\"Modelo guardado en: {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Probar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Probar generacion\nfrom transformers import pipeline\n\ndef generate_epicrisis(model, tokenizer, input_data, max_new_tokens=512):\n    \"\"\"\n    Genera una epicrisis dado un input JSON.\n    \"\"\"\n    json_str = json.dumps(input_data, ensure_ascii=False, indent=2)\n    \n    prompt = (\n        f\"<|im_start|>system\\n{SYSTEM_INSTRUCTION}<|im_end|>\\n\"\n        f\"<|im_start|>user\\n{json_str}<|im_end|>\\n\"\n        f\"<|im_start|>assistant\\n\"\n    )\n    \n    print(f\"Prompt length: {len(prompt)} chars\")\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n    \n    # Tokens especiales de Qwen para terminar generacion\n    eos_token_id = tokenizer.eos_token_id\n    im_end_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n    endoftext_id = tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n    \n    stop_ids = [eos_token_id]\n    if im_end_id and im_end_id != tokenizer.unk_token_id:\n        stop_ids.append(im_end_id)\n    if endoftext_id and endoftext_id != tokenizer.unk_token_id:\n        stop_ids.append(endoftext_id)\n    \n    print(f\"EOS token ID: {eos_token_id}\")\n    print(f\"im_end token ID: {im_end_id}\")\n    print(f\"Stop IDs: {stop_ids}\")\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_new_tokens=max_new_tokens,\n            min_new_tokens=100,  # Forzar minimo de tokens\n            temperature=0.7,\n            top_p=0.9,\n            top_k=50,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=stop_ids,\n            repetition_penalty=1.15,\n            no_repeat_ngram_size=3,\n        )\n    \n    # Decodificar solo los tokens nuevos\n    input_length = inputs['input_ids'].shape[1]\n    generated_tokens = outputs[0][input_length:]\n    print(f\"Generated {len(generated_tokens)} new tokens\")\n    \n    # Decodificar\n    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    print(f\"Decoded response length: {len(response)} chars\")\n    \n    return response.strip()\n\n\ndef generate_with_pipeline(model, tokenizer, input_data):\n    \"\"\"Genera usando pipeline de transformers.\"\"\"\n    json_str = json.dumps(input_data, ensure_ascii=False, indent=2)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTION},\n        {\"role\": \"user\", \"content\": json_str}\n    ]\n    \n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True,\n        return_full_text=False,\n    )\n    \n    result = pipe(messages)\n    return result[0][\"generated_text\"]\n\nprint(\"Funciones de generacion definidas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ejemplo de prueba\ntest_input = {\n    \"dx\": [\"Angina inestable (I20.0)\"],\n    \"proc\": [\"Coronariografia (K492)\", \"Angioplastia DA (K493)\"],\n    \"tto\": [\n        \"Aspirina 300mg carga (B01AC06)\",\n        \"Enoxaparina 60mg SC c/12h (B01AB05)\",\n    ],\n    \"evo\": \"SDST V1-V4. Oclusion DA proximal. Angioplastia exitosa con stent.\",\n    \"dx_alta\": [\"IAM pared anterior (I21.0)\"],\n    \"med\": [\n        \"Aspirina 100mg VO c/24h (B01AC06)\",\n        \"Clopidogrel 75mg VO c/24h 12m (B01AC04)\",\n    ],\n}\n\nprint(\"Input:\")\nprint(json.dumps(test_input, indent=2, ensure_ascii=False))\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Probar con el modelo base (sin fine-tuning) para comparar\nprint(\"Probando con modelo BASE (Qwen2.5-0.5B-Instruct sin fine-tuning)...\")\nprint(\"Cargando modelo base...\")\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbase_model_test = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n)\nbase_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\njson_str = json.dumps(test_input, ensure_ascii=False, indent=2)\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTION},\n    {\"role\": \"user\", \"content\": json_str}\n]\n\ntext = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = base_tokenizer(text, return_tensors=\"pt\").to(base_model_test.device)\n\nwith torch.no_grad():\n    outputs = base_model_test.generate(\n        **inputs,\n        max_new_tokens=400,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True,\n    )\n\nresponse = base_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\nprint(\"\\nRespuesta del modelo BASE:\")\nprint(\"=\"*60)\nprint(response)\n\n# Liberar memoria\ndel base_model_test\ntorch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAhora probando con modelo FINE-TUNED...\")\nprint(\"=\"*60)\n\nresponse_ft = generate_epicrisis(model, tokenizer, test_input)\nprint(\"\\nRespuesta del modelo FINE-TUNED:\")\nprint(\"=\"*60)\nprint(response_ft)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fusionar y exportar modelo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar LoRA con modelo base\n",
    "from peft import PeftModel\n",
    "\n",
    "# Cargar modelo base sin cuantizacion para fusion\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Cargar adaptadores LoRA\n",
    "merged_model = PeftModel.from_pretrained(base_model, f\"{OUTPUT_DIR}/final\")\n",
    "\n",
    "# Fusionar\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "print(\"Modelo fusionado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo fusionado\n",
    "MERGED_OUTPUT = f\"{OUTPUT_DIR}/merged\"\n",
    "\n",
    "merged_model.save_pretrained(MERGED_OUTPUT)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "\n",
    "print(f\"Modelo fusionado guardado en: {MERGED_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exportar a ONNX (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar optimum para exportar a ONNX\n",
    "!pip install -q optimum[exporters] onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a ONNX\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "ONNX_OUTPUT = f\"{OUTPUT_DIR}/onnx\"\n",
    "\n",
    "# Exportar\n",
    "ort_model = ORTModelForCausalLM.from_pretrained(\n",
    "    MERGED_OUTPUT,\n",
    "    export=True,\n",
    "    provider=\"CPUExecutionProvider\",\n",
    ")\n",
    "\n",
    "ort_model.save_pretrained(ONNX_OUTPUT)\n",
    "tokenizer.save_pretrained(ONNX_OUTPUT)\n",
    "\n",
    "print(f\"Modelo ONNX guardado en: {ONNX_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Descargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimir y descargar modelo fusionado\n",
    "!zip -r epicrisis-merged.zip {MERGED_OUTPUT}\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"epicrisis-merged.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimir y descargar modelo ONNX (si se exporto)\n",
    "if os.path.exists(ONNX_OUTPUT):\n",
    "    !zip -r epicrisis-onnx.zip {ONNX_OUTPUT}\n",
    "    files.download(\"epicrisis-onnx.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "Este notebook realiza:\n",
    "\n",
    "1. **Carga y unificacion de datasets** - Combina todos los archivos JSONL en un solo dataset con formato ChatML\n",
    "2. **Fine-tuning con LoRA** - Entrena el modelo Qwen2.5-0.5B-Instruct con cuantizacion 4-bit\n",
    "3. **Fusion del modelo** - Combina los adaptadores LoRA con el modelo base\n",
    "4. **Exportacion a ONNX** - Para uso en el navegador\n",
    "\n",
    "### Archivos generados:\n",
    "- `datasets/unified_train.jsonl` - Dataset de entrenamiento unificado\n",
    "- `datasets/unified_validation.jsonl` - Dataset de validacion unificado\n",
    "- `epicrisis-finetuned/final/` - Adaptadores LoRA\n",
    "- `epicrisis-finetuned/merged/` - Modelo fusionado completo\n",
    "- `epicrisis-finetuned/onnx/` - Modelo en formato ONNX"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}