{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b82d3",
   "metadata": {},
   "source": [
    "# Epicrisis Fine-Tuning + Export ONNX (Colab)\n",
    "\n",
    "Flujo completo:\n",
    "1) Entrenar (LoRA) con Unsloth\n",
    "2) Merge del LoRA con el modelo base\n",
    "3) Exportar a ONNX (fp16) con Optimum\n",
    "4) (Opcional) Export oficial Transformers.js q4f16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2501e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias (Colab)\n",
    "!pip -q install -U \"unsloth[cu121]\" transformers trl datasets accelerate peft bitsandbytes\n",
    "# Export/quantización a ONNX sin Optimum\n",
    "!pip -q install -U onnxruntime onnx onnxruntime-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f509c7",
   "metadata": {},
   "source": [
    "## Subir dataset\n",
    "Sube `train.jsonl` y `validation.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración base\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "TRAIN_FILE = \"train.jsonl\"\n",
    "VAL_FILE = \"validation.jsonl\"\n",
    "\n",
    "# Donde guardamos el adapter LoRA (PEFT)\n",
    "ADAPTER_DIR = \"./epicrisis-lora-adapter\"\n",
    "\n",
    "# (Opcional) donde guardar un modelo merged (no necesario para ONNX con ORT GenAI builder)\n",
    "MERGED_DIR = \"./epicrisis-merged\"\n",
    "\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"ADAPTER_DIR:\", ADAPTER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning con Unsloth + LoRA (PEFT)\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # Unsloth elige automáticamente\n",
    "load_in_4bit = True  # QLoRA estilo (reduce VRAM). Si prefieres LoRA \"pura\", pon False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# LoRA config (ajusta si quieres)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")\n",
    "\n",
    "# Dataset: espera columnas 'prompt' y 'completion' (ajusta según tu JSONL)\n",
    "def format_example(example):\n",
    "    # Cambia esto a tu formato real:\n",
    "    prompt = example.get(\"prompt\", \"\")\n",
    "    completion = example.get(\"completion\", \"\")\n",
    "    text = prompt + completion\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\").map(format_example)\n",
    "eval_ds  = load_dataset(\"json\", data_files=VAL_FILE, split=\"train\").map(format_example)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = eval_ds,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        output_dir = \"./trainer-out\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = True,\n",
    "        logging_steps = 10,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 2,\n",
    "        optim = \"adamw_8bit\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Guardar SOLO el adapter LoRA (esto es lo que usaremos para el build ONNX)\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "print(\"Adapter LoRA guardado en:\", ADAPTER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Merge LoRA -> modelo base (no es necesario para export ONNX con ORT GenAI builder)\n",
    "# Si lo necesitas por alguna razón, puedes descomentar.\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# model = trainer.model\n",
    "# model = FastLanguageModel.for_inference(model)\n",
    "# model.save_pretrained_merged(MERGED_DIR, tokenizer, save_method=\"merged_16bit\")\n",
    "# print(\"Modelo merged guardado en:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a4c6e",
   "metadata": {},
   "source": [
    "## Exportar a ONNX + cuantizar (INT4) con onnxruntime-genai (sin Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ONNX aplicando el adapter LoRA durante la conversión\n",
    "# Nota: aunque estés en Colab (sin WebGPU), puedes generar el paquete target 'webgpu' igualmente.\n",
    "\n",
    "!python -m onnxruntime_genai.models.builder   -m {MODEL_NAME}   -o ./epicrisis-onnx-webgpu-int4   -p int4   -e webgpu   --extra_options hf_remote=true adapter_path={ADAPTER_DIR}\n",
    "\n",
    "print(\"Export listo en: ./epicrisis-onnx-webgpu-int4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ac419",
   "metadata": {},
   "source": [
    "## Prueba rápida (Python) con onnxruntime-genai (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4735d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "model_dir = \"./epicrisis-onnx-webgpu-int4\"  # paquete generado por builder\n",
    "# En Python, el provider real depende de tu instalación; para validar lógica puedes usar CPU si generas paquete CPU.\n",
    "# Si quieres validar en CPU, genera también con: -e cpu -o ./epicrisis-onnx-cpu-int4\n",
    "\n",
    "print(\"Archivos en model_dir:\", model_dir)\n",
    "\n",
    "# Ejemplo mínimo (puede variar según versión). Si falla, usa el ejemplo oficial de ORT GenAI para tu versión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988f900",
   "metadata": {},
   "source": [
    "## (Opcional) Reorganizar estructura para Transformers.js\n",
    "Si vas a usar Transformers.js, normalmente esperas una carpeta `onnx/` y archivos tokenizer/config en raíz. Con ORT GenAI el paquete es distinto; para Transformers.js puede requerir adaptación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae411e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, json\n",
    "out_dir = './epicrisis-finetuned-onnx'\n",
    "onnx_dir = os.path.join(out_dir, 'onnx')\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "for f in os.listdir(out_dir):\n",
    "    if f.endswith('.onnx') or f.endswith('.onnx_data'):\n",
    "        shutil.move(os.path.join(out_dir, f), os.path.join(onnx_dir, f))\n",
    "\n",
    "cfg_path = os.path.join(out_dir, 'config.json')\n",
    "if os.path.exists(cfg_path):\n",
    "    with open(cfg_path, 'r') as f:\n",
    "        cfg = json.load(f)\n",
    "    cfg['transformers.js_config'] = {\n",
    "        'dtype': 'fp16',\n",
    "        'kv_cache_dtype': {\n",
    "            'fp16': 'float16'\n",
    "        }\n",
    "    }\n",
    "    with open(cfg_path, 'w') as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "print('Listo:', out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ee9bf",
   "metadata": {},
   "source": [
    "## (Opcional) Export oficial Transformers.js (q4f16)\n",
    "Requiere scripts de Transformers.js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers.js.git -q\n",
    "%cd transformers.js\n",
    "!pip -q install -r requirements.txt\n",
    "!python3 scripts/convert.py \\\n",
    "  --model_id ../epicrisis-merged \\\n",
    "  --task text-generation-with-past \\\n",
    "  --quantize q4f16 \\\n",
    "  --output_dir ../epicrisis-finetuned-tjs\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578310e7",
   "metadata": {},
   "source": [
    "## Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r epicrisis-onnx-webgpu-int4.zip epicrisis-onnx-webgpu-int4\n",
    "print('ZIP creado: epicrisis-onnx-webgpu-int4.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
