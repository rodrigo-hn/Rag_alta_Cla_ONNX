# Reporte de Modelos Probados - Epicrisis App

Este documento resume todos los modelos probados para la generaci√≥n de epicrisis m√©dicas, incluyendo configuraciones de fine-tuning, datasets utilizados y resultados obtenidos.

---

## 1. Modelos Base (Sin Fine-tuning)

| Modelo | Par√°metros | Tama√±o (q4f16) | WebGPU | Funciona | Calidad | Recomendado | Notas |
|--------|-----------|----------------|--------|----------|---------|-------------|-------|
| **Qwen2.5-0.5B-Instruct** | 500M | 483 MB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê | ‚úÖ **S√≠** | Mejor balance tama√±o/calidad, genera c√≥digos con prompt adecuado |
| **Qwen2.5-1.5B-Instruct** | 1.5B | 1.2 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è | Mejor calidad pero m√°s lento |
| **Qwen3-4B-ONNX** | 4B | 2.8 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è | Excelente pero grande, modo "thinking" |
| **Llama-3.2-1B-Instruct** | 1B | 1.1 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê | ‚ùå | Funciona pero peor en espa√±ol |
| **SmolLM2-360M-Instruct** | 360M | 200 MB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê | ‚ùå | Muy peque√±o, calidad limitada |
| **Phi-3.5-mini-instruct** | 3.8B | 2.2 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è | Bueno pero grande |
| **Ministral-3B-Instruct** | 3B | 2.4 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê | ‚ùå | Multimodal, no necesario |
| **Granite-3.0-2B** | 2B | 1.6 GB | ‚úÖ | ‚ö†Ô∏è | ‚≠ê‚≠ê‚≠ê | ‚ùå | IBM, funciona pero no ideal |

---

## 2. Modelos Fine-tuned

| Modelo Fine-tuned | Base | M√©todo | Dataset | Ejemplos | Precisi√≥n | WebGPU | Funciona | Calidad C√≥digos | Notas |
|-------------------|------|--------|---------|----------|-----------|--------|----------|-----------------|-------|
| **Unsloth FP16** | Qwen2.5-0.5B | Unsloth + LoRA | ChatML unificado | 321 train | FP16 | ‚úÖ | ‚ö†Ô∏è | ‚ùå No genera | Texto coherente pero sin c√≥digos CIE-10/K/ATC |
| **Unsloth ONNX-cached** | Qwen2.5-0.5B | Unsloth ‚Üí ONNX | ChatML unificado | 321 train | FP16 + KV-cache | ‚úÖ | ‚úÖ | ‚ùå No genera | KV-cache funciona, pero no aprendi√≥ c√≥digos |
| **MLX Merged** | Qwen2.5-1.5B | MLX + LoRA | mlx_data | 971 train | FP16 | ‚ùå | ‚úÖ (Mac) | ‚ö†Ô∏è Parcial | Solo Mac M1-M4, genera algunos c√≥digos |
| **HF PEFT q4f16** | Qwen2.5-0.5B | Transformers + PEFT | train.jsonl | 321 train | q4f16 | ‚ùå | ‚ùå | N/A | Opset 18 incompatible con Transformers.js |
| **HF PEFT q8** | Qwen2.5-0.5B | Transformers + PEFT | train.jsonl | 321 train | INT8 | ‚ùå | ‚ùå | N/A | Cuantizaci√≥n corrompe pesos |
| **HF PEFT FP32** | Qwen2.5-0.5B | Transformers + PEFT | train.jsonl | 321 train | FP32 | ‚ùå | ‚ùå | N/A | 6GB - demasiado grande para navegador |

---

## 3. Configuraci√≥n de Fine-tuning

### M√©todos Probados

| M√©todo | Librer√≠a | GPU Requerida | Tiempo | VRAM | Salida | Recomendado |
|--------|----------|---------------|--------|------|--------|-------------|
| **Unsloth** | unsloth + peft | T4 (Colab gratis) | ~30 min | 6 GB | LoRA ‚Üí FP16 ‚Üí GGUF/ONNX | ‚úÖ **S√≠** |
| **MLX** | mlx-lm | Apple M1-M4 | ~45 min | 8 GB | LoRA ‚Üí Merged | ‚ö†Ô∏è Solo Mac |
| **HF Standard** | transformers + peft | T4/A100 | ~60 min | 8-16 GB | LoRA ‚Üí Merged ‚Üí ONNX | ‚ö†Ô∏è Problemas cuantizaci√≥n |

### Hiperpar√°metros Utilizados (Unsloth - Recomendado)

```python
# Configuraci√≥n √≥ptima para Qwen2.5-0.5B
epochs = 3
batch_size = 4
learning_rate = 2e-4
lora_rank = 16
lora_alpha = 16
lora_dropout = 0
max_seq_length = 1024
gradient_accumulation_steps = 2
warmup_steps = 5
weight_decay = 0.01
optimizer = "adamw_8bit"
```

### Hiperpar√°metros MLX (Apple Silicon)

```python
# Configuraci√≥n para Mac M1-M4
epochs = 3-5
batch_size = 4
learning_rate = 1e-5  # Conservador
lora_rank = 8-16
lora_layers = 16-24
```

---

## 4. Datasets Disponibles

### Dataset Principal

| Dataset | Ubicaci√≥n | Ejemplos | Prop√≥sito | Calidad |
|---------|-----------|----------|-----------|---------|
| **train.jsonl** | datasets/ | 321 | Entrenamiento principal | ‚≠ê‚≠ê‚≠ê |
| **validation.jsonl** | datasets/ | 36 | Validaci√≥n | ‚≠ê‚≠ê‚≠ê |
| **mlx_data/train.jsonl** | datasets/mlx_data/ | 971 | MLX training | ‚≠ê‚≠ê‚≠ê‚≠ê |

### Datasets Especializados

| Dataset | Ubicaci√≥n | Ejemplos | Prop√≥sito | Calidad |
|---------|-----------|----------|-----------|---------|
| **anatomia_coronaria.jsonl** | datasets/ | 43 | Anatom√≠a DA/CD/CX correcta | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **codigos_correctos.jsonl** | datasets/ | 48 | CIE-10, K, ATC correctos | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **ejemplos_negativos.jsonl** | datasets/ | 29 | Contraejemplos (qu√© NO hacer) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **dataset_extra_1.jsonl** | datasets/ | 250 | Sint√©ticos generados | ‚≠ê‚≠ê |
| **dataset_extra_2.jsonl** | datasets/ | 250 | Sint√©ticos generados | ‚≠ê‚≠ê |
| **dataset_extra_3.jsonl** | datasets/ | 250 | Sint√©ticos generados | ‚≠ê‚≠ê |

### Formato de Datos

```json
{
  "instruction": "Epicrisis:",
  "input": {
    "dx": ["diagn√≥stico (CIE-10)"],
    "proc": ["procedimiento (c√≥digo K)"],
    "tto": ["tratamiento (ATC)"],
    "evo": "evoluci√≥n cl√≠nica",
    "dx_alta": ["diagn√≥stico alta"],
    "med": ["medicaci√≥n alta"]
  },
  "output": "Texto narrativo de epicrisis con c√≥digos..."
}
```

### Formato ChatML (para Qwen2.5-Instruct)

```
<|im_start|>system
Eres un m√©dico especialista que genera epicrisis...<|im_end|>
<|im_start|>user
{json_input}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>
```

---

## 5. Modelos Funcionales Actuales

Ubicaci√≥n: `app/public/models/`

| Modelo | Tama√±o | Tipo | Uso Principal |
|--------|--------|------|---------------|
| `lora-unsloth` | 49 MB | Adaptadores LoRA | Re-entrenar/fusionar |
| `merged-f16-unsloth` | 957 MB | Safetensors FP16 | Modelo base merged |
| `onnx-finetuned-unsloth` | 1.9 GB | ONNX + KV-cache | Inferencia con cache |
| `onnx-webgpu-fp16-chatml-v3` | 959 MB | ONNX WebGPU | ORT GenAI format |
| `onnx-webgpu-fp16-unsloth` | 1.9 GB | ONNX WebGPU | Transformers.js |

---

## 6. Modelos Eliminados (No Funcionan)

| Modelo | Tama√±o | Raz√≥n de Eliminaci√≥n |
|--------|--------|---------------------|
| epicrisis-q4f16-finetuned-tjs | 26 GB | Opset 18 incompatible con Transformers.js |
| epicrisis-q8-finetuned-tjs | 16 GB | INT8 corrompe pesos del modelo |
| onnx-cpu-fp16 | 5.8 GB | Demasiado grande para navegador |
| onnx-cpu-fp32 | 5.8 GB | Demasiado grande para navegador |
| onnx-cpu-int4* | 1.1 GB | CPU only, no soporta WebGPU |
| onnx-cpu-int8 | 0 GB | Carpeta vac√≠a |
| epicrisis-merged | 2.9 GB | Qwen 1.5B FP32 safetensors, no ONNX |
| epicrisis-onnx | 5.8 GB | ONNX FP32 con .onnx_data externo |
| epicrisis-onnx-q8 | 1.5 GB | INT8 corrompe pesos |
| mlx_adapters | 67 MB | Solo Mac, checkpoints intermedios |
| mlx_merged | 953 MB | Solo Mac (MLX format), no WebGPU |

**Total espacio liberado:** ~60 GB

---

## 7. Problemas Identificados y Soluciones

| Problema | Causa | Soluci√≥n | Estado |
|----------|-------|----------|--------|
| Modelo no genera c√≥digos CIE-10/K/ATC | Fine-tuning insuficiente | M√°s ejemplos + prompts expl√≠citos | üî¥ Pendiente |
| Texto repetitivo | Sin repetition penalty | Agregar `repetition_penalty=1.2-1.5` | ‚úÖ Resuelto |
| INT8/q8 incompatible | Opset ONNX incorrecto | Usar q4f16 o FP16 | ‚úÖ Resuelto |
| Modelo >3GB falla en browser | L√≠mite WASM | Usar modelos <2GB | ‚úÖ Resuelto |
| KV-cache no funciona | Inputs faltantes | Inicializar past_key_values | ‚úÖ Resuelto |
| Opset 18 incompatible | Transformers.js no soporta | Usar scripts oficiales de conversi√≥n | ‚úÖ Resuelto |

---

## 8. Tama√±os de Referencia por Precisi√≥n

| Modelo | Par√°metros | FP32 | FP16 | q4f16 | q8 |
|--------|-----------|------|------|-------|-----|
| Qwen2.5-0.5B | 500M | 2GB | 1GB | 512MB | 650MB |
| Qwen2.5-1.5B | 1.5B | 6GB | 3GB | 1.2GB | 1.5GB |
| Qwen3-4B | 4B | ~8GB | ~4GB | 2.8GB | 3.5GB |
| Llama-3.2-1B | 1B | 4GB | 2GB | 1.1GB | 1.3GB |
| SmolLM2-360M | 360M | 1.5GB | 750MB | 200MB | 350MB |
| Phi-3.5-mini | 3.8B | ~8GB | ~4GB | 2.2GB | 2.8GB |

---

## 9. Recomendaciones Finales

### Para Producci√≥n (WebGPU en Navegador)

| Escenario | Modelo Recomendado | Raz√≥n |
|-----------|-------------------|-------|
| **Uso general** | Qwen2.5-0.5B-Instruct (base) | Funciona, genera c√≥digos con prompt adecuado |
| **Mejor calidad** | Qwen3-4B-ONNX | Excelente pero 2.8GB, requiere buena GPU |
| **Desarrollo Mac** | MLX + Qwen2.5-1.5B | R√°pido en Apple Silicon |
| **Fine-tuning futuro** | Unsloth + m√°s datos | Necesita 500+ ejemplos con c√≥digos |

### Pr√≥ximos Pasos para Mejorar Fine-tuning

1. **Aumentar dataset** a 500-1000 ejemplos con c√≥digos expl√≠citos
2. **Usar few-shot prompting** en el system prompt
3. **Entrenar m√°s epochs** (5-10) con learning rate m√°s bajo
4. **Probar Qwen2.5-1.5B** como base (m√°s capacidad)
5. **Validar con m√©tricas** de presencia de c√≥digos CIE-10/K/ATC

---

## 10. Scripts y Notebooks Disponibles

### Notebooks (en `notebooks/`)

| Notebook | Prop√≥sito |
|----------|-----------|
| `colab_finetune_unsloth.ipynb` | ‚≠ê **Principal** - Fine-tuning con Unsloth en Colab |
| `colab_finetune.ipynb` | Fine-tuning est√°ndar con HuggingFace |
| `Epicrisis_FineTuning_ORTGenAI_UPDATED.ipynb` | Exportaci√≥n a ORT GenAI |
| `epicrisis_transformersjs_export.ipynb` | Exportaci√≥n a Transformers.js |

### Scripts de Conversi√≥n (en `scripts/conversion/`)

| Script | Prop√≥sito |
|--------|-----------|
| `convert_to_chatml.py` | Convertir datasets a formato ChatML |
| `convert_to_onnx.py` | Exportar modelo a ONNX |
| `export_ortgenai.py` | Exportar a ORT GenAI format |
| `quantize_onnx.py` | Cuantizar modelo ONNX |

### Scripts de Entrenamiento (en `scripts/training/`)

| Script | Prop√≥sito |
|--------|-----------|
| `finetune_epicrisis.py` | Fine-tuning con Transformers + PEFT |
| `mlx_finetune.py` | Fine-tuning con MLX (Mac) |
| `generate_extra_datasets.py` | Generar datasets sint√©ticos |
| `unify_datasets.py` | Combinar m√∫ltiples datasets |

---

*Documento generado: Enero 2025*
*√öltima actualizaci√≥n: Enero 17, 2025*
