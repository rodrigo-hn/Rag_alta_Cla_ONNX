{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f9b82d3",
      "metadata": {},
      "source": [
        "# Epicrisis Fine-Tuning + Export ONNX (Colab)\n",
        "\n",
        "Flujo completo:\n",
        "1) Entrenar (LoRA) con Unsloth\n",
        "2) Merge del LoRA con el modelo base\n",
        "3) Exportar a ONNX (fp16) con Optimum\n",
        "4) (Opcional) Export oficial Transformers.js q4f16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2501e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar dependencias\n",
        "!pip -q install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\n",
        "!pip -q install --no-deps trl peft accelerate bitsandbytes\n",
        "!pip -q install transformers datasets optimum[exporters] onnx onnxruntime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f509c7",
      "metadata": {},
      "source": [
        "## Subir dataset\n",
        "Sube `train.jsonl` y `validation.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67b7150",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cd4e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuracion base\n",
        "MODEL_NAME = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
        "TRAIN_FILE = 'train.jsonl'\n",
        "VAL_FILE = 'validation.jsonl'\n",
        "OUTPUT_DIR = './epicrisis-model-finetuned'\n",
        "MERGED_DIR = './epicrisis-merged'\n",
        "\n",
        "print('MODEL_NAME:', MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859e1825",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning con Unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None\n",
        "LOAD_IN_4BIT = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        'q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
        "        'gate_proj', 'up_proj', 'down_proj'\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias='none',\n",
        "    use_gradient_checkpointing='unsloth',\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "def format_prompt(example):\n",
        "    instruction = example.get('instruction', 'Epicrisis:')\n",
        "    input_data = example.get('input', {})\n",
        "    output = example.get('output', '')\n",
        "\n",
        "    if isinstance(input_data, dict):\n",
        "        import json\n",
        "        input_str = json.dumps(input_data, ensure_ascii=False)\n",
        "    else:\n",
        "        input_str = str(input_data)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'Eres un asistente medico experto en redaccion de epicrisis clinicas en espanol.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f\"{instruction}\\n{input_str}\"\n",
        "        },\n",
        "        {\n",
        "            'role': 'assistant',\n",
        "            'content': output\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "    return {'text': text}\n",
        "\n",
        "train_dataset = load_dataset('json', data_files=TRAIN_FILE, split='train')\n",
        "val_dataset = load_dataset('json', data_files=VAL_FILE, split='train')\n",
        "train_dataset = train_dataset.map(format_prompt)\n",
        "val_dataset = val_dataset.map(format_prompt)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.03,\n",
        "    optim='adamw_8bit',\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=0.3,\n",
        "    logging_steps=10,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    save_strategy='steps',\n",
        "    save_steps=50,\n",
        "    save_total_limit=3,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    seed=42,\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field='text',\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "print('Loss final:', trainer_stats.metrics.get('train_loss'))\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Modelo LoRA guardado en', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cb19cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA con modelo base\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print('Modelo merged guardado en', MERGED_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c3a4c6e",
      "metadata": {},
      "source": [
        "## Exportar a ONNX (fp16, GPU)\n",
        "Usa `--device cuda` para fp16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28fe288d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!optimum-cli export onnx \\\n",
        "  --model ./epicrisis-merged \\\n",
        "  --task text-generation-with-past \\\n",
        "  --dtype fp16 \\\n",
        "  --device cuda \\\n",
        "  --opset 18 \\\n",
        "  --no-dynamic-axes \\\n",
        "  --batch_size 1 \\\n",
        "  --sequence_length 2 \\\n",
        "  ./epicrisis-finetuned-onnx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3988f900",
      "metadata": {},
      "source": [
        "## Reorganizar estructura para Transformers.js (fp16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae411e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil, json\n",
        "out_dir = './epicrisis-finetuned-onnx'\n",
        "onnx_dir = os.path.join(out_dir, 'onnx')\n",
        "os.makedirs(onnx_dir, exist_ok=True)\n",
        "\n",
        "for f in os.listdir(out_dir):\n",
        "    if f.endswith('.onnx') or f.endswith('.onnx_data'):\n",
        "        shutil.move(os.path.join(out_dir, f), os.path.join(onnx_dir, f))\n",
        "\n",
        "cfg_path = os.path.join(out_dir, 'config.json')\n",
        "if os.path.exists(cfg_path):\n",
        "    with open(cfg_path, 'r') as f:\n",
        "        cfg = json.load(f)\n",
        "    cfg['transformers.js_config'] = {\n",
        "        'dtype': 'fp16',\n",
        "        'kv_cache_dtype': {\n",
        "            'fp16': 'float16'\n",
        "        }\n",
        "    }\n",
        "    with open(cfg_path, 'w') as f:\n",
        "        json.dump(cfg, f, indent=2)\n",
        "\n",
        "print('Listo:', out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26ee9bf",
      "metadata": {},
      "source": [
        "## (Opcional) Export oficial Transformers.js (q4f16)\n",
        "Requiere scripts de Transformers.js."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c4a52f",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/transformers.js.git -q\n",
        "%cd transformers.js\n",
        "!pip -q install -r requirements.txt\n",
        "!python3 scripts/convert.py \\\n",
        "  --model_id ../epicrisis-merged \\\n",
        "  --task text-generation-with-past \\\n",
        "  --quantize q4f16 \\\n",
        "  --output_dir ../epicrisis-finetuned-tjs\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578310e7",
      "metadata": {},
      "source": [
        "## Descargar resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d29f2d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r epicrisis-finetuned-onnx.zip epicrisis-finetuned-onnx\n",
        "from google.colab import files\n",
        "files.download('epicrisis-finetuned-onnx.zip')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
