{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9b82d3",
   "metadata": {
    "id": "3f9b82d3"
   },
   "source": [
    "# Epicrisis Fine-Tuning + Export ONNX (Colab)\n",
    "\n",
    "Flujo completo:\n",
    "1) Entrenar (LoRA) con Unsloth\n",
    "2) Merge del LoRA con el modelo base\n",
    "3) Exportar a ONNX (fp16) con Optimum\n",
    "4) (Opcional) Export oficial Transformers.js q4f16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7sAlKktSGcsF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sAlKktSGcsF",
    "outputId": "265d5e54-5a54-4346-ef3d-f843492b8897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TZOtWCWDGpLr",
   "metadata": {
    "id": "TZOtWCWDGpLr"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/content/drive/MyDrive/fine-tuning\"\n",
    "ADAPTER_DIR = f\"{BASE_DIR}/epicrisis-lora-adapter\"\n",
    "\n",
    "import os\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2501e8",
   "metadata": {
    "id": "0c2501e8"
   },
   "outputs": [],
   "source": [
    "!pip -q uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Instala el trío alineado con el stack actual de Colab (CUDA 12.6)\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu126 \\\n",
    "  torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0\n",
    "\n",
    "# Dependencias estables para SFT + LoRA + export ORT GenAI (evita cambios de API inesperados)\n",
    "!pip -q install -U \\\n",
    "  \"transformers==4.52.4\" \\\n",
    "  \"trl==0.11.4\" \\\n",
    "  \"peft==0.13.2\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  datasets bitsandbytes\n",
    "\n",
    "# Export ONNX + cuantización (ORT GenAI builder) + dependencia onnx-ir\n",
    "!pip -q install -U onnx onnxruntime onnxruntime-genai onnx-ir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "C8UxakQ-Ptko",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8UxakQ-Ptko",
    "outputId": "0603baf2-709d-4080-b2ac-9b35115fadcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q uninstall -y trl transformers peft accelerate\n",
    "!pip -q install -U \"transformers==4.46.3\" \"trl==0.11.4\" \"peft==0.13.2\" \"accelerate==0.34.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wN6CZ1vvMhj4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN6CZ1vvMhj4",
    "outputId": "674e1d8f-6cda-421c-b7ed-aa4650e2af42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.9.0+cu126\n",
      "torchvision 0.24.0+cu126\n",
      "torchaudio 2.9.0+cu126\n",
      "cuda 12.6\n",
      "cuda available True\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision, torchaudio\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)\n",
    "print(\"torchaudio\", torchaudio.__version__)\n",
    "print(\"cuda\", torch.version.cuda)\n",
    "print(\"cuda available\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f509c7",
   "metadata": {
    "id": "86f509c7"
   },
   "source": [
    "## Subir dataset\n",
    "Sube `train.jsonl` y `validation.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67b7150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "c67b7150",
    "outputId": "789332bd-bd29-4816-d389-1cdc89941fd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d3278097-8daa-4bc1-bb42-cbfb51bda376\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-d3278097-8daa-4bc1-bb42-cbfb51bda376\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train.jsonl to train.jsonl\n",
      "Saving validation.jsonl to validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "i32zOBpmNY0H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i32zOBpmNY0H",
    "outputId": "d8caf757-44c6-4138-b28b-142f493fbad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08cd4e47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08cd4e47",
    "outputId": "30b94929-9bcc-4032-f82f-837f0e275675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: Qwen/Qwen2.5-0.5B-Instruct\n",
      "TRAIN_FILE: /content/train.jsonl\n",
      "VAL_FILE: /content/validation.jsonl\n",
      "ADAPTER_DIR: /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-lora-adapter\n",
      "MERGED_DIR: /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-merged\n"
     ]
    }
   ],
   "source": [
    "# Configuración base\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "TRAIN_FILE = \"/content/train.jsonl\"\n",
    "VAL_FILE   = \"/content/validation.jsonl\"\n",
    "\n",
    "# Donde guardamos TODO en Google Drive\n",
    "BASE_DIR = \"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b\"\n",
    "ADAPTER_DIR = f\"{BASE_DIR}/epicrisis-lora-adapter\"\n",
    "\n",
    "# (Opcional) donde guardar un modelo merged (NO recomendado si tu objetivo es ORT GenAI builder)\n",
    "MERGED_DIR = f\"{BASE_DIR}/epicrisis-merged\"\n",
    "\n",
    "import os\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"TRAIN_FILE:\", TRAIN_FILE)\n",
    "print(\"VAL_FILE:\", VAL_FILE)\n",
    "print(\"ADAPTER_DIR:\", ADAPTER_DIR)\n",
    "print(\"MERGED_DIR:\", MERGED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859e1825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630,
     "referenced_widgets": [
      "fae2c963c89a4e5791d4b505083f55ab",
      "7f2b859e6a854b0dbe61b547e4204dea",
      "7a7be7054d664a95b704c72f5477b0f6",
      "e11091eaac404b3aa586cfb9b0fc5ca7",
      "ae4221453c8f4d9c939bc659680a7c87",
      "5e0520f0b6514d318684310eb31a46f0",
      "319d445016e946ef8d6eeca9c398e984",
      "5e08a8fd9682426da01458d15d6e4404",
      "abe6f3f9731443f6a48dfc945fa933fb",
      "32d96ec56a0a4a85b82d0e498df21d20",
      "2379f32dda2341a6b218eff4d8fd1e7c",
      "0bd71524f0bc4ba283b31d9b82418d85",
      "459fcb162a07456db312a0d79f79a53b",
      "a9fb1808d6ce41e78f262aeccfea8abd",
      "9d73f8f15393447b9d77108ea4cf1ddc",
      "d1056bdf56d74acc95c6c0c9f9023de3",
      "a184c4db4d4540e19513c4d9da445ce4",
      "4b54881eb3bb492e890d508a715fd6d8",
      "051fd6f6bb4c423eb11db9079005c7be",
      "35ce2b9869a7432c99b5f0f8c7ebaf9b",
      "a148cc6f6def45d0bd157f20d30d3998",
      "ca710803bbf34ebe98aef4404e17c792",
      "0447fd9f67c04cd5aadca1a3f06d98e5",
      "196e59030fe1475eb99a55b67c0b2f58",
      "c90c72559918474fab107f06ad00d1bc",
      "a39a722390664e1fb87b9266c566e041",
      "f0de7e74825a40cd90787ba620aeb740",
      "0e284328789f4381b77526bd5c131869",
      "f852c4ec4b46478c9cf909d069501151",
      "c0fd60027ee34dc49f576fba87f21b22",
      "af7d8583095045c88b787881c8afc0b5",
      "df010c9d61c7488b8eba24deb2d096b5",
      "9bd3593b11464402a1416a732cb30951",
      "3e131c16459f41da9597f547269a08c9",
      "3448729e8af54acbab460dc3a8657cfa",
      "325a3506898d41f2ad7249342daa914d",
      "082d64e50ac14d95a346299d40c1787f",
      "213365b419954c19a88b11bb27f21e98",
      "46c110ff2ce54f41b45529c17742dff7",
      "1e9d50e9df38443aa17f2d253ec99b7c",
      "9f8d41f6ca4e4880a76fd04e0f07d76e",
      "d32ee985f2e847f0b990b1c3647de0e2",
      "9997c25c46be4404b423f23b92449351",
      "d8c6821c86cb4d7caf410048e03902a0",
      "b3975e41e2a24e4089d9058c7724ba95",
      "271476620cc6459d977f0fad00290c39",
      "cb6f02cd689d4500896a637b153fb226",
      "35c392674d3d46cab5e78fb51f75edb4",
      "396caf00ae3d4cf39b543e4980fd065f",
      "601fcc12842f4ee1850cde97b24c043a",
      "4ceb21c89b1a4411853fce16a22a86f2",
      "88c167314b864a9ebb8bfb52005692f5",
      "b8175df4fcca4d3aa71afa2ad2ce56d0",
      "c5387efebdff43ce916bbd6f12745de2",
      "4c958169c8c44a02889f15fc546146dc"
     ]
    },
    "id": "859e1825",
    "outputId": "7c402c74-1db8-415f-c498-4808bdcd45dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae2c963c89a4e5791d4b505083f55ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd71524f0bc4ba283b31d9b82418d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0447fd9f67c04cd5aadca1a3f06d98e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e131c16459f41da9597f547269a08c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3975e41e2a24e4089d9058c7724ba95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Adapter LoRA guardado en: /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-lora-adapter\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning con Transformers + PEFT (LoRA) + TRL (SFTTrainer) - SIN Unsloth\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "# 1) Dataset (tu formato real: instruction / input / output)\n",
    "data_files = {\"train\": TRAIN_FILE, \"validation\": VAL_FILE}\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "def format_example(example, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un médico experto en redacción de epicrisis clínicas en español.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"] + \"\\n\" + json.dumps(example[\"input\"], ensure_ascii=False, indent=2)},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "# 2) Modelo + tokenizer (forzando SDPA para evitar xformers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",  # <- clave: estable en Colab sin xformers\n",
    ")\n",
    "\n",
    "# 3) LoRA config (estable para export/convert posterior)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def map_fn(ex):\n",
    "    return {\"text\": format_example(ex, tokenizer)}\n",
    "\n",
    "train_ds = ds[\"train\"].map(map_fn, remove_columns=ds[\"train\"].column_names)\n",
    "eval_ds  = ds[\"validation\"].map(map_fn, remove_columns=ds[\"validation\"].column_names)\n",
    "\n",
    "# 4) Trainer (sin optim 8bit para evitar dependencias extra; estable)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/_trainer-out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"no\",          # guardamos manualmente el adapter al final\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5) Guardar SOLO el adapter LoRA + tokenizer en Drive (persistente)\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "print(\"✅ Adapter LoRA guardado en:\", ADAPTER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb19cb",
   "metadata": {
    "id": "e5cb19cb"
   },
   "outputs": [],
   "source": [
    "# (Opcional) Merge LoRA -> modelo base (no es necesario para export ONNX con ORT GenAI builder)\n",
    "# Si lo necesitas por alguna razón, puedes descomentar.\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# model = trainer.model\n",
    "# model = FastLanguageModel.for_inference(model)\n",
    "# model.save_pretrained_merged(MERGED_DIR, tokenizer, save_method=\"merged_16bit\")\n",
    "# print(\"Modelo merged guardado en:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a4c6e",
   "metadata": {
    "id": "3c3a4c6e"
   },
   "source": [
    "## Exportar a ONNX + cuantizar (INT4) con onnxruntime-genai (sin Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mDhpMzV4RCGM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDhpMzV4RCGM",
    "outputId": "a7cef538-2399-44f5-9caa-8c75ca7bd7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Instala la dependencia faltante del builder\n",
    "!pip -q install -U onnx-ir\n",
    "\n",
    "# (Recomendado) actualiza onnxruntime-genai por si estabas en una versión vieja\n",
    "!pip -q install -U onnxruntime-genai onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33V_moPVRGyw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33V_moPVRGyw",
    "outputId": "dbbebb84-fbc7-482d-c02b-824dc2af8224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx_ir OK: 0.1.14\n"
     ]
    }
   ],
   "source": [
    "import onnx_ir\n",
    "print(\"onnx_ir OK:\", onnx_ir.__version__ if hasattr(onnx_ir, \"__version__\") else \"import ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1s1pKUf2RhZd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1s1pKUf2RhZd",
    "outputId": "aefa5214-860c-45a8-eee4-a6ccbfd55949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/10.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m176.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# 1) Asegura dependencias del builder\n",
    "!pip -q install -U onnx-ir onnx onnxruntime onnxruntime-genai\n",
    "\n",
    "# 2) Sube transformers a una versión que incluya Qwen2.5-VL\n",
    "# (esto NO afecta tu adapter ya entrenado)\n",
    "!pip -q install -U \"transformers==4.52.4\" \"tokenizers>=0.20.0\" \"huggingface-hub>=0.24.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbd0d4",
   "metadata": {},
   "source": [
    "> **Nota sobre HF_TOKEN:** para este modelo público normalmente no necesitas token.  \n",
    "> Si en algún caso necesitas autenticarte, usa `!huggingface-cli login` o los *Colab Secrets* (no pegues tokens en el notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28fe288d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28fe288d",
    "outputId": "056e44ba-abe9-4371-8d0e-a5f31453edce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, BF16 CUDA, FP16 TRT-RTX, INT4 CPU, INT4 CUDA, INT4 DML, INT4 WebGPU\n",
      "Extra options: {'hf_remote': 'true', 'adapter_path': '/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-lora-adapter'}\n",
      "GroupQueryAttention (GQA) is used in this model.\n",
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\n",
      "Saving GenAI config in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\n",
      "Saving processing files in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4 for GenAI\n",
      "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, BF16 CUDA, FP16 TRT-RTX, INT4 CPU, INT4 CUDA, INT4 DML, INT4 WebGPU\n",
      "Extra options: {'hf_remote': 'true', 'adapter_path': '/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-lora-adapter'}\n",
      "GroupQueryAttention (GQA) is used in this model.\n",
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4\n",
      "Saving GenAI config in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4\n",
      "Saving processing files in /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4 for GenAI\n",
      "✅ ONNX CPU: /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\n",
      "✅ ONNX WebGPU: /content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 06:20:43,076 numexpr.utils [INFO] - NumExpr defaulting to 2 threads.\n",
      "2026-01-15 06:20:45.456253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768458045.489094   16268 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768458045.499403   16268 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768458045.524424   16268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458045.524503   16268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458045.524517   16268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458045.524528   16268 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-15 06:20:45.531561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-15 06:21:11,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[1] ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/ReduceSum ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Shape ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/1 ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Gather ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm/LayerNorm ...\n",
      "2026-01-15 06:21:11,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,328 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,328 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,329 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,329 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:11,348 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,348 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,348 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,349 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,349 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,349 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,349 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:11,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:11,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,356 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,356 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,357 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,357 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:11,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:11,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,375 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,375 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:11,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,492 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,492 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:11,592 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:11,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:11,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/Mul ...\n",
      "2026-01-15 06:21:11,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,701 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,702 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,702 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:11,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,723 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:11,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:11,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,732 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,732 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:11,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:11,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:11,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:11,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:11,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:11,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:11,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/Mul ...\n",
      "2026-01-15 06:21:12,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,086 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,086 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:12,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,181 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,181 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,181 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:12,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:12,205 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,205 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,205 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,206 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,206 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,207 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,207 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:12,209 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,209 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,210 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,211 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,211 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:12,234 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,234 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,234 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,234 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,235 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,235 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,237 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,238 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:12,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,356 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,356 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/Mul ...\n",
      "2026-01-15 06:21:12,464 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,467 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,467 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,468 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,468 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:12,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,595 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:12,598 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,598 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,598 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,599 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,599 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,600 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,600 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:12,602 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,602 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,603 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,604 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,604 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:12,622 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,622 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,626 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,626 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:12,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:12,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:12,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:12,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/Mul ...\n",
      "2026-01-15 06:21:12,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:12,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,951 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,951 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,952 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,952 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:12,969 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,969 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,969 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:12,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:12,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,979 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:12,980 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,980 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:12,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:12,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:12,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:12,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,002 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,002 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:13,116 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,116 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,117 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,117 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,117 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/Mul ...\n",
      "2026-01-15 06:21:13,218 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,220 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,220 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:13,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:13,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,334 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,334 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:13,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:13,358 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,358 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,358 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,359 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,359 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,359 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,359 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:13,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,363 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,363 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,364 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,364 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:13,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:13,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,386 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,386 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:13,475 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,475 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,475 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/Mul ...\n",
      "2026-01-15 06:21:13,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,575 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,575 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,576 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,576 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:13,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:13,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,666 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,666 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:13,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:13,689 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,689 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,689 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,690 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,690 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,690 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,690 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:13,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,695 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,695 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:13,710 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,710 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,710 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:13,710 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,711 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,711 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,713 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,713 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:13,811 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,811 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,811 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,812 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,812 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,814 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,814 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/Mul ...\n",
      "2026-01-15 06:21:13,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:13,912 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,912 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:13,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:13,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:14,012 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,014 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,014 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:14,031 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,031 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,031 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,033 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,033 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:14,035 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,035 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,035 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,036 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,036 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:14,040 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,042 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,042 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:14,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,060 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,060 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,062 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,063 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:14,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/Mul ...\n",
      "2026-01-15 06:21:14,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,289 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,289 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:14,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:14,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:14,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,433 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,433 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,433 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,433 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:14,436 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,436 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,437 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,438 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:14,457 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,457 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,457 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,457 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,458 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,458 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,460 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,460 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:14,566 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,566 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,566 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,567 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,567 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,569 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,569 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:14,690 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:14,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:14,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/Mul ...\n",
      "2026-01-15 06:21:14,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,695 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,695 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:14,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:14,850 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,850 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,850 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,851 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,851 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:14,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,857 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,857 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,857 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,858 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:14,861 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,861 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,862 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:14,862 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:14,862 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:14,862 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:14,862 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:14,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:14,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:14,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:14,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:14,893 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:14,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:15,006 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,006 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,007 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,007 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,008 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/Mul ...\n",
      "2026-01-15 06:21:15,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:15,260 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,260 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,260 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,260 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,263 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,263 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:15,284 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:15,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,290 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,291 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,291 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:15,294 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,294 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:15,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,321 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:15,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,441 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,441 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,446 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,446 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/Mul ...\n",
      "2026-01-15 06:21:15,562 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,565 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,565 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,652 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,652 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:15,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:15,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:15,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:15,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:15,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:15,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:15,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,684 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:15,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,754 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,754 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:15,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:15,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:15,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/Mul ...\n",
      "2026-01-15 06:21:15,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:15,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:15,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,903 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,903 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:15,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:15,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:15,921 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,922 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,922 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:15,922 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,922 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,922 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:15,994 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,994 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:15,994 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:15,995 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,995 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:15,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:15,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/Mul ...\n",
      "2026-01-15 06:21:16,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,069 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,069 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:16,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:16,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,151 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,151 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,151 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:16,153 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,153 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:16,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:16,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,171 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:16,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,246 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,246 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:16,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,319 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:16,319 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:16,319 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/Mul ...\n",
      "2026-01-15 06:21:16,319 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,321 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,321 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:16,387 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,387 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,387 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,387 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,389 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,389 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:16,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:16,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:16,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,409 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,409 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,409 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,409 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:16,421 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,421 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:16,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,510 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,510 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:16,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:16,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:16,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/Mul ...\n",
      "2026-01-15 06:21:16,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,582 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,582 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:16,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,652 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,652 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,653 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,653 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:16,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,665 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,666 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,666 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:16,669 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,669 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,669 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:16,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,674 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,674 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,686 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,688 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,688 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:16,756 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,756 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,756 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,757 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,757 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,758 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,758 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/Mul ...\n",
      "2026-01-15 06:21:16,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,825 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,825 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,826 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,826 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,894 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:16,907 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,907 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,907 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,908 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:16,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,911 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,911 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,911 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,911 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:16,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:16,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:16,913 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:16,914 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:16,914 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,914 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,914 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,915 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,915 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:16,926 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:16,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,929 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,929 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:16,996 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:16,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:16,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:16,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:16,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/Mul ...\n",
      "2026-01-15 06:21:17,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,069 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,069 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,070 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,070 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:17,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:17,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:17,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,152 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,153 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,153 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,153 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:17,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,157 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:17,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:17,240 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,240 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,240 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/Mul ...\n",
      "2026-01-15 06:21:17,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:17,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,380 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,380 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,380 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:17,392 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,392 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,394 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,394 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:17,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:17,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:17,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:17,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:17,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:17,413 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,413 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,413 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,413 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,414 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,414 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:17,487 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,487 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,487 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,488 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,488 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,490 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,490 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:17,571 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,571 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,571 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:17,571 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:17,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/Mul ...\n",
      "2026-01-15 06:21:17,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:17,640 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,640 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,641 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,641 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,641 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,641 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,642 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,642 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:17,655 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,655 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,655 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,655 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,655 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:17,658 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,658 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,658 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:17,661 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,661 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,661 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:17,661 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:17,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,676 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,676 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,678 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,678 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:17,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,749 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,750 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/Mul ...\n",
      "2026-01-15 06:21:17,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:17,897 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,897 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,897 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,898 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,898 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,898 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,898 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:17,900 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,900 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,900 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,901 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:17,903 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,903 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,903 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:17,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:17,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,918 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,918 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,919 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,919 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:17,986 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,986 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:17,986 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:17,987 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,987 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:17,989 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:17,989 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/Mul ...\n",
      "2026-01-15 06:21:18,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:18,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:18,141 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:18,149 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:18,237 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,237 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,237 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,238 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,238 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,240 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,240 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/Mul ...\n",
      "2026-01-15 06:21:18,311 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:18,380 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,380 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,381 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,382 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,382 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:18,394 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,394 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,394 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,395 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,395 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:18,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,398 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,399 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:18,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,415 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,417 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,417 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:18,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,495 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,496 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,496 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,498 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,499 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/Mul ...\n",
      "2026-01-15 06:21:18,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,584 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,584 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:18,648 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,648 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,648 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,649 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,649 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,649 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,650 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,650 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:21:18,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,664 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:21:18,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,667 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,668 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:21:18,670 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/q_proj/Add ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/k_proj/Add ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/v_proj/Add ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:21:18,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:21:18,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,691 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:21:18,760 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,760 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,760 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,763 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,763 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/Mul ...\n",
      "2026-01-15 06:21:18,831 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:21:18,833 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,833 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:21:18,834 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,834 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:21:18,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:21:18,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:21:18,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/final_norm_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:21:18,902 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
      "2026-01-15 06:21:21,588 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /lm_head/MatMul with 4 bits ...\n",
      "\r",
      "0it [00:00, ?it/s]\r",
      "Saving model.layers.0.attn.k_proj.Add.bias (f32, [128]):   0%|          | 1/1134 [00:00<00:30, 36.91it/s]\r",
      "Saving model.layers.0.attn.v_proj.Add.bias (f32, [128]):   0%|          | 2/1134 [00:00<00:15, 73.18it/s]\r",
      "Saving model.layers.1.attn.k_proj.Add.bias (f32, [128]):   0%|          | 3/1134 [00:00<00:10, 109.15it/s]\r",
      "Saving model.layers.1.attn.v_proj.Add.bias (f32, [128]):   0%|          | 4/1134 [00:00<00:07, 144.83it/s]\r",
      "Saving model.layers.2.attn.k_proj.Add.bias (f32, [128]):   0%|          | 5/1134 [00:00<00:06, 180.27it/s]\r",
      "Saving model.layers.2.attn.v_proj.Add.bias (f32, [128]):   1%|          | 6/1134 [00:00<00:05, 215.53it/s]\r",
      "Saving model.layers.3.attn.k_proj.Add.bias (f32, [128]):   1%|          | 7/1134 [00:00<00:04, 250.52it/s]\r",
      "Saving model.layers.3.attn.v_proj.Add.bias (f32, [128]):   1%|          | 8/1134 [00:00<00:03, 285.28it/s]\r",
      "Saving model.layers.4.attn.k_proj.Add.bias (f32, [128]):   1%|          | 9/1134 [00:00<00:03, 319.86it/s]\r",
      "Saving model.layers.4.attn.v_proj.Add.bias (f32, [128]):   1%|          | 10/1134 [00:00<00:03, 353.99it/s]\r",
      "Saving model.layers.5.attn.k_proj.Add.bias (f32, [128]):   1%|          | 11/1134 [00:00<00:02, 388.11it/s]\r",
      "Saving model.layers.5.attn.v_proj.Add.bias (f32, [128]):   1%|          | 12/1134 [00:00<00:02, 421.95it/s]\r",
      "Saving model.layers.6.attn.k_proj.Add.bias (f32, [128]):   1%|          | 13/1134 [00:00<00:02, 455.45it/s]\r",
      "Saving model.layers.6.attn.v_proj.Add.bias (f32, [128]):   1%|          | 14/1134 [00:00<00:02, 488.90it/s]\r",
      "Saving model.layers.7.attn.k_proj.Add.bias (f32, [128]):   1%|▏         | 15/1134 [00:00<00:02, 522.10it/s]\r",
      "Saving model.layers.7.attn.v_proj.Add.bias (f32, [128]):   1%|▏         | 16/1134 [00:00<00:02, 554.83it/s]\r",
      "Saving model.layers.8.attn.k_proj.Add.bias (f32, [128]):   1%|▏         | 17/1134 [00:00<00:01, 587.58it/s]\r",
      "Saving model.layers.8.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 18/1134 [00:00<00:01, 619.95it/s]\r",
      "Saving model.layers.9.attn.k_proj.Add.bias (f32, [128]):   2%|▏         | 19/1134 [00:00<00:01, 652.38it/s]\r",
      "Saving model.layers.9.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 20/1134 [00:00<00:01, 683.32it/s]\r",
      "Saving model.layers.10.attn.k_proj.Add.bias (f32, [128]):   2%|▏         | 21/1134 [00:00<00:01, 714.98it/s]\r",
      "Saving model.layers.10.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 22/1134 [00:00<00:01, 747.56it/s]\r",
      "Saving model.layers.11.attn.k_proj.Add.bias (f32, [128]):   2%|▏         | 23/1134 [00:00<00:01, 780.18it/s]\r",
      "Saving model.layers.11.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 24/1134 [00:00<00:01, 812.34it/s]\r",
      "Saving model.layers.12.attn.k_proj.Add.bias (f32, [128]):   2%|▏         | 25/1134 [00:00<00:01, 844.71it/s]\r",
      "Saving model.layers.12.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 26/1134 [00:00<00:01, 877.00it/s]\r",
      "Saving model.layers.13.attn.k_proj.Add.bias (f32, [128]):   2%|▏         | 27/1134 [00:00<00:01, 909.17it/s]\r",
      "Saving model.layers.13.attn.v_proj.Add.bias (f32, [128]):   2%|▏         | 28/1134 [00:00<00:01, 941.18it/s]\r",
      "Saving model.layers.14.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 29/1134 [00:00<00:01, 972.98it/s]\r",
      "Saving model.layers.14.attn.v_proj.Add.bias (f32, [128]):   3%|▎         | 30/1134 [00:00<00:01, 1004.90it/s]\r",
      "Saving model.layers.15.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 31/1134 [00:00<00:01, 1036.70it/s]\r",
      "Saving model.layers.15.attn.v_proj.Add.bias (f32, [128]):   3%|▎         | 32/1134 [00:00<00:01, 1068.43it/s]\r",
      "Saving model.layers.16.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 33/1134 [00:00<00:01, 1100.05it/s]\r",
      "Saving model.layers.16.attn.v_proj.Add.bias (f32, [128]):   3%|▎         | 34/1134 [00:00<00:00, 1131.59it/s]\r",
      "Saving model.layers.17.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 35/1134 [00:00<00:00, 1163.03it/s]\r",
      "Saving model.layers.17.attn.v_proj.Add.bias (f32, [128]):   3%|▎         | 36/1134 [00:00<00:00, 1194.37it/s]\r",
      "Saving model.layers.18.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 37/1134 [00:00<00:00, 1223.51it/s]\r",
      "Saving model.layers.18.attn.v_proj.Add.bias (f32, [128]):   3%|▎         | 38/1134 [00:00<00:00, 1253.65it/s]\r",
      "Saving model.layers.19.attn.k_proj.Add.bias (f32, [128]):   3%|▎         | 39/1134 [00:00<00:00, 1283.03it/s]\r",
      "Saving model.layers.19.attn.v_proj.Add.bias (f32, [128]):   4%|▎         | 40/1134 [00:00<00:00, 1312.60it/s]\r",
      "Saving model.layers.20.attn.k_proj.Add.bias (f32, [128]):   4%|▎         | 41/1134 [00:00<00:00, 1341.61it/s]\r",
      "Saving model.layers.20.attn.v_proj.Add.bias (f32, [128]):   4%|▎         | 42/1134 [00:00<00:00, 1370.33it/s]\r",
      "Saving model.layers.21.attn.k_proj.Add.bias (f32, [128]):   4%|▍         | 43/1134 [00:00<00:00, 1398.71it/s]\r",
      "Saving model.layers.21.attn.v_proj.Add.bias (f32, [128]):   4%|▍         | 44/1134 [00:00<00:00, 1426.92it/s]\r",
      "Saving model.layers.22.attn.k_proj.Add.bias (f32, [128]):   4%|▍         | 45/1134 [00:00<00:00, 1455.05it/s]\r",
      "Saving model.layers.22.attn.v_proj.Add.bias (f32, [128]):   4%|▍         | 46/1134 [00:00<00:00, 1483.18it/s]\r",
      "Saving model.layers.23.attn.k_proj.Add.bias (f32, [128]):   4%|▍         | 47/1134 [00:00<00:00, 1511.10it/s]\r",
      "Saving model.layers.23.attn.v_proj.Add.bias (f32, [128]):   4%|▍         | 48/1134 [00:00<00:00, 1517.18it/s]\r",
      "Saving model.layers.0.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   4%|▍         | 49/1134 [00:00<00:00, 1533.04it/s]\r",
      "Saving model.layers.0.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   4%|▍         | 50/1134 [00:00<00:00, 1556.66it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   4%|▍         | 51/1134 [00:00<00:00, 1581.32it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▍         | 52/1134 [00:00<00:00, 1607.20it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▍         | 53/1134 [00:00<00:00, 1632.18it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▍         | 54/1134 [00:00<00:00, 1657.02it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▍         | 55/1134 [00:00<00:00, 1682.20it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▍         | 56/1134 [00:00<00:00, 1707.10it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 57/1134 [00:00<00:00, 1731.10it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 58/1134 [00:00<00:00, 1755.90it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 59/1134 [00:00<00:00, 1780.62it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 60/1134 [00:00<00:00, 1803.50it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 61/1134 [00:00<00:00, 1827.39it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   5%|▌         | 62/1134 [00:00<00:00, 1843.52it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 63/1134 [00:00<00:00, 1864.69it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 64/1134 [00:00<00:00, 1887.29it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 65/1134 [00:00<00:00, 1910.87it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 66/1134 [00:00<00:00, 1934.51it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 67/1134 [00:00<00:00, 1956.30it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 68/1134 [00:00<00:00, 1974.02it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 69/1134 [00:00<00:00, 1992.83it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▌         | 70/1134 [00:00<00:00, 2001.86it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▋         | 71/1134 [00:00<00:00, 2023.74it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▋         | 72/1134 [00:00<00:00, 2044.45it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   6%|▋         | 73/1134 [00:00<00:00, 2066.72it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 74/1134 [00:00<00:00, 2089.19it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 75/1134 [00:00<00:00, 2110.72it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 76/1134 [00:00<00:00, 2132.91it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 77/1134 [00:00<00:00, 2154.89it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 78/1134 [00:00<00:00, 2176.77it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 79/1134 [00:00<00:00, 2198.67it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 80/1134 [00:00<00:00, 2220.41it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 81/1134 [00:00<00:00, 2242.10it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 82/1134 [00:00<00:00, 2262.67it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 83/1134 [00:00<00:00, 2284.24it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 84/1134 [00:00<00:00, 2305.62it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   7%|▋         | 85/1134 [00:00<00:00, 2327.58it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 86/1134 [00:00<00:00, 2348.27it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 87/1134 [00:00<00:00, 2369.19it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 88/1134 [00:00<00:00, 2390.44it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 89/1134 [00:00<00:00, 2411.66it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 90/1134 [00:00<00:00, 2432.89it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 91/1134 [00:00<00:00, 2453.41it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 92/1134 [00:00<00:00, 2473.02it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 93/1134 [00:00<00:00, 2494.06it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 94/1134 [00:00<00:00, 2514.62it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 95/1134 [00:00<00:00, 2535.27it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_B.MatMul.weight_scales (f32, [128]):   8%|▊         | 96/1134 [00:00<00:00, 2555.51it/s]\r",
      "Saving model.layers.0.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▊         | 97/1134 [00:00<00:00, 2575.80it/s] \r",
      "Saving model.layers.0.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▊         | 98/1134 [00:00<00:00, 2595.58it/s]\r",
      "Saving model.layers.0.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▊         | 99/1134 [00:00<00:00, 2616.11it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 100/1134 [00:00<00:00, 2636.19it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 101/1134 [00:00<00:00, 2656.12it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 102/1134 [00:00<00:00, 2676.06it/s]  \r",
      "Saving model.layers.1.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 103/1134 [00:00<00:00, 2694.51it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 104/1134 [00:00<00:00, 2714.12it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 105/1134 [00:00<00:00, 2733.72it/s]\r",
      "Saving model.layers.1.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 106/1134 [00:00<00:00, 2753.22it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):   9%|▉         | 107/1134 [00:00<00:00, 2670.40it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 108/1134 [00:00<00:00, 2686.57it/s]  \r",
      "Saving model.layers.2.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 109/1134 [00:00<00:00, 2706.40it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 110/1134 [00:00<00:00, 2727.15it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 111/1134 [00:00<00:00, 2748.11it/s]\r",
      "Saving model.layers.2.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 112/1134 [00:00<00:00, 2769.19it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|▉         | 113/1134 [00:00<00:00, 2790.26it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 114/1134 [00:00<00:00, 2810.82it/s]  \r",
      "Saving model.layers.3.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 115/1134 [00:00<00:00, 2831.03it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 116/1134 [00:00<00:00, 2851.85it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 117/1134 [00:00<00:00, 2872.79it/s]\r",
      "Saving model.layers.3.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 118/1134 [00:00<00:00, 2893.32it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  10%|█         | 119/1134 [00:00<00:00, 2914.09it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 120/1134 [00:00<00:00, 2934.86it/s]  \r",
      "Saving model.layers.4.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 121/1134 [00:00<00:00, 2955.51it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 122/1134 [00:00<00:00, 2975.96it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 123/1134 [00:00<00:00, 2996.63it/s]\r",
      "Saving model.layers.4.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 124/1134 [00:00<00:00, 3017.29it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 125/1134 [00:00<00:00, 3037.87it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 126/1134 [00:00<00:00, 3055.85it/s]  \r",
      "Saving model.layers.5.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█         | 127/1134 [00:00<00:00, 3076.14it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█▏        | 128/1134 [00:00<00:00, 3096.59it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█▏        | 129/1134 [00:00<00:00, 3116.90it/s]\r",
      "Saving model.layers.5.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  11%|█▏        | 130/1134 [00:00<00:00, 3137.19it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 131/1134 [00:00<00:00, 3157.47it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 132/1134 [00:00<00:00, 3177.45it/s]  \r",
      "Saving model.layers.6.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 133/1134 [00:00<00:00, 3197.61it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 134/1134 [00:00<00:00, 3217.76it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 135/1134 [00:00<00:00, 3236.11it/s]\r",
      "Saving model.layers.6.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 136/1134 [00:00<00:00, 3255.18it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 137/1134 [00:00<00:00, 3274.95it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 138/1134 [00:00<00:00, 3294.65it/s]  \r",
      "Saving model.layers.7.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 139/1134 [00:00<00:00, 3314.24it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 140/1134 [00:00<00:00, 3333.97it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  12%|█▏        | 141/1134 [00:00<00:00, 3353.75it/s]\r",
      "Saving model.layers.7.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 142/1134 [00:00<00:00, 3373.48it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 143/1134 [00:00<00:00, 3372.06it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 144/1134 [00:00<00:00, 3386.35it/s]  \r",
      "Saving model.layers.8.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 145/1134 [00:00<00:00, 3403.67it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 146/1134 [00:00<00:00, 3421.66it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 147/1134 [00:00<00:00, 3439.83it/s]\r",
      "Saving model.layers.8.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 148/1134 [00:00<00:00, 3457.14it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 149/1134 [00:00<00:00, 3475.20it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 150/1134 [00:00<00:00, 3493.16it/s]  \r",
      "Saving model.layers.9.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 151/1134 [00:00<00:00, 3511.22it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 152/1134 [00:00<00:00, 3529.29it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  13%|█▎        | 153/1134 [00:00<00:00, 3547.36it/s]\r",
      "Saving model.layers.9.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▎        | 154/1134 [00:00<00:00, 3561.49it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▎        | 155/1134 [00:00<00:00, 3577.30it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 156/1134 [00:00<00:00, 3593.30it/s]  \r",
      "Saving model.layers.10.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 157/1134 [00:00<00:00, 3608.73it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 158/1134 [00:00<00:00, 3623.67it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 159/1134 [00:00<00:00, 3637.83it/s]\r",
      "Saving model.layers.10.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 160/1134 [00:00<00:00, 3652.48it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 161/1134 [00:00<00:00, 3667.44it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 162/1134 [00:00<00:00, 3682.12it/s]  \r",
      "Saving model.layers.11.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 163/1134 [00:00<00:00, 3696.92it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  14%|█▍        | 164/1134 [00:00<00:00, 3709.99it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 165/1134 [00:00<00:00, 3724.52it/s]\r",
      "Saving model.layers.11.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 166/1134 [00:00<00:00, 3738.98it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 167/1134 [00:00<00:00, 3753.52it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 168/1134 [00:00<00:00, 3768.23it/s]  \r",
      "Saving model.layers.12.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 169/1134 [00:00<00:00, 3782.52it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▍        | 170/1134 [00:00<00:00, 3794.36it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▌        | 171/1134 [00:00<00:00, 3808.71it/s]\r",
      "Saving model.layers.12.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▌        | 172/1134 [00:00<00:00, 3822.44it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▌        | 173/1134 [00:00<00:00, 3836.55it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▌        | 174/1134 [00:00<00:00, 3848.51it/s]  \r",
      "Saving model.layers.13.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  15%|█▌        | 175/1134 [00:00<00:00, 3862.89it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 176/1134 [00:00<00:00, 3876.82it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 177/1134 [00:00<00:00, 3890.78it/s]\r",
      "Saving model.layers.13.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 178/1134 [00:00<00:00, 3904.60it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 179/1134 [00:00<00:00, 3900.87it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 180/1134 [00:00<00:00, 3913.81it/s]  \r",
      "Saving model.layers.14.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 181/1134 [00:00<00:00, 3926.52it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 182/1134 [00:00<00:00, 3938.25it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 183/1134 [00:00<00:00, 3952.27it/s]\r",
      "Saving model.layers.14.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▌        | 184/1134 [00:00<00:00, 3965.80it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▋        | 185/1134 [00:00<00:00, 3979.37it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▋        | 186/1134 [00:00<00:00, 3992.08it/s]  \r",
      "Saving model.layers.15.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  16%|█▋        | 187/1134 [00:00<00:00, 4005.06it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 188/1134 [00:00<00:00, 4018.56it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 189/1134 [00:00<00:00, 4030.52it/s]\r",
      "Saving model.layers.15.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 190/1134 [00:00<00:00, 4043.85it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 191/1134 [00:00<00:00, 4056.47it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 192/1134 [00:00<00:00, 4068.27it/s]  \r",
      "Saving model.layers.16.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 193/1134 [00:00<00:00, 4081.67it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 194/1134 [00:00<00:00, 4094.87it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 195/1134 [00:00<00:00, 4108.04it/s]\r",
      "Saving model.layers.16.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 196/1134 [00:00<00:00, 4120.60it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 197/1134 [00:00<00:00, 4133.15it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  17%|█▋        | 198/1134 [00:00<00:00, 4145.98it/s]  \r",
      "Saving model.layers.17.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 199/1134 [00:00<00:00, 4158.84it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 200/1134 [00:00<00:00, 4171.47it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 201/1134 [00:00<00:00, 4184.31it/s]\r",
      "Saving model.layers.17.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 202/1134 [00:00<00:00, 4196.80it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 203/1134 [00:00<00:00, 4207.24it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 204/1134 [00:00<00:00, 4217.92it/s]  \r",
      "Saving model.layers.18.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 205/1134 [00:00<00:00, 4230.79it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 206/1134 [00:00<00:00, 4242.54it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 207/1134 [00:00<00:00, 4255.05it/s]\r",
      "Saving model.layers.18.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 208/1134 [00:00<00:00, 4266.63it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  18%|█▊        | 209/1134 [00:00<00:00, 4278.67it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▊        | 210/1134 [00:00<00:00, 4291.08it/s]  \r",
      "Saving model.layers.19.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▊        | 211/1134 [00:00<00:00, 4303.46it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▊        | 212/1134 [00:00<00:00, 4315.86it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 213/1134 [00:00<00:00, 4326.73it/s]\r",
      "Saving model.layers.19.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 214/1134 [00:00<00:00, 4339.54it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 215/1134 [00:00<00:00, 4332.08it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 216/1134 [00:00<00:00, 4342.87it/s]  \r",
      "Saving model.layers.20.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 217/1134 [00:00<00:00, 4353.58it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 218/1134 [00:00<00:00, 4365.06it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 219/1134 [00:00<00:00, 4376.58it/s]\r",
      "Saving model.layers.20.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 220/1134 [00:00<00:00, 4388.06it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  19%|█▉        | 221/1134 [00:00<00:00, 4396.77it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|█▉        | 222/1134 [00:00<00:00, 4408.18it/s]  \r",
      "Saving model.layers.21.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|█▉        | 223/1134 [00:00<00:00, 4419.46it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|█▉        | 224/1134 [00:00<00:00, 4430.28it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|█▉        | 225/1134 [00:00<00:00, 4441.61it/s]\r",
      "Saving model.layers.21.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|█▉        | 226/1134 [00:00<00:00, 4452.87it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 227/1134 [00:00<00:00, 4464.14it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 228/1134 [00:00<00:00, 4475.26it/s]  \r",
      "Saving model.layers.22.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 229/1134 [00:00<00:00, 4486.45it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 230/1134 [00:00<00:00, 4496.99it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 231/1134 [00:00<00:00, 4506.54it/s]\r",
      "Saving model.layers.22.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  20%|██        | 232/1134 [00:00<00:00, 4517.58it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 233/1134 [00:00<00:00, 4528.67it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 234/1134 [00:00<00:00, 4538.68it/s]  \r",
      "Saving model.layers.23.attn.q_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 235/1134 [00:00<00:00, 4549.72it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 236/1134 [00:00<00:00, 4560.50it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 237/1134 [00:00<00:00, 4571.35it/s]\r",
      "Saving model.layers.23.attn.o_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 238/1134 [00:00<00:00, 4582.17it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 239/1134 [00:00<00:00, 4593.01it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_A.MatMul.weight_scales (f32, [448]):  21%|██        | 240/1134 [00:00<00:00, 4603.77it/s]  \r",
      "Saving model.layers.0.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  21%|██▏       | 241/1134 [00:00<00:00, 4612.34it/s] \r",
      "Saving model.layers.0.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  21%|██▏       | 242/1134 [00:00<00:00, 4622.50it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  21%|██▏       | 243/1134 [00:00<00:00, 4633.01it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 244/1134 [00:00<00:00, 4642.83it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 245/1134 [00:00<00:00, 4653.50it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 246/1134 [00:00<00:00, 4664.02it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 247/1134 [00:00<00:00, 4674.38it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 248/1134 [00:00<00:00, 4684.56it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 249/1134 [00:00<00:00, 4694.69it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 250/1134 [00:00<00:00, 4686.71it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 251/1134 [00:00<00:00, 4693.50it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 252/1134 [00:00<00:00, 4702.26it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 253/1134 [00:00<00:00, 4711.61it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 254/1134 [00:00<00:00, 4721.62it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  22%|██▏       | 255/1134 [00:00<00:00, 4731.19it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 256/1134 [00:00<00:00, 4741.17it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 257/1134 [00:00<00:00, 4751.07it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 258/1134 [00:00<00:00, 4758.86it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 259/1134 [00:00<00:00, 4769.02it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 260/1134 [00:00<00:00, 4778.74it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 261/1134 [00:00<00:00, 4787.75it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 262/1134 [00:00<00:00, 4796.63it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 263/1134 [00:00<00:00, 4805.81it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 264/1134 [00:00<00:00, 4815.48it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 265/1134 [00:00<00:00, 4825.12it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  23%|██▎       | 266/1134 [00:00<00:00, 4834.85it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▎       | 267/1134 [00:00<00:00, 4844.48it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▎       | 268/1134 [00:00<00:00, 4852.76it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▎       | 269/1134 [00:00<00:00, 4862.80it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 270/1134 [00:00<00:00, 4872.23it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 271/1134 [00:00<00:00, 4881.77it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 272/1134 [00:00<00:00, 4890.92it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 273/1134 [00:00<00:00, 4900.37it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 274/1134 [00:00<00:00, 4909.64it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 275/1134 [00:00<00:00, 4918.55it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 276/1134 [00:00<00:00, 4928.03it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  24%|██▍       | 277/1134 [00:00<00:00, 4937.33it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 278/1134 [00:00<00:00, 4945.76it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 279/1134 [00:00<00:00, 4956.08it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 280/1134 [00:00<00:00, 4966.42it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 281/1134 [00:00<00:00, 4975.87it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 282/1134 [00:00<00:00, 4967.57it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▍       | 283/1134 [00:00<00:00, 4975.66it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▌       | 284/1134 [00:00<00:00, 4983.82it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▌       | 285/1134 [00:00<00:00, 4992.74it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▌       | 286/1134 [00:00<00:00, 5000.34it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▌       | 287/1134 [00:00<00:00, 5009.24it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  25%|██▌       | 288/1134 [00:00<00:00, 5018.15it/s]\r",
      "Saving model.layers.0.input_layernorm.weight (f32, [896]):  25%|██▌       | 289/1134 [00:00<00:00, 5027.12it/s]                  \r",
      "Saving model.layers.0.attn.q_proj.Add.bias (f32, [896]):  26%|██▌       | 290/1134 [00:00<00:00, 5035.70it/s]  \r",
      "Saving model.layers.0.post_attention_layernorm.weight (f32, [896]):  26%|██▌       | 291/1134 [00:00<00:00, 5044.59it/s]\r",
      "Saving model.layers.1.input_layernorm.weight (f32, [896]):  26%|██▌       | 292/1134 [00:00<00:00, 5053.48it/s]         \r",
      "Saving model.layers.1.attn.q_proj.Add.bias (f32, [896]):  26%|██▌       | 293/1134 [00:00<00:00, 5062.02it/s]  \r",
      "Saving model.layers.1.post_attention_layernorm.weight (f32, [896]):  26%|██▌       | 294/1134 [00:00<00:00, 5070.96it/s]\r",
      "Saving model.layers.2.input_layernorm.weight (f32, [896]):  26%|██▌       | 295/1134 [00:00<00:00, 5079.83it/s]         \r",
      "Saving model.layers.2.attn.q_proj.Add.bias (f32, [896]):  26%|██▌       | 296/1134 [00:00<00:00, 5087.25it/s]  \r",
      "Saving model.layers.2.post_attention_layernorm.weight (f32, [896]):  26%|██▌       | 297/1134 [00:00<00:00, 5095.96it/s]\r",
      "Saving model.layers.3.input_layernorm.weight (f32, [896]):  26%|██▋       | 298/1134 [00:00<00:00, 5104.08it/s]         \r",
      "Saving model.layers.3.attn.q_proj.Add.bias (f32, [896]):  26%|██▋       | 299/1134 [00:00<00:00, 5112.92it/s]  \r",
      "Saving model.layers.3.post_attention_layernorm.weight (f32, [896]):  26%|██▋       | 300/1134 [00:00<00:00, 5121.35it/s]\r",
      "Saving model.layers.4.input_layernorm.weight (f32, [896]):  27%|██▋       | 301/1134 [00:00<00:00, 5130.14it/s]         \r",
      "Saving model.layers.4.attn.q_proj.Add.bias (f32, [896]):  27%|██▋       | 302/1134 [00:00<00:00, 5138.83it/s]  \r",
      "Saving model.layers.4.post_attention_layernorm.weight (f32, [896]):  27%|██▋       | 303/1134 [00:00<00:00, 5132.50it/s]\r",
      "Saving model.layers.5.input_layernorm.weight (f32, [896]):  27%|██▋       | 304/1134 [00:00<00:00, 5140.60it/s]         \r",
      "Saving model.layers.5.attn.q_proj.Add.bias (f32, [896]):  27%|██▋       | 305/1134 [00:00<00:00, 5146.55it/s]  \r",
      "Saving model.layers.5.post_attention_layernorm.weight (f32, [896]):  27%|██▋       | 306/1134 [00:00<00:00, 5154.90it/s]\r",
      "Saving model.layers.6.input_layernorm.weight (f32, [896]):  27%|██▋       | 307/1134 [00:00<00:00, 5163.29it/s]         \r",
      "Saving model.layers.6.attn.q_proj.Add.bias (f32, [896]):  27%|██▋       | 308/1134 [00:00<00:00, 5171.11it/s]  \r",
      "Saving model.layers.6.post_attention_layernorm.weight (f32, [896]):  27%|██▋       | 309/1134 [00:00<00:00, 5179.54it/s]\r",
      "Saving model.layers.7.input_layernorm.weight (f32, [896]):  27%|██▋       | 310/1134 [00:00<00:00, 5187.93it/s]         \r",
      "Saving model.layers.7.attn.q_proj.Add.bias (f32, [896]):  27%|██▋       | 311/1134 [00:00<00:00, 5196.47it/s]  \r",
      "Saving model.layers.7.post_attention_layernorm.weight (f32, [896]):  28%|██▊       | 312/1134 [00:00<00:00, 5204.91it/s]\r",
      "Saving model.layers.8.input_layernorm.weight (f32, [896]):  28%|██▊       | 313/1134 [00:00<00:00, 5213.32it/s]         \r",
      "Saving model.layers.8.attn.q_proj.Add.bias (f32, [896]):  28%|██▊       | 314/1134 [00:00<00:00, 5221.70it/s]  \r",
      "Saving model.layers.8.post_attention_layernorm.weight (f32, [896]):  28%|██▊       | 315/1134 [00:00<00:00, 5228.61it/s]\r",
      "Saving model.layers.9.input_layernorm.weight (f32, [896]):  28%|██▊       | 316/1134 [00:00<00:00, 5236.62it/s]         \r",
      "Saving model.layers.9.attn.q_proj.Add.bias (f32, [896]):  28%|██▊       | 317/1134 [00:00<00:00, 5244.93it/s]  \r",
      "Saving model.layers.9.post_attention_layernorm.weight (f32, [896]):  28%|██▊       | 318/1134 [00:00<00:00, 5252.79it/s]\r",
      "Saving model.layers.10.input_layernorm.weight (f32, [896]):  28%|██▊       | 319/1134 [00:00<00:00, 5261.00it/s]        \r",
      "Saving model.layers.10.attn.q_proj.Add.bias (f32, [896]):  28%|██▊       | 320/1134 [00:00<00:00, 5269.25it/s]  \r",
      "Saving model.layers.10.post_attention_layernorm.weight (f32, [896]):  28%|██▊       | 321/1134 [00:00<00:00, 5260.25it/s]\r",
      "Saving model.layers.11.input_layernorm.weight (f32, [896]):  28%|██▊       | 322/1134 [00:00<00:00, 5266.68it/s]         \r",
      "Saving model.layers.11.attn.q_proj.Add.bias (f32, [896]):  28%|██▊       | 323/1134 [00:00<00:00, 5272.34it/s]  \r",
      "Saving model.layers.11.post_attention_layernorm.weight (f32, [896]):  29%|██▊       | 324/1134 [00:00<00:00, 5280.26it/s]\r",
      "Saving model.layers.12.input_layernorm.weight (f32, [896]):  29%|██▊       | 325/1134 [00:00<00:00, 5288.26it/s]         \r",
      "Saving model.layers.12.attn.q_proj.Add.bias (f32, [896]):  29%|██▊       | 326/1134 [00:00<00:00, 5295.80it/s]  \r",
      "Saving model.layers.12.post_attention_layernorm.weight (f32, [896]):  29%|██▉       | 327/1134 [00:00<00:00, 5303.11it/s]\r",
      "Saving model.layers.13.input_layernorm.weight (f32, [896]):  29%|██▉       | 328/1134 [00:00<00:00, 5311.09it/s]         \r",
      "Saving model.layers.13.attn.q_proj.Add.bias (f32, [896]):  29%|██▉       | 329/1134 [00:00<00:00, 5319.03it/s]  \r",
      "Saving model.layers.13.post_attention_layernorm.weight (f32, [896]):  29%|██▉       | 330/1134 [00:00<00:00, 5327.06it/s]\r",
      "Saving model.layers.14.input_layernorm.weight (f32, [896]):  29%|██▉       | 331/1134 [00:00<00:00, 5335.01it/s]         \r",
      "Saving model.layers.14.attn.q_proj.Add.bias (f32, [896]):  29%|██▉       | 332/1134 [00:00<00:00, 5343.04it/s]  \r",
      "Saving model.layers.14.post_attention_layernorm.weight (f32, [896]):  29%|██▉       | 333/1134 [00:00<00:00, 5349.39it/s]\r",
      "Saving model.layers.15.input_layernorm.weight (f32, [896]):  29%|██▉       | 334/1134 [00:00<00:00, 5356.71it/s]         \r",
      "Saving model.layers.15.attn.q_proj.Add.bias (f32, [896]):  30%|██▉       | 335/1134 [00:00<00:00, 5364.48it/s]  \r",
      "Saving model.layers.15.post_attention_layernorm.weight (f32, [896]):  30%|██▉       | 336/1134 [00:00<00:00, 5371.78it/s]\r",
      "Saving model.layers.16.input_layernorm.weight (f32, [896]):  30%|██▉       | 337/1134 [00:00<00:00, 5379.85it/s]         \r",
      "Saving model.layers.16.attn.q_proj.Add.bias (f32, [896]):  30%|██▉       | 338/1134 [00:00<00:00, 5387.65it/s]  \r",
      "Saving model.layers.16.post_attention_layernorm.weight (f32, [896]):  30%|██▉       | 339/1134 [00:00<00:00, 5379.12it/s]\r",
      "Saving model.layers.17.input_layernorm.weight (f32, [896]):  30%|██▉       | 340/1134 [00:00<00:00, 5386.03it/s]         \r",
      "Saving model.layers.17.attn.q_proj.Add.bias (f32, [896]):  30%|███       | 341/1134 [00:00<00:00, 5391.79it/s]  \r",
      "Saving model.layers.17.post_attention_layernorm.weight (f32, [896]):  30%|███       | 342/1134 [00:00<00:00, 5399.82it/s]\r",
      "Saving model.layers.18.input_layernorm.weight (f32, [896]):  30%|███       | 343/1134 [00:00<00:00, 5407.08it/s]         \r",
      "Saving model.layers.18.attn.q_proj.Add.bias (f32, [896]):  30%|███       | 344/1134 [00:00<00:00, 5414.06it/s]  \r",
      "Saving model.layers.18.post_attention_layernorm.weight (f32, [896]):  30%|███       | 345/1134 [00:00<00:00, 5421.70it/s]\r",
      "Saving model.layers.19.input_layernorm.weight (f32, [896]):  31%|███       | 346/1134 [00:00<00:00, 5428.85it/s]         \r",
      "Saving model.layers.19.attn.q_proj.Add.bias (f32, [896]):  31%|███       | 347/1134 [00:00<00:00, 5436.43it/s]  \r",
      "Saving model.layers.19.post_attention_layernorm.weight (f32, [896]):  31%|███       | 348/1134 [00:00<00:00, 5444.08it/s]\r",
      "Saving model.layers.20.input_layernorm.weight (f32, [896]):  31%|███       | 349/1134 [00:00<00:00, 5451.55it/s]         \r",
      "Saving model.layers.20.attn.q_proj.Add.bias (f32, [896]):  31%|███       | 350/1134 [00:00<00:00, 5459.20it/s]  \r",
      "Saving model.layers.20.post_attention_layernorm.weight (f32, [896]):  31%|███       | 351/1134 [00:00<00:00, 5465.73it/s]\r",
      "Saving model.layers.21.input_layernorm.weight (f32, [896]):  31%|███       | 352/1134 [00:00<00:00, 5473.62it/s]         \r",
      "Saving model.layers.21.attn.q_proj.Add.bias (f32, [896]):  31%|███       | 353/1134 [00:00<00:00, 5480.86it/s]  \r",
      "Saving model.layers.21.post_attention_layernorm.weight (f32, [896]):  31%|███       | 354/1134 [00:00<00:00, 5488.49it/s]\r",
      "Saving model.layers.22.input_layernorm.weight (f32, [896]):  31%|███▏      | 355/1134 [00:00<00:00, 5495.05it/s]         \r",
      "Saving model.layers.22.attn.q_proj.Add.bias (f32, [896]):  31%|███▏      | 356/1134 [00:00<00:00, 5502.45it/s]  \r",
      "Saving model.layers.22.post_attention_layernorm.weight (f32, [896]):  31%|███▏      | 357/1134 [00:00<00:00, 5459.22it/s]\r",
      "Saving model.layers.23.input_layernorm.weight (f32, [896]):  32%|███▏      | 358/1134 [00:00<00:00, 5465.51it/s]         \r",
      "Saving model.layers.23.attn.q_proj.Add.bias (f32, [896]):  32%|███▏      | 359/1134 [00:00<00:00, 5472.25it/s]  \r",
      "Saving model.layers.23.post_attention_layernorm.weight (f32, [896]):  32%|███▏      | 360/1134 [00:00<00:00, 5479.73it/s]\r",
      "Saving model.layers.24.final_norm_layernorm.weight (f32, [896]):  32%|███▏      | 361/1134 [00:00<00:00, 5486.68it/s]    \r",
      "Saving model.layers.0.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 362/1134 [00:00<00:00, 5493.58it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 363/1134 [00:00<00:00, 5500.70it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 364/1134 [00:00<00:00, 5507.79it/s]\r",
      "Saving model.layers.1.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 365/1134 [00:00<00:00, 5512.92it/s]  \r",
      "Saving model.layers.1.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 366/1134 [00:00<00:00, 5520.31it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 367/1134 [00:00<00:00, 5527.24it/s]\r",
      "Saving model.layers.2.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  32%|███▏      | 368/1134 [00:00<00:00, 5534.32it/s]  \r",
      "Saving model.layers.2.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 369/1134 [00:00<00:00, 5540.53it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 370/1134 [00:00<00:00, 5547.45it/s]\r",
      "Saving model.layers.3.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 371/1134 [00:00<00:00, 5553.80it/s]  \r",
      "Saving model.layers.3.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 372/1134 [00:00<00:00, 5560.26it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 373/1134 [00:00<00:00, 5567.29it/s]\r",
      "Saving model.layers.4.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 374/1134 [00:00<00:00, 5574.22it/s]  \r",
      "Saving model.layers.4.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 375/1134 [00:00<00:00, 5564.06it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 376/1134 [00:00<00:00, 5569.99it/s]\r",
      "Saving model.layers.5.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 377/1134 [00:00<00:00, 5575.60it/s]  \r",
      "Saving model.layers.5.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 378/1134 [00:00<00:00, 5582.13it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  33%|███▎      | 379/1134 [00:00<00:00, 5588.16it/s]\r",
      "Saving model.layers.6.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▎      | 380/1134 [00:00<00:00, 5594.68it/s]  \r",
      "Saving model.layers.6.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▎      | 381/1134 [00:00<00:00, 5601.42it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▎      | 382/1134 [00:00<00:00, 5608.16it/s]\r",
      "Saving model.layers.7.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 383/1134 [00:00<00:00, 5613.35it/s]  \r",
      "Saving model.layers.7.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 384/1134 [00:00<00:00, 5620.12it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 385/1134 [00:00<00:00, 5626.82it/s]\r",
      "Saving model.layers.8.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 386/1134 [00:00<00:00, 5633.58it/s]  \r",
      "Saving model.layers.8.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 387/1134 [00:00<00:00, 5639.97it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 388/1134 [00:00<00:00, 5646.66it/s]\r",
      "Saving model.layers.9.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 389/1134 [00:00<00:00, 5653.03it/s]  \r",
      "Saving model.layers.9.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 390/1134 [00:00<00:00, 5659.60it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  34%|███▍      | 391/1134 [00:00<00:00, 5665.86it/s]\r",
      "Saving model.layers.10.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▍      | 392/1134 [00:00<00:00, 5671.79it/s] \r",
      "Saving model.layers.10.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▍      | 393/1134 [00:00<00:00, 5661.17it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▍      | 394/1134 [00:00<00:00, 5664.91it/s]\r",
      "Saving model.layers.11.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▍      | 395/1134 [00:00<00:00, 5670.73it/s]  \r",
      "Saving model.layers.11.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▍      | 396/1134 [00:00<00:00, 5676.93it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 397/1134 [00:00<00:00, 5683.20it/s]\r",
      "Saving model.layers.12.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 398/1134 [00:00<00:00, 5689.61it/s]  \r",
      "Saving model.layers.12.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 399/1134 [00:00<00:00, 5695.87it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 400/1134 [00:00<00:00, 5700.31it/s]\r",
      "Saving model.layers.13.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 401/1134 [00:00<00:00, 5706.44it/s]  \r",
      "Saving model.layers.13.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  35%|███▌      | 402/1134 [00:00<00:00, 5712.57it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 403/1134 [00:00<00:00, 5718.95it/s]\r",
      "Saving model.layers.14.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 404/1134 [00:00<00:00, 5724.83it/s]  \r",
      "Saving model.layers.14.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 405/1134 [00:00<00:00, 5731.08it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 406/1134 [00:00<00:00, 5737.39it/s]\r",
      "Saving model.layers.15.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 407/1134 [00:00<00:00, 5743.50it/s]  \r",
      "Saving model.layers.15.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 408/1134 [00:00<00:00, 5749.81it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 409/1134 [00:00<00:00, 5756.07it/s]\r",
      "Saving model.layers.16.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 410/1134 [00:00<00:00, 5761.35it/s]  \r",
      "Saving model.layers.16.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▌      | 411/1134 [00:00<00:00, 5750.87it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▋      | 412/1134 [00:00<00:00, 5755.48it/s]\r",
      "Saving model.layers.17.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  36%|███▋      | 413/1134 [00:00<00:00, 5760.77it/s]  \r",
      "Saving model.layers.17.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 414/1134 [00:00<00:00, 5766.80it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 415/1134 [00:00<00:00, 5772.51it/s]\r",
      "Saving model.layers.18.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 416/1134 [00:00<00:00, 5778.62it/s]  \r",
      "Saving model.layers.18.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 417/1134 [00:00<00:00, 5784.63it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 418/1134 [00:00<00:00, 5788.97it/s]\r",
      "Saving model.layers.19.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 419/1134 [00:00<00:00, 5795.30it/s]  \r",
      "Saving model.layers.19.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 420/1134 [00:00<00:00, 5801.33it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 421/1134 [00:00<00:00, 5807.41it/s]\r",
      "Saving model.layers.20.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 422/1134 [00:00<00:00, 5812.57it/s]  \r",
      "Saving model.layers.20.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 423/1134 [00:00<00:00, 5818.58it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 424/1134 [00:00<00:00, 5824.58it/s]\r",
      "Saving model.layers.21.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  37%|███▋      | 425/1134 [00:00<00:00, 5830.24it/s]  \r",
      "Saving model.layers.21.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 426/1134 [00:00<00:00, 5836.31it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 427/1134 [00:00<00:00, 5842.34it/s]\r",
      "Saving model.layers.22.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 428/1134 [00:00<00:00, 5847.19it/s]  \r",
      "Saving model.layers.22.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 429/1134 [00:00<00:00, 5836.85it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 430/1134 [00:00<00:00, 5841.33it/s]\r",
      "Saving model.layers.23.attn.q_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 431/1134 [00:00<00:00, 5846.43it/s]  \r",
      "Saving model.layers.23.attn.o_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 432/1134 [00:00<00:00, 5852.12it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_B.MatMul.weight_scales (f32, [896]):  38%|███▊      | 433/1134 [00:00<00:00, 5857.93it/s]\r",
      "Saving model.layers.0.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  38%|███▊      | 434/1134 [00:00<00:00, 5863.57it/s]   \r",
      "Saving model.layers.0.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  38%|███▊      | 435/1134 [00:00<00:00, 5869.07it/s]\r",
      "Saving model.layers.0.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  38%|███▊      | 436/1134 [00:00<00:00, 5873.13it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▊      | 437/1134 [00:00<00:00, 5879.16it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▊      | 438/1134 [00:00<00:00, 5884.82it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▊      | 439/1134 [00:00<00:00, 5889.95it/s]  \r",
      "Saving model.layers.1.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 440/1134 [00:00<00:00, 5895.73it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 441/1134 [00:00<00:00, 5881.75it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 442/1134 [00:00<00:00, 5886.84it/s]\r",
      "Saving model.layers.1.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 443/1134 [00:00<00:00, 5889.91it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 444/1134 [00:00<00:00, 5895.45it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 445/1134 [00:00<00:00, 5900.77it/s]  \r",
      "Saving model.layers.2.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 446/1134 [00:00<00:00, 5906.26it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  39%|███▉      | 447/1134 [00:00<00:00, 5911.05it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 448/1134 [00:00<00:00, 5916.60it/s]\r",
      "Saving model.layers.2.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 449/1134 [00:00<00:00, 5922.11it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 450/1134 [00:00<00:00, 5911.67it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 451/1134 [00:00<00:00, 5912.95it/s]  \r",
      "Saving model.layers.3.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 452/1134 [00:00<00:00, 5917.91it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|███▉      | 453/1134 [00:00<00:00, 5923.33it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 454/1134 [00:00<00:00, 5928.51it/s]\r",
      "Saving model.layers.3.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 455/1134 [00:00<00:00, 5933.87it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 456/1134 [00:00<00:00, 5939.39it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 457/1134 [00:00<00:00, 5944.72it/s]  \r",
      "Saving model.layers.4.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 458/1134 [00:00<00:00, 5950.07it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 459/1134 [00:00<00:00, 5939.00it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 460/1134 [00:00<00:00, 5943.54it/s]\r",
      "Saving model.layers.4.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 461/1134 [00:00<00:00, 5948.27it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 462/1134 [00:00<00:00, 5953.17it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 463/1134 [00:00<00:00, 5958.22it/s]  \r",
      "Saving model.layers.5.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 464/1134 [00:00<00:00, 5963.46it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 465/1134 [00:00<00:00, 5968.37it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 466/1134 [00:00<00:00, 5973.73it/s]\r",
      "Saving model.layers.5.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 467/1134 [00:00<00:00, 5979.06it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████▏     | 468/1134 [00:00<00:00, 5967.65it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████▏     | 469/1134 [00:00<00:00, 5971.45it/s]  \r",
      "Saving model.layers.6.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████▏     | 470/1134 [00:00<00:00, 5975.61it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 471/1134 [00:00<00:00, 5980.51it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 472/1134 [00:00<00:00, 5985.69it/s]\r",
      "Saving model.layers.6.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 473/1134 [00:00<00:00, 5990.89it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 474/1134 [00:00<00:00, 5996.09it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 475/1134 [00:00<00:00, 6000.11it/s]  \r",
      "Saving model.layers.7.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 476/1134 [00:00<00:00, 6004.69it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 477/1134 [00:00<00:00, 5960.57it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 478/1134 [00:00<00:00, 5965.09it/s]\r",
      "Saving model.layers.7.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 479/1134 [00:00<00:00, 5966.82it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 480/1134 [00:00<00:00, 5971.90it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 481/1134 [00:00<00:00, 5976.62it/s]  \r",
      "Saving model.layers.8.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 482/1134 [00:00<00:00, 5981.28it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 483/1134 [00:00<00:00, 5986.23it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 484/1134 [00:00<00:00, 5991.35it/s]\r",
      "Saving model.layers.8.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 485/1134 [00:00<00:00, 5996.08it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 486/1134 [00:00<00:00, 5984.74it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 487/1134 [00:00<00:00, 5988.79it/s]  \r",
      "Saving model.layers.9.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 488/1134 [00:00<00:00, 5992.65it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 489/1134 [00:00<00:00, 5997.03it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 490/1134 [00:00<00:00, 6002.29it/s]\r",
      "Saving model.layers.9.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 491/1134 [00:00<00:00, 6007.16it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 492/1134 [00:00<00:00, 6012.22it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 493/1134 [00:00<00:00, 6017.20it/s]  \r",
      "Saving model.layers.10.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▎     | 494/1134 [00:00<00:00, 6022.25it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▎     | 495/1134 [00:00<00:00, 6011.26it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▎     | 496/1134 [00:00<00:00, 6015.29it/s]\r",
      "Saving model.layers.10.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 497/1134 [00:00<00:00, 6019.16it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 498/1134 [00:00<00:00, 6023.94it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 499/1134 [00:00<00:00, 6028.29it/s]  \r",
      "Saving model.layers.11.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 500/1134 [00:00<00:00, 6032.94it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 501/1134 [00:00<00:00, 6037.87it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 502/1134 [00:00<00:00, 6042.82it/s]\r",
      "Saving model.layers.11.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 503/1134 [00:00<00:00, 6046.19it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 504/1134 [00:00<00:00, 6025.20it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 505/1134 [00:00<00:00, 6028.37it/s]  \r",
      "Saving model.layers.12.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 506/1134 [00:00<00:00, 6031.56it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 507/1134 [00:00<00:00, 6035.43it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 508/1134 [00:00<00:00, 6039.66it/s]\r",
      "Saving model.layers.12.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 509/1134 [00:00<00:00, 6042.54it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 510/1134 [00:00<00:00, 6047.46it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 511/1134 [00:00<00:00, 6051.82it/s]  \r",
      "Saving model.layers.13.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 512/1134 [00:00<00:00, 6056.48it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 513/1134 [00:00<00:00, 6045.98it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 514/1134 [00:00<00:00, 6049.77it/s]\r",
      "Saving model.layers.13.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 515/1134 [00:00<00:00, 6053.79it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 516/1134 [00:00<00:00, 6056.64it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 517/1134 [00:00<00:00, 6061.30it/s]  \r",
      "Saving model.layers.14.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 518/1134 [00:00<00:00, 6065.84it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 519/1134 [00:00<00:00, 6070.60it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 520/1134 [00:00<00:00, 6075.26it/s]\r",
      "Saving model.layers.14.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 521/1134 [00:00<00:00, 6079.92it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 522/1134 [00:00<00:00, 6070.54it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 523/1134 [00:00<00:00, 6074.21it/s]  \r",
      "Saving model.layers.15.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 524/1134 [00:00<00:00, 6077.09it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▋     | 525/1134 [00:00<00:00, 6082.06it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▋     | 526/1134 [00:00<00:00, 6086.58it/s]\r",
      "Saving model.layers.15.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▋     | 527/1134 [00:00<00:00, 6091.16it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 528/1134 [00:00<00:00, 6095.78it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 529/1134 [00:00<00:00, 6099.98it/s]  \r",
      "Saving model.layers.16.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 530/1134 [00:00<00:00, 6104.17it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 531/1134 [00:00<00:00, 6094.97it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 532/1134 [00:00<00:00, 6095.91it/s]\r",
      "Saving model.layers.16.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 533/1134 [00:00<00:00, 6100.05it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 534/1134 [00:00<00:00, 6104.48it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 535/1134 [00:00<00:00, 6108.80it/s]  \r",
      "Saving model.layers.17.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 536/1134 [00:00<00:00, 6113.35it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 537/1134 [00:00<00:00, 6117.55it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 538/1134 [00:00<00:00, 6122.22it/s]\r",
      "Saving model.layers.17.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 539/1134 [00:00<00:00, 6126.74it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 540/1134 [00:00<00:00, 6116.85it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 541/1134 [00:00<00:00, 6120.51it/s]  \r",
      "Saving model.layers.18.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 542/1134 [00:00<00:00, 6124.39it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 543/1134 [00:00<00:00, 6128.82it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 544/1134 [00:00<00:00, 6132.57it/s]\r",
      "Saving model.layers.18.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 545/1134 [00:00<00:00, 6136.48it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 546/1134 [00:00<00:00, 6141.25it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 547/1134 [00:00<00:00, 6145.62it/s]  \r",
      "Saving model.layers.19.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 548/1134 [00:00<00:00, 6150.09it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 549/1134 [00:00<00:00, 6140.05it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▊     | 550/1134 [00:00<00:00, 6143.19it/s]\r",
      "Saving model.layers.19.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▊     | 551/1134 [00:00<00:00, 6146.64it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▊     | 552/1134 [00:00<00:00, 6150.89it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 553/1134 [00:00<00:00, 6154.98it/s]  \r",
      "Saving model.layers.20.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 554/1134 [00:00<00:00, 6159.33it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 555/1134 [00:00<00:00, 6163.70it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 556/1134 [00:00<00:00, 6168.19it/s]\r",
      "Saving model.layers.20.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 557/1134 [00:00<00:00, 6170.88it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 558/1134 [00:00<00:00, 6130.79it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 559/1134 [00:00<00:00, 6134.40it/s]  \r",
      "Saving model.layers.21.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 560/1134 [00:00<00:00, 6136.61it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 561/1134 [00:00<00:00, 6140.94it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 562/1134 [00:00<00:00, 6145.15it/s]\r",
      "Saving model.layers.21.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 563/1134 [00:00<00:00, 6149.40it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 564/1134 [00:00<00:00, 6153.51it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 565/1134 [00:00<00:00, 6157.80it/s]  \r",
      "Saving model.layers.22.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 566/1134 [00:00<00:00, 6161.82it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 567/1134 [00:00<00:00, 6152.09it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 568/1134 [00:00<00:00, 6154.66it/s]\r",
      "Saving model.layers.22.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 569/1134 [00:00<00:00, 6158.39it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 570/1134 [00:00<00:00, 6162.53it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 571/1134 [00:00<00:00, 6166.60it/s]  \r",
      "Saving model.layers.23.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 572/1134 [00:00<00:00, 6170.46it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 573/1134 [00:00<00:00, 6174.23it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 574/1134 [00:00<00:00, 6178.65it/s]\r",
      "Saving model.layers.23.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 575/1134 [00:00<00:00, 6182.88it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 576/1134 [00:00<00:00, 6173.42it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 577/1134 [00:00<00:00, 6176.85it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████     | 578/1134 [00:00<00:00, 6180.52it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████     | 579/1134 [00:00<00:00, 6184.48it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████     | 580/1134 [00:00<00:00, 6188.59it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████     | 581/1134 [00:00<00:00, 6191.93it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████▏    | 582/1134 [00:00<00:00, 6195.93it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████▏    | 583/1134 [00:00<00:00, 6182.14it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  51%|█████▏    | 584/1134 [00:00<00:00, 6185.63it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 585/1134 [00:00<00:00, 6189.04it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 586/1134 [00:00<00:00, 6193.05it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 587/1134 [00:00<00:00, 6197.20it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 588/1134 [00:00<00:00, 6200.91it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 589/1134 [00:00<00:00, 6191.89it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 590/1134 [00:00<00:00, 6192.56it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 591/1134 [00:00<00:00, 6196.33it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 592/1134 [00:00<00:00, 6200.30it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 593/1134 [00:00<00:00, 6204.14it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 594/1134 [00:00<00:00, 6208.01it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  52%|█████▏    | 595/1134 [00:00<00:00, 6199.80it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 596/1134 [00:00<00:00, 6203.10it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 597/1134 [00:00<00:00, 6205.64it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 598/1134 [00:00<00:00, 6209.65it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 599/1134 [00:00<00:00, 6213.61it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 600/1134 [00:00<00:00, 6217.59it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_A.MatMul.weight_scales (f32, [2432]):  53%|█████▎    | 601/1134 [00:00<00:00, 6209.25it/s]\r",
      "Saving model.layers.0.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  53%|█████▎    | 602/1134 [00:00<00:00, 6212.48it/s]    \r",
      "Saving model.layers.0.attn.k_proj.MatMul.weight_scales (f32, [3584]):  53%|█████▎    | 603/1134 [00:00<00:00, 6215.80it/s]      \r",
      "Saving model.layers.0.attn.v_proj.MatMul.weight_scales (f32, [3584]):  53%|█████▎    | 604/1134 [00:00<00:00, 6219.72it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  53%|█████▎    | 605/1134 [00:00<00:00, 6222.16it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  53%|█████▎    | 606/1134 [00:00<00:00, 6214.68it/s]\r",
      "Saving model.layers.1.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▎    | 607/1134 [00:00<00:00, 6217.68it/s]  \r",
      "Saving model.layers.1.attn.k_proj.MatMul.weight_scales (f32, [3584]):  54%|█████▎    | 608/1134 [00:00<00:00, 6221.09it/s]      \r",
      "Saving model.layers.1.attn.v_proj.MatMul.weight_scales (f32, [3584]):  54%|█████▎    | 609/1134 [00:00<00:00, 6224.50it/s]\r",
      "Saving model.layers.1.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 610/1134 [00:00<00:00, 6216.73it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 611/1134 [00:00<00:00, 6217.07it/s]\r",
      "Saving model.layers.2.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 612/1134 [00:00<00:00, 6220.56it/s]  \r",
      "Saving model.layers.2.attn.k_proj.MatMul.weight_scales (f32, [3584]):  54%|█████▍    | 613/1134 [00:00<00:00, 6224.37it/s]      \r",
      "Saving model.layers.2.attn.v_proj.MatMul.weight_scales (f32, [3584]):  54%|█████▍    | 614/1134 [00:00<00:00, 6188.22it/s]\r",
      "Saving model.layers.2.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 615/1134 [00:00<00:00, 6191.25it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 616/1134 [00:00<00:00, 6194.92it/s]\r",
      "Saving model.layers.3.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 617/1134 [00:00<00:00, 6199.06it/s]  \r",
      "Saving model.layers.3.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  54%|█████▍    | 618/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.k_proj.MatMul.weight_scales (f32, [3584]):  54%|█████▍    | 618/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.3.attn.v_proj.MatMul.weight_scales (f32, [3584]):  55%|█████▍    | 619/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▍    | 620/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▍    | 621/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▍    | 622/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.4.attn.k_proj.MatMul.weight_scales (f32, [3584]):  55%|█████▍    | 623/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.4.attn.v_proj.MatMul.weight_scales (f32, [3584]):  55%|█████▌    | 624/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▌    | 625/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▌    | 626/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  55%|█████▌    | 627/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.5.attn.k_proj.MatMul.weight_scales (f32, [3584]):  55%|█████▌    | 628/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.5.attn.v_proj.MatMul.weight_scales (f32, [3584]):  55%|█████▌    | 629/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 630/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 631/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 632/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.6.attn.k_proj.MatMul.weight_scales (f32, [3584]):  56%|█████▌    | 633/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.6.attn.v_proj.MatMul.weight_scales (f32, [3584]):  56%|█████▌    | 634/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 635/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 636/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▌    | 637/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.7.attn.k_proj.MatMul.weight_scales (f32, [3584]):  56%|█████▋    | 638/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.7.attn.v_proj.MatMul.weight_scales (f32, [3584]):  56%|█████▋    | 639/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  56%|█████▋    | 640/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 641/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 642/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.8.attn.k_proj.MatMul.weight_scales (f32, [3584]):  57%|█████▋    | 643/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.8.attn.v_proj.MatMul.weight_scales (f32, [3584]):  57%|█████▋    | 644/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 645/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 646/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 647/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.9.attn.k_proj.MatMul.weight_scales (f32, [3584]):  57%|█████▋    | 648/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.9.attn.v_proj.MatMul.weight_scales (f32, [3584]):  57%|█████▋    | 649/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 650/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 651/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  57%|█████▋    | 652/1134 [00:00<00:00, 6169.86it/s] \r",
      "Saving model.layers.10.attn.k_proj.MatMul.weight_scales (f32, [3584]):  58%|█████▊    | 653/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.10.attn.v_proj.MatMul.weight_scales (f32, [3584]):  58%|█████▊    | 654/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 655/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 656/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 657/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.11.attn.k_proj.MatMul.weight_scales (f32, [3584]):  58%|█████▊    | 658/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.11.attn.v_proj.MatMul.weight_scales (f32, [3584]):  58%|█████▊    | 659/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 660/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 661/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  58%|█████▊    | 662/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.12.attn.k_proj.MatMul.weight_scales (f32, [3584]):  58%|█████▊    | 663/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.12.attn.v_proj.MatMul.weight_scales (f32, [3584]):  59%|█████▊    | 664/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▊    | 665/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▊    | 666/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▉    | 667/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.13.attn.k_proj.MatMul.weight_scales (f32, [3584]):  59%|█████▉    | 668/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.13.attn.v_proj.MatMul.weight_scales (f32, [3584]):  59%|█████▉    | 669/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▉    | 670/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▉    | 671/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  59%|█████▉    | 672/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.14.attn.k_proj.MatMul.weight_scales (f32, [3584]):  59%|█████▉    | 673/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.14.attn.v_proj.MatMul.weight_scales (f32, [3584]):  59%|█████▉    | 674/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|█████▉    | 675/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|█████▉    | 676/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|█████▉    | 677/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.15.attn.k_proj.MatMul.weight_scales (f32, [3584]):  60%|█████▉    | 678/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.15.attn.v_proj.MatMul.weight_scales (f32, [3584]):  60%|█████▉    | 679/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|█████▉    | 680/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|██████    | 681/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|██████    | 682/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.16.attn.k_proj.MatMul.weight_scales (f32, [3584]):  60%|██████    | 683/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.16.attn.v_proj.MatMul.weight_scales (f32, [3584]):  60%|██████    | 684/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|██████    | 685/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  60%|██████    | 686/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████    | 687/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.17.attn.k_proj.MatMul.weight_scales (f32, [3584]):  61%|██████    | 688/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.17.attn.v_proj.MatMul.weight_scales (f32, [3584]):  61%|██████    | 689/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████    | 690/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████    | 691/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████    | 692/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.18.attn.k_proj.MatMul.weight_scales (f32, [3584]):  61%|██████    | 693/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.18.attn.v_proj.MatMul.weight_scales (f32, [3584]):  61%|██████    | 694/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████▏   | 695/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████▏   | 696/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  61%|██████▏   | 697/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.19.attn.k_proj.MatMul.weight_scales (f32, [3584]):  62%|██████▏   | 698/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.19.attn.v_proj.MatMul.weight_scales (f32, [3584]):  62%|██████▏   | 699/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 700/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 701/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 702/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.20.attn.k_proj.MatMul.weight_scales (f32, [3584]):  62%|██████▏   | 703/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.20.attn.v_proj.MatMul.weight_scales (f32, [3584]):  62%|██████▏   | 704/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 705/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 706/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 707/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.21.attn.k_proj.MatMul.weight_scales (f32, [3584]):  62%|██████▏   | 708/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.21.attn.v_proj.MatMul.weight_scales (f32, [3584]):  63%|██████▎   | 709/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 710/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 711/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 712/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.22.attn.k_proj.MatMul.weight_scales (f32, [3584]):  63%|██████▎   | 713/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.22.attn.v_proj.MatMul.weight_scales (f32, [3584]):  63%|██████▎   | 714/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 715/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 716/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 717/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.23.attn.k_proj.MatMul.weight_scales (f32, [3584]):  63%|██████▎   | 718/1134 [00:00<00:00, 6169.86it/s]      \r",
      "Saving model.layers.23.attn.v_proj.MatMul.weight_scales (f32, [3584]):  63%|██████▎   | 719/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 720/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▎   | 721/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▎   | 722/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 723/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.1.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 724/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 725/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.2.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 726/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 727/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.3.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 728/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 729/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.4.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 730/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  64%|██████▍   | 731/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.5.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 732/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 733/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.6.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 734/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 735/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.7.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 736/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▍   | 737/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.8.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▌   | 738/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▌   | 739/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.9.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▌   | 740/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▌   | 741/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.10.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  65%|██████▌   | 742/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 743/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.11.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 744/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 745/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.12.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 746/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 747/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.13.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 748/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 749/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.14.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 750/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▌   | 751/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.15.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▋   | 752/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▋   | 753/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.16.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  66%|██████▋   | 754/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 755/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.17.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 756/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 757/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.18.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 758/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 759/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.19.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 760/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 761/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.20.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 762/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 763/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.21.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 764/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  67%|██████▋   | 765/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.22.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  68%|██████▊   | 766/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  68%|██████▊   | 767/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.23.mlp.gate_proj.lora_B.MatMul.weight_scales (f32, [4864]):  68%|██████▊   | 768/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_B.MatMul.weight_scales (f32, [4864]):  68%|██████▊   | 769/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 770/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 771/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 772/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 773/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 774/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 775/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 776/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 777/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 778/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 779/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 780/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 781/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 782/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 783/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 784/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 785/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 786/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 787/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 788/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 789/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 790/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 791/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 792/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 793/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 794/1134 [00:00<00:00, 6169.86it/s]          \r",
      "Saving model.layers.0.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 795/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 796/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 797/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 798/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  70%|███████   | 799/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 800/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 801/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 802/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 803/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 804/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 805/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 806/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████   | 807/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████▏  | 808/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████▏  | 809/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  71%|███████▏  | 810/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 811/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 812/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 813/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 814/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 815/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 816/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 817/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 818/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 819/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 820/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 821/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  72%|███████▏  | 822/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 823/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 824/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 825/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 826/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 827/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 828/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 829/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 830/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 831/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 832/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  73%|███████▎  | 833/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▎  | 834/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▎  | 835/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▎  | 836/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 837/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 838/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 839/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 840/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 841/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  74%|███████▍  | 842/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  74%|███████▍  | 843/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.1.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  74%|███████▍  | 844/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 845/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.2.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 846/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 847/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.3.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 848/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 849/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.4.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▍  | 850/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 851/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.5.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 852/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 853/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.6.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 854/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 855/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.7.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  75%|███████▌  | 856/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 857/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.8.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 858/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 859/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.9.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 860/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 861/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.10.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 862/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 863/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.11.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▌  | 864/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▋  | 865/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.12.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▋  | 866/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  76%|███████▋  | 867/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.13.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 868/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 869/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.14.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 870/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 871/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.15.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 872/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 873/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.16.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 874/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 875/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.17.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 876/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 877/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.18.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  77%|███████▋  | 878/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 879/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.19.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 880/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 881/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.20.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 882/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 883/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.21.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 884/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 885/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.22.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 886/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 887/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.23.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 888/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 889/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.attn.q_proj.MatMul.weight_scales (f32, [25088]):  78%|███████▊  | 890/1134 [00:00<00:00, 6169.86it/s]       \r",
      "Saving model.layers.0.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▊  | 891/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.q_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▊  | 892/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▊  | 893/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.q_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 894/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 895/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.q_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 896/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 897/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.q_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 898/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 899/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.q_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 900/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.o_proj.MatMul.weight_scales (f32, [25088]):  79%|███████▉  | 901/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 902/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.o_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 903/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 904/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.o_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 905/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 906/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.o_proj.MatMul.weight_scales (f32, [25088]):  80%|███████▉  | 907/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|████████  | 908/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.o_proj.MatMul.weight_scales (f32, [25088]):  80%|████████  | 909/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|████████  | 910/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.o_proj.MatMul.weight_scales (f32, [25088]):  80%|████████  | 911/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.q_proj.MatMul.weight_scales (f32, [25088]):  80%|████████  | 912/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 913/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 914/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 915/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 916/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 917/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 918/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 919/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 920/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████  | 921/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████▏ | 922/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.o_proj.MatMul.weight_scales (f32, [25088]):  81%|████████▏ | 923/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.q_proj.MatMul.weight_scales (f32, [25088]):  81%|████████▏ | 924/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 925/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.q_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 926/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 927/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.q_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 928/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 929/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.q_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 930/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 931/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.q_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 932/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 933/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.q_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 934/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.o_proj.MatMul.weight_scales (f32, [25088]):  82%|████████▏ | 935/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.q_proj.MatMul.weight_scales (f32, [25088]):  83%|████████▎ | 936/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.o_proj.MatMul.weight_scales (f32, [25088]):  83%|████████▎ | 937/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 938/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 939/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 940/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 941/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 942/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 943/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 944/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 945/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  83%|████████▎ | 946/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▎ | 947/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▎ | 948/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▎ | 949/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 950/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 951/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 952/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 953/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 954/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 955/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 956/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 957/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  84%|████████▍ | 958/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▍ | 959/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▍ | 960/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▍ | 961/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▍ | 962/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▍ | 963/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 964/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 965/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 966/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 967/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 968/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  85%|████████▌ | 969/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 970/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 971/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 972/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 973/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 974/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 975/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 976/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 977/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▌ | 978/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▋ | 979/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  86%|████████▋ | 980/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  87%|████████▋ | 981/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  87%|████████▋ | 982/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  87%|████████▋ | 983/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  87%|████████▋ | 984/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  87%|████████▋ | 985/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 986/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 987/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 988/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 989/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 990/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.1.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 991/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  87%|████████▋ | 992/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 993/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.2.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 994/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 995/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 996/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.3.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 997/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 998/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 999/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.4.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 1000/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 1001/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 1002/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.5.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  88%|████████▊ | 1003/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▊ | 1004/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▊ | 1005/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.6.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▊ | 1006/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1007/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1008/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.7.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1009/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1010/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1011/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.8.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1012/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1013/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  89%|████████▉ | 1014/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.9.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1015/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1016/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1017/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.10.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1018/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1019/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  90%|████████▉ | 1020/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.11.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1021/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1022/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1023/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.12.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1024/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1025/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  90%|█████████ | 1026/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.13.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1027/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1028/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1029/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.14.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1030/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1031/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1032/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.15.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1033/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████ | 1034/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████▏| 1035/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.16.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████▏| 1036/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  91%|█████████▏| 1037/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1038/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.17.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1039/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1040/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1041/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.18.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1042/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1043/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1044/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.19.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1045/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1046/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1047/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.20.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  92%|█████████▏| 1048/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1049/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1050/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.21.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1051/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1052/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1053/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.22.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1054/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1055/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.up_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1056/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.23.mlp.down_proj.MatMul.weight_scales (f32, [136192]):  93%|█████████▎| 1057/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  93%|█████████▎| 1058/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  93%|█████████▎| 1059/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  93%|█████████▎| 1060/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▎| 1061/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.1.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▎| 1062/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.1.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▎| 1063/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1064/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.2.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1065/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.2.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▍| 1066/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1067/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.3.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1068/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.3.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▍| 1069/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1070/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.4.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1071/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.4.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▍| 1072/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1073/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.5.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1074/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.5.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▍| 1075/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1076/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.6.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1077/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.6.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▌| 1078/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1079/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.7.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1080/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.7.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▌| 1081/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1082/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.8.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1083/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.8.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1084/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1085/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.9.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1086/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.9.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1087/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1088/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.10.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1089/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.10.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1090/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1091/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.11.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▋| 1092/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.11.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▋| 1093/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▋| 1094/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.12.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1095/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.12.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1096/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1097/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.13.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1098/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.13.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1099/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1100/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.14.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1101/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.14.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1102/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1103/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.15.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1104/1134 [00:00<00:00, 6169.86it/s]  \r",
      "Saving model.layers.15.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1105/1134 [00:00<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1106/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.16.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1107/1134 [00:01<00:00, 6169.86it/s]  \r",
      "Saving model.layers.16.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1108/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1109/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.17.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1110/1134 [00:01<00:00, 6169.86it/s]  \r",
      "Saving model.layers.17.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1111/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1112/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.18.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1113/1134 [00:01<00:00, 6169.86it/s]  \r",
      "Saving model.layers.18.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1114/1134 [00:01<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1115/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.19.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1116/1134 [00:04<00:00, 6169.86it/s]  \r",
      "Saving model.layers.19.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▊| 1117/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▊| 1118/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.20.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▊| 1119/1134 [00:04<00:00, 6169.86it/s]  \r",
      "Saving model.layers.20.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1120/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1121/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.21.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1122/1134 [00:04<00:00, 6169.86it/s]  \r",
      "Saving model.layers.21.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1123/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1124/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.22.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1125/1134 [00:04<00:00, 6169.86it/s]  \r",
      "Saving model.layers.22.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1126/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1127/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving model.layers.23.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1128/1134 [00:04<00:00, 6169.86it/s]  \r",
      "Saving model.layers.23.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]): 100%|█████████▉| 1129/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving cos_cache (f32, [32768,32]): 100%|█████████▉| 1130/1134 [00:04<00:00, 6169.86it/s]                                      \r",
      "Saving sin_cache (f32, [32768,32]): 100%|█████████▉| 1131/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving lm_head.MatMul.weight_scales (f32, [4254208]): 100%|█████████▉| 1132/1134 [00:04<00:00, 6169.86it/s]\r",
      "Saving lm_head.MatMul.weight_Q4 (u8, [151936,28,16]): 100%|█████████▉| 1133/1134 [00:05<00:00, 6169.86it/s]\r",
      "Saving model.embed_tokens.weight (f32, [151936,896]): 100%|██████████| 1134/1134 [00:05<00:00, 6169.86it/s]\r",
      "Saving model.embed_tokens.weight (f32, [151936,896]): 100%|██████████| 1134/1134 [00:19<00:00, 6169.86it/s]\r",
      "Saving model.embed_tokens.weight (f32, [151936,896]): 100%|██████████| 1134/1134 [00:37<00:00, 30.31it/s]  \n",
      "2026-01-15 06:22:19,232 numexpr.utils [INFO] - NumExpr defaulting to 2 threads.\n",
      "2026-01-15 06:22:23.323636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768458143.354573   16710 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768458143.363119   16710 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768458143.395024   16710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458143.395051   16710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458143.395054   16710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768458143.395057   16710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-15 06:22:23.399667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[1] ...\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/ReduceSum ...\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub ...\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast ...\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Shape ...\n",
      "2026-01-15 06:22:46,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/1 ...\n",
      "2026-01-15 06:22:46,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather ...\n",
      "2026-01-15 06:22:46,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast ...\n",
      "2026-01-15 06:22:46,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Gather ...\n",
      "2026-01-15 06:22:46,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm/LayerNorm ...\n",
      "2026-01-15 06:22:46,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,099 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:46,125 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,125 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,125 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,126 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,126 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,127 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:46,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,132 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:46,135 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:46,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:46,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:46,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,167 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,168 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:46,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,328 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,328 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:46,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:46,473 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:46,473 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/Mul ...\n",
      "2026-01-15 06:22:46,473 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,477 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,477 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:46,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:46,572 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,573 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,574 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:46,589 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,589 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,589 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,590 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,590 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,590 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,590 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:46,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,593 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,594 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,597 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,598 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,598 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:46,613 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,613 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,613 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:46,613 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,616 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,616 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:46,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,719 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,720 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:46,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,816 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:46,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:46,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/Mul ...\n",
      "2026-01-15 06:22:46,817 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:46,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,904 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:46,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,905 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:46,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,925 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,925 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:46,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,927 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,928 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,928 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,928 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,928 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,931 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,932 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,932 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:46,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:46,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:46,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:46,950 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,951 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:46,953 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:46,953 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:47,046 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,046 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,046 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,047 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,047 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/Mul ...\n",
      "2026-01-15 06:22:47,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,142 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,225 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,225 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:47,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,241 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,242 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,242 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:47,244 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,244 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,244 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:47,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,249 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,249 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,249 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,249 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:47,267 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,267 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,267 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,267 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,267 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,268 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:47,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,364 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,365 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:47,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:47,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:47,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/Mul ...\n",
      "2026-01-15 06:22:47,454 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,455 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,455 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,456 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,456 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:47,537 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,538 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,538 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,538 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,538 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,538 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:47,555 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,555 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,555 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,556 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,557 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,557 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:47,559 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,559 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,559 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,560 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,560 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,560 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,560 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:47,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:47,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,580 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,581 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,583 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:47,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,672 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,673 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,675 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:47,774 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,774 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:47,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:47,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/Mul ...\n",
      "2026-01-15 06:22:47,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,777 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,777 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:47,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,863 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:47,881 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,881 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,881 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,882 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,882 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,882 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,882 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:47,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,886 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,886 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,886 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,886 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:47,888 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:47,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,906 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:47,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,909 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:47,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:47,997 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:47,998 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:47,998 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/Mul ...\n",
      "2026-01-15 06:22:48,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:48,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,174 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,175 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,175 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,176 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,176 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:48,191 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,191 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,191 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,192 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,192 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,192 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,192 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:48,195 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,195 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,195 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:48,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:48,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,219 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,219 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:48,309 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,309 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,309 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,310 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,310 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,312 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,312 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:48,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:48,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:48,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/Mul ...\n",
      "2026-01-15 06:22:48,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,405 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,492 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,492 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:48,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,508 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,509 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:48,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,512 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,513 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,513 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:48,515 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,515 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,515 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:48,515 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:48,515 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:48,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:48,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,517 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,517 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,533 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,536 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,536 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:48,625 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,625 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,625 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,625 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,626 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/Mul ...\n",
      "2026-01-15 06:22:48,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,729 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,729 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:48,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,821 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:48,839 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,839 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,839 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,840 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,840 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,840 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,840 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:48,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,844 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,844 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:48,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:48,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:48,846 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:48,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:48,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:48,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:48,865 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,866 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,866 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:48,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:48,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:48,957 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,957 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:48,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:48,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/Mul ...\n",
      "2026-01-15 06:22:49,050 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,053 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,053 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:49,137 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,138 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:49,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,156 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:49,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:49,162 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,163 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:49,179 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,179 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,179 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,179 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,182 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,182 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:49,269 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,270 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,273 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,273 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:49,365 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,365 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,366 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:49,366 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:49,366 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/Mul ...\n",
      "2026-01-15 06:22:49,366 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,369 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:49,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:49,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:49,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,473 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,473 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,474 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,474 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:49,476 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,477 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,477 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,478 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,494 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,497 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,497 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:49,586 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,586 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,586 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,587 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,587 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,589 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,589 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/Mul ...\n",
      "2026-01-15 06:22:49,679 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:49,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,775 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,776 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,777 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,778 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:49,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,795 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,795 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,795 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,796 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:49,798 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,798 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,798 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,799 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,799 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,799 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,799 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:49,802 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,804 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,804 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,820 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,822 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:49,915 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,915 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:49,915 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:49,916 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,916 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:49,919 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:49,919 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:50,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:50,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:50,013 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/Mul ...\n",
      "2026-01-15 06:22:50,014 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,015 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,015 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,016 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:50,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,103 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,103 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,104 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,104 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:50,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,121 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,121 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,122 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,122 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:50,124 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,124 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,124 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,125 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,125 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,126 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,126 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:50,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,128 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,129 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,130 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:50,147 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,147 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,147 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,147 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,148 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,148 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,150 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:50,242 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,242 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,245 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/Mul ...\n",
      "2026-01-15 06:22:50,339 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,341 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,341 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,342 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,342 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:50,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,426 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:50,444 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,444 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,444 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:50,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:50,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,453 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:50,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,469 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:50,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,564 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,565 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,567 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,567 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/Mul ...\n",
      "2026-01-15 06:22:50,656 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,658 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,658 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,659 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,741 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,741 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:50,757 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,757 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,757 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,758 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,758 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,758 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,759 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:50,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,761 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,762 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,762 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,762 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,762 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:50,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,766 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,766 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,767 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,767 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:50,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:50,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,787 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,787 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,790 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,790 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:50,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,889 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,890 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,892 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,892 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/Mul ...\n",
      "2026-01-15 06:22:50,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:50,984 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,984 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:50,985 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:50,985 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:51,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,074 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,074 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:51,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,093 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:51,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,097 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:51,100 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,101 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,101 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:51,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,120 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,122 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,122 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:51,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,216 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,220 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,220 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:51,317 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:51,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:51,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/Mul ...\n",
      "2026-01-15 06:22:51,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,320 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,321 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,321 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:51,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,407 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,407 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:51,423 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,423 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,423 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,424 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:51,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:51,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,432 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,433 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:51,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:51,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,540 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,540 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/Mul ...\n",
      "2026-01-15 06:22:51,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,639 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,639 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,639 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,640 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:51,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,725 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,725 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:51,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,744 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,744 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:51,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:51,751 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,753 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,753 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:51,767 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:51,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,769 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,769 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,771 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,771 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:51,876 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,876 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,876 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,879 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,879 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:51,970 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:51,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:51,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:51,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/Mul ...\n",
      "2026-01-15 06:22:51,971 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:51,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,973 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:51,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:51,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:52,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,057 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,058 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,059 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:52,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,075 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,076 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,076 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,076 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:52,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,080 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,080 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,080 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,080 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:52,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,084 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,084 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:52,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,102 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,103 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,103 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,105 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,105 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:52,197 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,197 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,197 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,198 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,198 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,200 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/Mul ...\n",
      "2026-01-15 06:22:52,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,298 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,298 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,299 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,299 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:52,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.18/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,383 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:52,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:52,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,406 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,407 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,407 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,407 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,408 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:52,410 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,410 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,410 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,411 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,412 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,412 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:52,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,427 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,428 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,431 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:52,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,522 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,523 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,523 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,525 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,526 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/Mul ...\n",
      "2026-01-15 06:22:52,615 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,617 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,617 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,618 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,618 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:52,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,704 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.19/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,704 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,704 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,705 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,705 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,706 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:52,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,724 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,725 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,725 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,726 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:52,729 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,729 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,729 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,730 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,731 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:52,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:52,733 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:52,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:52,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:52,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,734 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,735 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,735 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:52,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:52,752 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,753 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,753 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:52,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,847 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/Mul ...\n",
      "2026-01-15 06:22:52,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:52,958 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,958 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:52,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:52,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:53,043 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,044 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.20/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,044 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,044 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,044 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,045 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,045 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,046 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:53,062 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,062 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,062 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,063 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,063 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:53,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,066 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:53,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,072 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,072 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,073 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:53,089 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,089 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,089 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,089 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,090 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:53,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,187 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,187 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/Mul ...\n",
      "2026-01-15 06:22:53,283 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,286 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,287 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,287 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:53,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.21/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,374 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,374 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,375 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,375 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:53,392 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,392 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,392 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,393 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:53,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,397 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:53,400 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,401 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:53,418 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,418 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,418 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,419 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,419 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,419 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,422 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:53,513 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,513 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,513 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,514 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,514 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,516 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/Mul ...\n",
      "2026-01-15 06:22:53,606 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,610 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,610 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:53,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,692 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.22/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/input_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,693 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,694 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/MatMul ...\n",
      "2026-01-15 06:22:53,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/q_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,713 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,713 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,713 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/MatMul ...\n",
      "2026-01-15 06:22:53,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/k_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,716 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,717 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,717 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/MatMul ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/v_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/q_proj/Add ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/k_proj/Add ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/v_proj/Add ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/GroupQueryAttention ...\n",
      "2026-01-15 06:22:53,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,723 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,723 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/MatMul ...\n",
      "2026-01-15 06:22:53,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/attn/o_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/post_attention_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:53,740 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,741 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,741 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/MatMul ...\n",
      "2026-01-15 06:22:53,835 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,835 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/gate_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,835 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,836 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,836 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,838 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,838 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/MatMul ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/up_proj/lora/Add ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Sigmoid ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/act_fn/Mul ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/Mul ...\n",
      "2026-01-15 06:22:53,945 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/lora_A/MatMul ...\n",
      "2026-01-15 06:22:53,948 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/lora_A/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,948 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/lora_B/MatMul ...\n",
      "2026-01-15 06:22:53,949 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/lora_B/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:53,949 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/MatMul ...\n",
      "2026-01-15 06:22:54,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/MatMul with 4 bits ...\n",
      "2026-01-15 06:22:54,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.23/mlp/down_proj/lora/Add ...\n",
      "2026-01-15 06:22:54,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.24/final_norm_layernorm/SkipLayerNorm ...\n",
      "2026-01-15 06:22:54,032 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
      "2026-01-15 06:22:57,498 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /lm_head/MatMul with 4 bits ...\n",
      "\r",
      "0it [00:00, ?it/s]\r",
      "Saving model.layers.0.attn.k_proj.Add.bias (f16, [128]):   0%|          | 1/1134 [00:00<00:38, 29.17it/s]\r",
      "Saving model.layers.0.attn.v_proj.Add.bias (f16, [128]):   0%|          | 2/1134 [00:00<00:19, 57.36it/s]\r",
      "Saving model.layers.1.attn.k_proj.Add.bias (f16, [128]):   0%|          | 3/1134 [00:00<00:13, 84.84it/s]\r",
      "Saving model.layers.1.attn.v_proj.Add.bias (f16, [128]):   0%|          | 4/1134 [00:00<00:10, 112.04it/s]\r",
      "Saving model.layers.2.attn.k_proj.Add.bias (f16, [128]):   0%|          | 5/1134 [00:00<00:08, 138.44it/s]\r",
      "Saving model.layers.2.attn.v_proj.Add.bias (f16, [128]):   1%|          | 6/1134 [00:00<00:06, 164.34it/s]\r",
      "Saving model.layers.3.attn.k_proj.Add.bias (f16, [128]):   1%|          | 7/1134 [00:00<00:05, 190.47it/s]\r",
      "Saving model.layers.3.attn.v_proj.Add.bias (f16, [128]):   1%|          | 8/1134 [00:00<00:05, 215.75it/s]\r",
      "Saving model.layers.4.attn.k_proj.Add.bias (f16, [128]):   1%|          | 9/1134 [00:00<00:04, 240.49it/s]\r",
      "Saving model.layers.4.attn.v_proj.Add.bias (f16, [128]):   1%|          | 10/1134 [00:00<00:04, 266.14it/s]\r",
      "Saving model.layers.5.attn.k_proj.Add.bias (f16, [128]):   1%|          | 11/1134 [00:00<00:03, 291.83it/s]\r",
      "Saving model.layers.5.attn.v_proj.Add.bias (f16, [128]):   1%|          | 12/1134 [00:00<00:03, 317.43it/s]\r",
      "Saving model.layers.6.attn.k_proj.Add.bias (f16, [128]):   1%|          | 13/1134 [00:00<00:03, 342.65it/s]\r",
      "Saving model.layers.6.attn.v_proj.Add.bias (f16, [128]):   1%|          | 14/1134 [00:00<00:03, 367.83it/s]\r",
      "Saving model.layers.7.attn.k_proj.Add.bias (f16, [128]):   1%|▏         | 15/1134 [00:00<00:02, 392.99it/s]\r",
      "Saving model.layers.7.attn.v_proj.Add.bias (f16, [128]):   1%|▏         | 16/1134 [00:00<00:02, 417.62it/s]\r",
      "Saving model.layers.8.attn.k_proj.Add.bias (f16, [128]):   1%|▏         | 17/1134 [00:00<00:02, 442.43it/s]\r",
      "Saving model.layers.8.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 18/1134 [00:00<00:02, 467.05it/s]\r",
      "Saving model.layers.9.attn.k_proj.Add.bias (f16, [128]):   2%|▏         | 19/1134 [00:00<00:02, 491.57it/s]\r",
      "Saving model.layers.9.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 20/1134 [00:00<00:02, 516.00it/s]\r",
      "Saving model.layers.10.attn.k_proj.Add.bias (f16, [128]):   2%|▏         | 21/1134 [00:00<00:02, 540.19it/s]\r",
      "Saving model.layers.10.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 22/1134 [00:00<00:01, 564.35it/s]\r",
      "Saving model.layers.11.attn.k_proj.Add.bias (f16, [128]):   2%|▏         | 23/1134 [00:00<00:01, 588.36it/s]\r",
      "Saving model.layers.11.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 24/1134 [00:00<00:01, 612.27it/s]\r",
      "Saving model.layers.12.attn.k_proj.Add.bias (f16, [128]):   2%|▏         | 25/1134 [00:00<00:01, 635.58it/s]\r",
      "Saving model.layers.12.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 26/1134 [00:00<00:01, 659.15it/s]\r",
      "Saving model.layers.13.attn.k_proj.Add.bias (f16, [128]):   2%|▏         | 27/1134 [00:00<00:01, 682.63it/s]\r",
      "Saving model.layers.13.attn.v_proj.Add.bias (f16, [128]):   2%|▏         | 28/1134 [00:00<00:01, 705.98it/s]\r",
      "Saving model.layers.14.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 29/1134 [00:00<00:01, 729.17it/s]\r",
      "Saving model.layers.14.attn.v_proj.Add.bias (f16, [128]):   3%|▎         | 30/1134 [00:00<00:01, 752.05it/s]\r",
      "Saving model.layers.15.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 31/1134 [00:00<00:01, 774.73it/s]\r",
      "Saving model.layers.15.attn.v_proj.Add.bias (f16, [128]):   3%|▎         | 32/1134 [00:00<00:01, 797.52it/s]\r",
      "Saving model.layers.16.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 33/1134 [00:00<00:01, 819.96it/s]\r",
      "Saving model.layers.16.attn.v_proj.Add.bias (f16, [128]):   3%|▎         | 34/1134 [00:00<00:01, 842.26it/s]\r",
      "Saving model.layers.17.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 35/1134 [00:00<00:01, 864.63it/s]\r",
      "Saving model.layers.17.attn.v_proj.Add.bias (f16, [128]):   3%|▎         | 36/1134 [00:00<00:01, 886.98it/s]\r",
      "Saving model.layers.18.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 37/1134 [00:00<00:01, 909.28it/s]\r",
      "Saving model.layers.18.attn.v_proj.Add.bias (f16, [128]):   3%|▎         | 38/1134 [00:00<00:01, 931.51it/s]\r",
      "Saving model.layers.19.attn.k_proj.Add.bias (f16, [128]):   3%|▎         | 39/1134 [00:00<00:01, 953.37it/s]\r",
      "Saving model.layers.19.attn.v_proj.Add.bias (f16, [128]):   4%|▎         | 40/1134 [00:00<00:01, 975.18it/s]\r",
      "Saving model.layers.20.attn.k_proj.Add.bias (f16, [128]):   4%|▎         | 41/1134 [00:00<00:01, 996.93it/s]\r",
      "Saving model.layers.20.attn.v_proj.Add.bias (f16, [128]):   4%|▎         | 42/1134 [00:00<00:01, 1018.39it/s]\r",
      "Saving model.layers.21.attn.k_proj.Add.bias (f16, [128]):   4%|▍         | 43/1134 [00:00<00:01, 1039.74it/s]\r",
      "Saving model.layers.21.attn.v_proj.Add.bias (f16, [128]):   4%|▍         | 44/1134 [00:00<00:01, 1061.30it/s]\r",
      "Saving model.layers.22.attn.k_proj.Add.bias (f16, [128]):   4%|▍         | 45/1134 [00:00<00:01, 1082.75it/s]\r",
      "Saving model.layers.22.attn.v_proj.Add.bias (f16, [128]):   4%|▍         | 46/1134 [00:00<00:00, 1104.13it/s]\r",
      "Saving model.layers.23.attn.k_proj.Add.bias (f16, [128]):   4%|▍         | 47/1134 [00:00<00:00, 1125.00it/s]\r",
      "Saving model.layers.23.attn.v_proj.Add.bias (f16, [128]):   4%|▍         | 48/1134 [00:00<00:00, 1146.10it/s]\r",
      "Saving model.layers.0.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   4%|▍         | 49/1134 [00:00<00:00, 1167.77it/s]\r",
      "Saving model.layers.0.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   4%|▍         | 50/1134 [00:00<00:00, 1189.27it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   4%|▍         | 51/1134 [00:00<00:00, 1210.72it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▍         | 52/1134 [00:00<00:00, 1232.10it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▍         | 53/1134 [00:00<00:00, 1252.64it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▍         | 54/1134 [00:00<00:00, 1273.72it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▍         | 55/1134 [00:00<00:00, 1294.84it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▍         | 56/1134 [00:00<00:00, 1315.93it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 57/1134 [00:00<00:00, 1336.95it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 58/1134 [00:00<00:00, 1357.80it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 59/1134 [00:00<00:00, 1378.52it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 60/1134 [00:00<00:00, 1398.96it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 61/1134 [00:00<00:00, 1419.60it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   5%|▌         | 62/1134 [00:00<00:00, 1440.25it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 63/1134 [00:00<00:00, 1460.84it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 64/1134 [00:00<00:00, 1481.37it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 65/1134 [00:00<00:00, 1499.70it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 66/1134 [00:00<00:00, 1519.21it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 67/1134 [00:00<00:00, 1538.80it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 68/1134 [00:00<00:00, 1558.09it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 69/1134 [00:00<00:00, 1577.41it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▌         | 70/1134 [00:00<00:00, 1596.78it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▋         | 71/1134 [00:00<00:00, 1615.72it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▋         | 72/1134 [00:00<00:00, 1634.96it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   6%|▋         | 73/1134 [00:00<00:00, 1654.18it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 74/1134 [00:00<00:00, 1673.31it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 75/1134 [00:00<00:00, 1691.12it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 76/1134 [00:00<00:00, 1709.90it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 77/1134 [00:00<00:00, 1728.68it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 78/1134 [00:00<00:00, 1747.38it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 79/1134 [00:00<00:00, 1765.99it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 80/1134 [00:00<00:00, 1784.60it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 81/1134 [00:00<00:00, 1802.77it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 82/1134 [00:00<00:00, 1821.17it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 83/1134 [00:00<00:00, 1839.52it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 84/1134 [00:00<00:00, 1857.46it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   7%|▋         | 85/1134 [00:00<00:00, 1874.42it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 86/1134 [00:00<00:00, 1892.37it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 87/1134 [00:00<00:00, 1910.38it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 88/1134 [00:00<00:00, 1928.36it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 89/1134 [00:00<00:00, 1946.01it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 90/1134 [00:00<00:00, 1963.61it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 91/1134 [00:00<00:00, 1980.82it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 92/1134 [00:00<00:00, 1998.44it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 93/1134 [00:00<00:00, 2016.02it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 94/1134 [00:00<00:00, 2033.59it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 95/1134 [00:00<00:00, 2050.08it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_B.MatMul.weight_scales (f16, [128]):   8%|▊         | 96/1134 [00:00<00:00, 2067.33it/s]\r",
      "Saving model.layers.0.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▊         | 97/1134 [00:00<00:00, 2084.60it/s] \r",
      "Saving model.layers.0.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▊         | 98/1134 [00:00<00:00, 2101.40it/s]\r",
      "Saving model.layers.0.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▊         | 99/1134 [00:00<00:00, 2118.43it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 100/1134 [00:00<00:00, 2135.53it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 101/1134 [00:00<00:00, 2152.09it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 102/1134 [00:00<00:00, 2168.99it/s]  \r",
      "Saving model.layers.1.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 103/1134 [00:00<00:00, 2185.90it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 104/1134 [00:00<00:00, 2202.70it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 105/1134 [00:00<00:00, 2218.47it/s]\r",
      "Saving model.layers.1.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 106/1134 [00:00<00:00, 2234.96it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):   9%|▉         | 107/1134 [00:00<00:00, 2251.47it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 108/1134 [00:00<00:00, 2267.88it/s]  \r",
      "Saving model.layers.2.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 109/1134 [00:00<00:00, 2284.23it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 110/1134 [00:00<00:00, 2300.62it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 111/1134 [00:00<00:00, 2316.40it/s]\r",
      "Saving model.layers.2.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 112/1134 [00:00<00:00, 2332.37it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|▉         | 113/1134 [00:00<00:00, 2347.64it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 114/1134 [00:00<00:00, 2362.84it/s]  \r",
      "Saving model.layers.3.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 115/1134 [00:00<00:00, 2378.36it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 116/1134 [00:00<00:00, 2394.20it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 117/1134 [00:00<00:00, 2410.09it/s]\r",
      "Saving model.layers.3.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 118/1134 [00:00<00:00, 2424.32it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  10%|█         | 119/1134 [00:00<00:00, 2439.71it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 120/1134 [00:00<00:00, 2454.88it/s]  \r",
      "Saving model.layers.4.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 121/1134 [00:00<00:00, 2470.26it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 122/1134 [00:00<00:00, 2485.55it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 123/1134 [00:00<00:00, 2500.97it/s]\r",
      "Saving model.layers.4.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 124/1134 [00:00<00:00, 2515.39it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 125/1134 [00:00<00:00, 2530.54it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 126/1134 [00:00<00:00, 2545.59it/s]  \r",
      "Saving model.layers.5.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█         | 127/1134 [00:00<00:00, 2560.66it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█▏        | 128/1134 [00:00<00:00, 2575.51it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█▏        | 129/1134 [00:00<00:00, 2589.96it/s]\r",
      "Saving model.layers.5.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  11%|█▏        | 130/1134 [00:00<00:00, 2603.83it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 131/1134 [00:00<00:00, 2618.10it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 132/1134 [00:00<00:00, 2632.32it/s]  \r",
      "Saving model.layers.6.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 133/1134 [00:00<00:00, 2645.55it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 134/1134 [00:00<00:00, 2659.41it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 135/1134 [00:00<00:00, 2673.66it/s]\r",
      "Saving model.layers.6.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 136/1134 [00:00<00:00, 2687.73it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 137/1134 [00:00<00:00, 2701.45it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 138/1134 [00:00<00:00, 2715.52it/s]  \r",
      "Saving model.layers.7.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 139/1134 [00:00<00:00, 2729.18it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 140/1134 [00:00<00:00, 2742.80it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  12%|█▏        | 141/1134 [00:00<00:00, 2754.58it/s]\r",
      "Saving model.layers.7.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 142/1134 [00:00<00:00, 2767.55it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 143/1134 [00:00<00:00, 2695.46it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 144/1134 [00:00<00:00, 2706.03it/s]  \r",
      "Saving model.layers.8.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 145/1134 [00:00<00:00, 2717.67it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 146/1134 [00:00<00:00, 2730.86it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 147/1134 [00:00<00:00, 2744.40it/s]\r",
      "Saving model.layers.8.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 148/1134 [00:00<00:00, 2757.79it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 149/1134 [00:00<00:00, 2771.32it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 150/1134 [00:00<00:00, 2784.46it/s]  \r",
      "Saving model.layers.9.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 151/1134 [00:00<00:00, 2797.87it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 152/1134 [00:00<00:00, 2811.41it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  13%|█▎        | 153/1134 [00:00<00:00, 2824.92it/s]\r",
      "Saving model.layers.9.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▎        | 154/1134 [00:00<00:00, 2837.62it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▎        | 155/1134 [00:00<00:00, 2850.79it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 156/1134 [00:00<00:00, 2864.18it/s]  \r",
      "Saving model.layers.10.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 157/1134 [00:00<00:00, 2877.43it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 158/1134 [00:00<00:00, 2890.53it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 159/1134 [00:00<00:00, 2903.25it/s]\r",
      "Saving model.layers.10.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 160/1134 [00:00<00:00, 2915.71it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 161/1134 [00:00<00:00, 2928.74it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 162/1134 [00:00<00:00, 2941.70it/s]  \r",
      "Saving model.layers.11.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 163/1134 [00:00<00:00, 2954.69it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  14%|█▍        | 164/1134 [00:00<00:00, 2966.84it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 165/1134 [00:00<00:00, 2978.90it/s]\r",
      "Saving model.layers.11.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 166/1134 [00:00<00:00, 2990.89it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 167/1134 [00:00<00:00, 3002.79it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 168/1134 [00:00<00:00, 3015.07it/s]  \r",
      "Saving model.layers.12.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 169/1134 [00:00<00:00, 3027.45it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▍        | 170/1134 [00:00<00:00, 3038.69it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▌        | 171/1134 [00:00<00:00, 3050.75it/s]\r",
      "Saving model.layers.12.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▌        | 172/1134 [00:00<00:00, 3063.04it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▌        | 173/1134 [00:00<00:00, 3064.96it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▌        | 174/1134 [00:00<00:00, 3076.75it/s]  \r",
      "Saving model.layers.13.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  15%|█▌        | 175/1134 [00:00<00:00, 3088.67it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 176/1134 [00:00<00:00, 3101.28it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 177/1134 [00:00<00:00, 3113.94it/s]\r",
      "Saving model.layers.13.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 178/1134 [00:00<00:00, 3126.05it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 179/1134 [00:00<00:00, 3138.12it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 180/1134 [00:00<00:00, 3150.22it/s]  \r",
      "Saving model.layers.14.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 181/1134 [00:00<00:00, 3161.48it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 182/1134 [00:00<00:00, 3173.12it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 183/1134 [00:00<00:00, 3184.75it/s]\r",
      "Saving model.layers.14.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▌        | 184/1134 [00:00<00:00, 3196.76it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▋        | 185/1134 [00:00<00:00, 3208.67it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▋        | 186/1134 [00:00<00:00, 3220.51it/s]  \r",
      "Saving model.layers.15.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  16%|█▋        | 187/1134 [00:00<00:00, 3232.10it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 188/1134 [00:00<00:00, 3244.03it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 189/1134 [00:00<00:00, 3255.85it/s]\r",
      "Saving model.layers.15.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 190/1134 [00:00<00:00, 3267.65it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 191/1134 [00:00<00:00, 3265.65it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 192/1134 [00:00<00:00, 3275.20it/s]  \r",
      "Saving model.layers.16.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 193/1134 [00:00<00:00, 3285.95it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 194/1134 [00:00<00:00, 3297.52it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 195/1134 [00:00<00:00, 3308.56it/s]\r",
      "Saving model.layers.16.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 196/1134 [00:00<00:00, 3319.78it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 197/1134 [00:00<00:00, 3330.88it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  17%|█▋        | 198/1134 [00:00<00:00, 3341.08it/s]  \r",
      "Saving model.layers.17.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 199/1134 [00:00<00:00, 3351.95it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 200/1134 [00:00<00:00, 3362.83it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 201/1134 [00:00<00:00, 3373.91it/s]\r",
      "Saving model.layers.17.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 202/1134 [00:00<00:00, 3385.06it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 203/1134 [00:00<00:00, 3396.40it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 204/1134 [00:00<00:00, 3407.23it/s]  \r",
      "Saving model.layers.18.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 205/1134 [00:00<00:00, 3418.33it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 206/1134 [00:00<00:00, 3429.58it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 207/1134 [00:00<00:00, 3440.91it/s]\r",
      "Saving model.layers.18.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 208/1134 [00:00<00:00, 3448.69it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  18%|█▊        | 209/1134 [00:00<00:00, 3456.78it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▊        | 210/1134 [00:00<00:00, 3465.68it/s]  \r",
      "Saving model.layers.19.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▊        | 211/1134 [00:00<00:00, 3474.34it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▊        | 212/1134 [00:00<00:00, 3482.24it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 213/1134 [00:00<00:00, 3491.35it/s]\r",
      "Saving model.layers.19.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 214/1134 [00:00<00:00, 3500.34it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 215/1134 [00:00<00:00, 3507.91it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 216/1134 [00:00<00:00, 3505.10it/s]  \r",
      "Saving model.layers.20.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 217/1134 [00:00<00:00, 3511.37it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 218/1134 [00:00<00:00, 3518.85it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 219/1134 [00:00<00:00, 3526.72it/s]\r",
      "Saving model.layers.20.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 220/1134 [00:00<00:00, 3532.03it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  19%|█▉        | 221/1134 [00:00<00:00, 3539.08it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|█▉        | 222/1134 [00:00<00:00, 3547.34it/s]  \r",
      "Saving model.layers.21.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|█▉        | 223/1134 [00:00<00:00, 3555.28it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|█▉        | 224/1134 [00:00<00:00, 3563.42it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|█▉        | 225/1134 [00:00<00:00, 3571.32it/s]\r",
      "Saving model.layers.21.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|█▉        | 226/1134 [00:00<00:00, 3579.81it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 227/1134 [00:00<00:00, 3587.41it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 228/1134 [00:00<00:00, 3595.32it/s]  \r",
      "Saving model.layers.22.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 229/1134 [00:00<00:00, 3603.04it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 230/1134 [00:00<00:00, 3611.20it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 231/1134 [00:00<00:00, 3618.82it/s]\r",
      "Saving model.layers.22.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  20%|██        | 232/1134 [00:00<00:00, 3625.44it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 233/1134 [00:00<00:00, 3633.50it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 234/1134 [00:00<00:00, 3640.70it/s]  \r",
      "Saving model.layers.23.attn.q_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 235/1134 [00:00<00:00, 3648.41it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 236/1134 [00:00<00:00, 3656.80it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 237/1134 [00:00<00:00, 3664.89it/s]\r",
      "Saving model.layers.23.attn.o_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 238/1134 [00:00<00:00, 3672.72it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 239/1134 [00:00<00:00, 3679.57it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_A.MatMul.weight_scales (f16, [448]):  21%|██        | 240/1134 [00:00<00:00, 3687.64it/s]  \r",
      "Saving model.layers.0.input_layernorm.weight (f16, [896]):  21%|██▏       | 241/1134 [00:00<00:00, 3696.07it/s]                  \r",
      "Saving model.layers.0.attn.q_proj.Add.bias (f16, [896]):  21%|██▏       | 242/1134 [00:00<00:00, 3702.12it/s]  \r",
      "Saving model.layers.0.post_attention_layernorm.weight (f16, [896]):  21%|██▏       | 243/1134 [00:00<00:00, 3709.38it/s]\r",
      "Saving model.layers.1.input_layernorm.weight (f16, [896]):  22%|██▏       | 244/1134 [00:00<00:00, 3717.25it/s]         \r",
      "Saving model.layers.1.attn.q_proj.Add.bias (f16, [896]):  22%|██▏       | 245/1134 [00:00<00:00, 3724.77it/s]  \r",
      "Saving model.layers.1.post_attention_layernorm.weight (f16, [896]):  22%|██▏       | 246/1134 [00:00<00:00, 3731.68it/s]\r",
      "Saving model.layers.2.input_layernorm.weight (f16, [896]):  22%|██▏       | 247/1134 [00:00<00:00, 3739.52it/s]         \r",
      "Saving model.layers.2.attn.q_proj.Add.bias (f16, [896]):  22%|██▏       | 248/1134 [00:00<00:00, 3747.83it/s]  \r",
      "Saving model.layers.2.post_attention_layernorm.weight (f16, [896]):  22%|██▏       | 249/1134 [00:00<00:00, 3754.41it/s]\r",
      "Saving model.layers.3.input_layernorm.weight (f16, [896]):  22%|██▏       | 250/1134 [00:00<00:00, 3762.06it/s]         \r",
      "Saving model.layers.3.attn.q_proj.Add.bias (f16, [896]):  22%|██▏       | 251/1134 [00:00<00:00, 3769.72it/s]  \r",
      "Saving model.layers.3.post_attention_layernorm.weight (f16, [896]):  22%|██▏       | 252/1134 [00:00<00:00, 3777.29it/s]\r",
      "Saving model.layers.4.input_layernorm.weight (f16, [896]):  22%|██▏       | 253/1134 [00:00<00:00, 3784.57it/s]         \r",
      "Saving model.layers.4.attn.q_proj.Add.bias (f16, [896]):  22%|██▏       | 254/1134 [00:00<00:00, 3791.35it/s]  \r",
      "Saving model.layers.4.post_attention_layernorm.weight (f16, [896]):  22%|██▏       | 255/1134 [00:00<00:00, 3798.96it/s]\r",
      "Saving model.layers.5.input_layernorm.weight (f16, [896]):  23%|██▎       | 256/1134 [00:00<00:00, 3805.53it/s]         \r",
      "Saving model.layers.5.attn.q_proj.Add.bias (f16, [896]):  23%|██▎       | 257/1134 [00:00<00:00, 3812.91it/s]  \r",
      "Saving model.layers.5.post_attention_layernorm.weight (f16, [896]):  23%|██▎       | 258/1134 [00:00<00:00, 3820.60it/s]\r",
      "Saving model.layers.6.input_layernorm.weight (f16, [896]):  23%|██▎       | 259/1134 [00:00<00:00, 3827.50it/s]         \r",
      "Saving model.layers.6.attn.q_proj.Add.bias (f16, [896]):  23%|██▎       | 260/1134 [00:00<00:00, 3834.81it/s]  \r",
      "Saving model.layers.6.post_attention_layernorm.weight (f16, [896]):  23%|██▎       | 261/1134 [00:00<00:00, 3841.11it/s]\r",
      "Saving model.layers.7.input_layernorm.weight (f16, [896]):  23%|██▎       | 262/1134 [00:00<00:00, 3848.16it/s]         \r",
      "Saving model.layers.7.attn.q_proj.Add.bias (f16, [896]):  23%|██▎       | 263/1134 [00:00<00:00, 3855.23it/s]  \r",
      "Saving model.layers.7.post_attention_layernorm.weight (f16, [896]):  23%|██▎       | 264/1134 [00:00<00:00, 3860.57it/s]\r",
      "Saving model.layers.8.input_layernorm.weight (f16, [896]):  23%|██▎       | 265/1134 [00:00<00:00, 3853.80it/s]         \r",
      "Saving model.layers.8.attn.q_proj.Add.bias (f16, [896]):  23%|██▎       | 266/1134 [00:00<00:00, 3858.47it/s]  \r",
      "Saving model.layers.8.post_attention_layernorm.weight (f16, [896]):  24%|██▎       | 267/1134 [00:00<00:00, 3864.34it/s]\r",
      "Saving model.layers.9.input_layernorm.weight (f16, [896]):  24%|██▎       | 268/1134 [00:00<00:00, 3869.64it/s]         \r",
      "Saving model.layers.9.attn.q_proj.Add.bias (f16, [896]):  24%|██▎       | 269/1134 [00:00<00:00, 3875.62it/s]  \r",
      "Saving model.layers.9.post_attention_layernorm.weight (f16, [896]):  24%|██▍       | 270/1134 [00:00<00:00, 3882.98it/s]\r",
      "Saving model.layers.10.input_layernorm.weight (f16, [896]):  24%|██▍       | 271/1134 [00:00<00:00, 3889.67it/s]        \r",
      "Saving model.layers.10.attn.q_proj.Add.bias (f16, [896]):  24%|██▍       | 272/1134 [00:00<00:00, 3896.88it/s]  \r",
      "Saving model.layers.10.post_attention_layernorm.weight (f16, [896]):  24%|██▍       | 273/1134 [00:00<00:00, 3903.70it/s]\r",
      "Saving model.layers.11.input_layernorm.weight (f16, [896]):  24%|██▍       | 274/1134 [00:00<00:00, 3911.08it/s]         \r",
      "Saving model.layers.11.attn.q_proj.Add.bias (f16, [896]):  24%|██▍       | 275/1134 [00:00<00:00, 3918.05it/s]  \r",
      "Saving model.layers.11.post_attention_layernorm.weight (f16, [896]):  24%|██▍       | 276/1134 [00:00<00:00, 3923.55it/s]\r",
      "Saving model.layers.12.input_layernorm.weight (f16, [896]):  24%|██▍       | 277/1134 [00:00<00:00, 3930.44it/s]         \r",
      "Saving model.layers.12.attn.q_proj.Add.bias (f16, [896]):  25%|██▍       | 278/1134 [00:00<00:00, 3937.05it/s]  \r",
      "Saving model.layers.12.post_attention_layernorm.weight (f16, [896]):  25%|██▍       | 279/1134 [00:00<00:00, 3943.21it/s]\r",
      "Saving model.layers.13.input_layernorm.weight (f16, [896]):  25%|██▍       | 280/1134 [00:00<00:00, 3949.47it/s]         \r",
      "Saving model.layers.13.attn.q_proj.Add.bias (f16, [896]):  25%|██▍       | 281/1134 [00:00<00:00, 3956.60it/s]  \r",
      "Saving model.layers.13.post_attention_layernorm.weight (f16, [896]):  25%|██▍       | 282/1134 [00:00<00:00, 3963.89it/s]\r",
      "Saving model.layers.14.input_layernorm.weight (f16, [896]):  25%|██▍       | 283/1134 [00:00<00:00, 3969.61it/s]         \r",
      "Saving model.layers.14.attn.q_proj.Add.bias (f16, [896]):  25%|██▌       | 284/1134 [00:00<00:00, 3976.25it/s]  \r",
      "Saving model.layers.14.post_attention_layernorm.weight (f16, [896]):  25%|██▌       | 285/1134 [00:00<00:00, 3983.99it/s]\r",
      "Saving model.layers.15.input_layernorm.weight (f16, [896]):  25%|██▌       | 286/1134 [00:00<00:00, 3991.96it/s]         \r",
      "Saving model.layers.15.attn.q_proj.Add.bias (f16, [896]):  25%|██▌       | 287/1134 [00:00<00:00, 3999.91it/s]  \r",
      "Saving model.layers.15.post_attention_layernorm.weight (f16, [896]):  25%|██▌       | 288/1134 [00:00<00:00, 4006.81it/s]\r",
      "Saving model.layers.16.input_layernorm.weight (f16, [896]):  25%|██▌       | 289/1134 [00:00<00:00, 4013.95it/s]         \r",
      "Saving model.layers.16.attn.q_proj.Add.bias (f16, [896]):  26%|██▌       | 290/1134 [00:00<00:00, 4021.25it/s]  \r",
      "Saving model.layers.16.post_attention_layernorm.weight (f16, [896]):  26%|██▌       | 291/1134 [00:00<00:00, 4027.05it/s]\r",
      "Saving model.layers.17.input_layernorm.weight (f16, [896]):  26%|██▌       | 292/1134 [00:00<00:00, 4031.95it/s]         \r",
      "Saving model.layers.17.attn.q_proj.Add.bias (f16, [896]):  26%|██▌       | 293/1134 [00:00<00:00, 4038.52it/s]  \r",
      "Saving model.layers.17.post_attention_layernorm.weight (f16, [896]):  26%|██▌       | 294/1134 [00:00<00:00, 4044.68it/s]\r",
      "Saving model.layers.18.input_layernorm.weight (f16, [896]):  26%|██▌       | 295/1134 [00:00<00:00, 4050.70it/s]         \r",
      "Saving model.layers.18.attn.q_proj.Add.bias (f16, [896]):  26%|██▌       | 296/1134 [00:00<00:00, 4056.64it/s]  \r",
      "Saving model.layers.18.post_attention_layernorm.weight (f16, [896]):  26%|██▌       | 297/1134 [00:00<00:00, 4063.65it/s]\r",
      "Saving model.layers.19.input_layernorm.weight (f16, [896]):  26%|██▋       | 298/1134 [00:00<00:00, 4070.73it/s]         \r",
      "Saving model.layers.19.attn.q_proj.Add.bias (f16, [896]):  26%|██▋       | 299/1134 [00:00<00:00, 4075.91it/s]  \r",
      "Saving model.layers.19.post_attention_layernorm.weight (f16, [896]):  26%|██▋       | 300/1134 [00:00<00:00, 4082.26it/s]\r",
      "Saving model.layers.20.input_layernorm.weight (f16, [896]):  27%|██▋       | 301/1134 [00:00<00:00, 4077.95it/s]         \r",
      "Saving model.layers.20.attn.q_proj.Add.bias (f16, [896]):  27%|██▋       | 302/1134 [00:00<00:00, 4081.47it/s]  \r",
      "Saving model.layers.20.post_attention_layernorm.weight (f16, [896]):  27%|██▋       | 303/1134 [00:00<00:00, 4087.11it/s]\r",
      "Saving model.layers.21.input_layernorm.weight (f16, [896]):  27%|██▋       | 304/1134 [00:00<00:00, 4092.37it/s]         \r",
      "Saving model.layers.21.attn.q_proj.Add.bias (f16, [896]):  27%|██▋       | 305/1134 [00:00<00:00, 4098.35it/s]  \r",
      "Saving model.layers.21.post_attention_layernorm.weight (f16, [896]):  27%|██▋       | 306/1134 [00:00<00:00, 4105.89it/s]\r",
      "Saving model.layers.22.input_layernorm.weight (f16, [896]):  27%|██▋       | 307/1134 [00:00<00:00, 4113.57it/s]         \r",
      "Saving model.layers.22.attn.q_proj.Add.bias (f16, [896]):  27%|██▋       | 308/1134 [00:00<00:00, 4121.82it/s]  \r",
      "Saving model.layers.22.post_attention_layernorm.weight (f16, [896]):  27%|██▋       | 309/1134 [00:00<00:00, 4129.96it/s]\r",
      "Saving model.layers.23.input_layernorm.weight (f16, [896]):  27%|██▋       | 310/1134 [00:00<00:00, 4137.75it/s]         \r",
      "Saving model.layers.23.attn.q_proj.Add.bias (f16, [896]):  27%|██▋       | 311/1134 [00:00<00:00, 4146.29it/s]  \r",
      "Saving model.layers.23.post_attention_layernorm.weight (f16, [896]):  28%|██▊       | 312/1134 [00:00<00:00, 4154.48it/s]\r",
      "Saving model.layers.24.final_norm_layernorm.weight (f16, [896]):  28%|██▊       | 313/1134 [00:00<00:00, 4162.48it/s]    \r",
      "Saving model.layers.0.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 314/1134 [00:00<00:00, 4165.86it/s]\r",
      "Saving model.layers.0.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 315/1134 [00:00<00:00, 4171.51it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 316/1134 [00:00<00:00, 4176.99it/s]\r",
      "Saving model.layers.1.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 317/1134 [00:00<00:00, 4182.55it/s]  \r",
      "Saving model.layers.1.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 318/1134 [00:00<00:00, 4187.94it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 319/1134 [00:00<00:00, 4193.95it/s]\r",
      "Saving model.layers.2.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 320/1134 [00:00<00:00, 4200.13it/s]  \r",
      "Saving model.layers.2.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 321/1134 [00:00<00:00, 4204.93it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 322/1134 [00:00<00:00, 4210.44it/s]\r",
      "Saving model.layers.3.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  28%|██▊       | 323/1134 [00:00<00:00, 4216.65it/s]  \r",
      "Saving model.layers.3.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▊       | 324/1134 [00:00<00:00, 4222.65it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▊       | 325/1134 [00:00<00:00, 4146.08it/s]\r",
      "Saving model.layers.4.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▊       | 326/1134 [00:00<00:00, 4151.31it/s]  \r",
      "Saving model.layers.4.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 327/1134 [00:00<00:00, 4156.46it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 328/1134 [00:00<00:00, 4161.23it/s]\r",
      "Saving model.layers.5.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 329/1134 [00:00<00:00, 4165.83it/s]  \r",
      "Saving model.layers.5.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 330/1134 [00:00<00:00, 4171.01it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 331/1134 [00:00<00:00, 4174.53it/s]\r",
      "Saving model.layers.6.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 332/1134 [00:00<00:00, 4179.39it/s]  \r",
      "Saving model.layers.6.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 333/1134 [00:00<00:00, 4184.38it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  29%|██▉       | 334/1134 [00:00<00:00, 4189.32it/s]\r",
      "Saving model.layers.7.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 335/1134 [00:00<00:00, 4193.99it/s]  \r",
      "Saving model.layers.7.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 336/1134 [00:00<00:00, 4198.79it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 337/1134 [00:00<00:00, 4192.91it/s]\r",
      "Saving model.layers.8.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 338/1134 [00:00<00:00, 4196.66it/s]  \r",
      "Saving model.layers.8.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 339/1134 [00:00<00:00, 4201.63it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|██▉       | 340/1134 [00:00<00:00, 4206.83it/s]\r",
      "Saving model.layers.9.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|███       | 341/1134 [00:00<00:00, 4211.26it/s]  \r",
      "Saving model.layers.9.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|███       | 342/1134 [00:00<00:00, 4216.53it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|███       | 343/1134 [00:00<00:00, 4220.10it/s]\r",
      "Saving model.layers.10.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|███       | 344/1134 [00:00<00:00, 4224.86it/s] \r",
      "Saving model.layers.10.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  30%|███       | 345/1134 [00:00<00:00, 4230.33it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 346/1134 [00:00<00:00, 4236.21it/s]\r",
      "Saving model.layers.11.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 347/1134 [00:00<00:00, 4241.71it/s]  \r",
      "Saving model.layers.11.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 348/1134 [00:00<00:00, 4245.84it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 349/1134 [00:00<00:00, 4251.07it/s]\r",
      "Saving model.layers.12.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 350/1134 [00:00<00:00, 4254.89it/s]  \r",
      "Saving model.layers.12.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 351/1134 [00:00<00:00, 4260.02it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 352/1134 [00:00<00:00, 4265.52it/s]\r",
      "Saving model.layers.13.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 353/1134 [00:00<00:00, 4270.34it/s]  \r",
      "Saving model.layers.13.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███       | 354/1134 [00:00<00:00, 4275.35it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███▏      | 355/1134 [00:00<00:00, 4279.57it/s]\r",
      "Saving model.layers.14.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███▏      | 356/1134 [00:00<00:00, 4285.69it/s]  \r",
      "Saving model.layers.14.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  31%|███▏      | 357/1134 [00:00<00:00, 4292.66it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 358/1134 [00:00<00:00, 4298.56it/s]\r",
      "Saving model.layers.15.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 359/1134 [00:00<00:00, 4305.44it/s]  \r",
      "Saving model.layers.15.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 360/1134 [00:00<00:00, 4312.43it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 361/1134 [00:00<00:00, 4319.42it/s]\r",
      "Saving model.layers.16.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 362/1134 [00:00<00:00, 4326.45it/s]  \r",
      "Saving model.layers.16.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 363/1134 [00:00<00:00, 4332.56it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 364/1134 [00:00<00:00, 4339.18it/s]\r",
      "Saving model.layers.17.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 365/1134 [00:00<00:00, 4346.34it/s]  \r",
      "Saving model.layers.17.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 366/1134 [00:00<00:00, 4352.75it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 367/1134 [00:00<00:00, 4359.02it/s]\r",
      "Saving model.layers.18.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  32%|███▏      | 368/1134 [00:00<00:00, 4364.74it/s]  \r",
      "Saving model.layers.18.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 369/1134 [00:00<00:00, 4371.36it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 370/1134 [00:00<00:00, 4377.36it/s]\r",
      "Saving model.layers.19.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 371/1134 [00:00<00:00, 4383.64it/s]  \r",
      "Saving model.layers.19.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 372/1134 [00:00<00:00, 4389.32it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 373/1134 [00:00<00:00, 4383.33it/s]\r",
      "Saving model.layers.20.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 374/1134 [00:00<00:00, 4389.58it/s]  \r",
      "Saving model.layers.20.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 375/1134 [00:00<00:00, 4393.70it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 376/1134 [00:00<00:00, 4400.25it/s]\r",
      "Saving model.layers.21.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 377/1134 [00:00<00:00, 4406.71it/s]  \r",
      "Saving model.layers.21.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 378/1134 [00:00<00:00, 4412.94it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  33%|███▎      | 379/1134 [00:00<00:00, 4419.40it/s]\r",
      "Saving model.layers.22.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▎      | 380/1134 [00:00<00:00, 4425.96it/s]  \r",
      "Saving model.layers.22.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▎      | 381/1134 [00:00<00:00, 4432.35it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▎      | 382/1134 [00:00<00:00, 4438.87it/s]\r",
      "Saving model.layers.23.attn.q_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▍      | 383/1134 [00:00<00:00, 4445.43it/s]  \r",
      "Saving model.layers.23.attn.o_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▍      | 384/1134 [00:00<00:00, 4448.73it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_B.MatMul.weight_scales (f16, [896]):  34%|███▍      | 385/1134 [00:00<00:00, 4455.57it/s]\r",
      "Saving model.layers.0.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 386/1134 [00:00<00:00, 4461.51it/s]   \r",
      "Saving model.layers.0.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 387/1134 [00:00<00:00, 4467.52it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 388/1134 [00:00<00:00, 4474.06it/s]\r",
      "Saving model.layers.1.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 389/1134 [00:00<00:00, 4480.40it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 390/1134 [00:00<00:00, 4486.80it/s]\r",
      "Saving model.layers.2.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  34%|███▍      | 391/1134 [00:00<00:00, 4493.25it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▍      | 392/1134 [00:00<00:00, 4499.71it/s]\r",
      "Saving model.layers.3.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▍      | 393/1134 [00:00<00:00, 4506.13it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▍      | 394/1134 [00:00<00:00, 4511.37it/s]\r",
      "Saving model.layers.4.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▍      | 395/1134 [00:00<00:00, 4517.72it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▍      | 396/1134 [00:00<00:00, 4524.74it/s]\r",
      "Saving model.layers.5.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 397/1134 [00:00<00:00, 4530.37it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 398/1134 [00:00<00:00, 4536.93it/s]\r",
      "Saving model.layers.6.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 399/1134 [00:00<00:00, 4543.69it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 400/1134 [00:00<00:00, 4550.01it/s]\r",
      "Saving model.layers.7.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 401/1134 [00:00<00:00, 4556.40it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  35%|███▌      | 402/1134 [00:00<00:00, 4562.96it/s]\r",
      "Saving model.layers.8.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 403/1134 [00:00<00:00, 4569.09it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 404/1134 [00:00<00:00, 4572.77it/s]\r",
      "Saving model.layers.9.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 405/1134 [00:00<00:00, 4579.09it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 406/1134 [00:00<00:00, 4571.44it/s]\r",
      "Saving model.layers.10.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 407/1134 [00:00<00:00, 4574.19it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 408/1134 [00:00<00:00, 4577.82it/s]\r",
      "Saving model.layers.11.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 409/1134 [00:00<00:00, 4583.03it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 410/1134 [00:00<00:00, 4587.00it/s]\r",
      "Saving model.layers.12.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▌      | 411/1134 [00:00<00:00, 4592.46it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▋      | 412/1134 [00:00<00:00, 4597.49it/s]\r",
      "Saving model.layers.13.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  36%|███▋      | 413/1134 [00:00<00:00, 4602.93it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 414/1134 [00:00<00:00, 4608.56it/s]\r",
      "Saving model.layers.14.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 415/1134 [00:00<00:00, 4613.79it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 416/1134 [00:00<00:00, 4619.79it/s]\r",
      "Saving model.layers.15.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 417/1134 [00:00<00:00, 4625.76it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 418/1134 [00:00<00:00, 4631.19it/s]\r",
      "Saving model.layers.16.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 419/1134 [00:00<00:00, 4636.66it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 420/1134 [00:00<00:00, 4642.11it/s]\r",
      "Saving model.layers.17.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 421/1134 [00:00<00:00, 4647.95it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 422/1134 [00:00<00:00, 4653.34it/s]\r",
      "Saving model.layers.18.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 423/1134 [00:00<00:00, 4659.20it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 424/1134 [00:00<00:00, 4664.96it/s]\r",
      "Saving model.layers.19.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  37%|███▋      | 425/1134 [00:00<00:00, 4670.79it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 426/1134 [00:00<00:00, 4676.65it/s]\r",
      "Saving model.layers.20.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 427/1134 [00:00<00:00, 4683.06it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 428/1134 [00:00<00:00, 4688.87it/s]\r",
      "Saving model.layers.21.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 429/1134 [00:00<00:00, 4695.32it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 430/1134 [00:00<00:00, 4701.90it/s]\r",
      "Saving model.layers.22.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 431/1134 [00:00<00:00, 4708.48it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 432/1134 [00:00<00:00, 4714.56it/s]\r",
      "Saving model.layers.23.attn.v_proj.lora_B.MatMul.weight_Q4 (u8, [128,1,16]):  38%|███▊      | 433/1134 [00:00<00:00, 4721.09it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  38%|███▊      | 434/1134 [00:00<00:00, 4727.62it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  38%|███▊      | 435/1134 [00:00<00:00, 4734.10it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  38%|███▊      | 436/1134 [00:00<00:00, 4730.19it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▊      | 437/1134 [00:00<00:00, 4733.82it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▊      | 438/1134 [00:00<00:00, 4739.90it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▊      | 439/1134 [00:00<00:00, 4746.33it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 440/1134 [00:00<00:00, 4752.80it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 441/1134 [00:00<00:00, 4758.75it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 442/1134 [00:00<00:00, 4765.20it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 443/1134 [00:00<00:00, 4771.68it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 444/1134 [00:00<00:00, 4778.09it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 445/1134 [00:00<00:00, 4784.41it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 446/1134 [00:00<00:00, 4790.81it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  39%|███▉      | 447/1134 [00:00<00:00, 4797.26it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 448/1134 [00:00<00:00, 4802.56it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 449/1134 [00:00<00:00, 4798.82it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 450/1134 [00:00<00:00, 4801.58it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 451/1134 [00:00<00:00, 4806.79it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 452/1134 [00:00<00:00, 4812.13it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|███▉      | 453/1134 [00:00<00:00, 4817.57it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|████      | 454/1134 [00:00<00:00, 4823.04it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|████      | 455/1134 [00:00<00:00, 4827.59it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|████      | 456/1134 [00:00<00:00, 4832.42it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_A.MatMul.weight_scales (f16, [2432]):  40%|████      | 457/1134 [00:00<00:00, 4837.84it/s]\r",
      "Saving model.layers.0.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 458/1134 [00:00<00:00, 4843.15it/s]    \r",
      "Saving model.layers.0.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  40%|████      | 459/1134 [00:00<00:00, 4847.05it/s]\r",
      "Saving model.layers.0.attn.k_proj.MatMul.weight_scales (f16, [3584]):  41%|████      | 460/1134 [00:00<00:00, 4851.65it/s]      \r",
      "Saving model.layers.0.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 461/1134 [00:00<00:00, 4688.07it/s]\r",
      "Saving model.layers.0.attn.v_proj.MatMul.weight_scales (f16, [3584]):  41%|████      | 462/1134 [00:00<00:00, 4689.78it/s]      \r",
      "Saving model.layers.0.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 463/1134 [00:00<00:00, 4693.69it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 464/1134 [00:00<00:00, 4698.04it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 465/1134 [00:00<00:00, 4701.84it/s]  \r",
      "Saving model.layers.1.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 466/1134 [00:00<00:00, 4705.43it/s]\r",
      "Saving model.layers.1.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████      | 467/1134 [00:00<00:00, 4710.25it/s]\r",
      "Saving model.layers.1.attn.k_proj.MatMul.weight_scales (f16, [3584]):  41%|████▏     | 468/1134 [00:00<00:00, 4714.16it/s]      \r",
      "Saving model.layers.1.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  41%|████▏     | 469/1134 [00:00<00:00, 4717.40it/s]\r",
      "Saving model.layers.1.attn.v_proj.MatMul.weight_scales (f16, [3584]):  41%|████▏     | 470/1134 [00:00<00:00, 4713.49it/s]      \r",
      "Saving model.layers.1.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 471/1134 [00:00<00:00, 4715.87it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 472/1134 [00:00<00:00, 4720.05it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 473/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 473/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.2.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 474/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 475/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.k_proj.MatMul.weight_scales (f16, [3584]):  42%|████▏     | 476/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.2.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 477/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.v_proj.MatMul.weight_scales (f16, [3584]):  42%|████▏     | 478/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.2.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 479/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 480/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  42%|████▏     | 481/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.3.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 482/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 483/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.k_proj.MatMul.weight_scales (f16, [3584]):  43%|████▎     | 484/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.3.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 485/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.v_proj.MatMul.weight_scales (f16, [3584]):  43%|████▎     | 486/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.3.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 487/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 488/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 489/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.4.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 490/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 491/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.k_proj.MatMul.weight_scales (f16, [3584]):  43%|████▎     | 492/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.4.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  43%|████▎     | 493/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.v_proj.MatMul.weight_scales (f16, [3584]):  44%|████▎     | 494/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.4.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▎     | 495/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▎     | 496/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 497/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.5.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 498/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 499/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.k_proj.MatMul.weight_scales (f16, [3584]):  44%|████▍     | 500/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.5.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 501/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.v_proj.MatMul.weight_scales (f16, [3584]):  44%|████▍     | 502/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.5.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 503/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  44%|████▍     | 504/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 505/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.6.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 506/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 507/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.k_proj.MatMul.weight_scales (f16, [3584]):  45%|████▍     | 508/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.6.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▍     | 509/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.v_proj.MatMul.weight_scales (f16, [3584]):  45%|████▍     | 510/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.6.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 511/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 512/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 513/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.7.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 514/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  45%|████▌     | 515/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.k_proj.MatMul.weight_scales (f16, [3584]):  46%|████▌     | 516/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.7.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 517/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.v_proj.MatMul.weight_scales (f16, [3584]):  46%|████▌     | 518/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.7.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 519/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 520/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 521/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.8.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 522/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▌     | 523/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.k_proj.MatMul.weight_scales (f16, [3584]):  46%|████▌     | 524/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.8.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▋     | 525/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.v_proj.MatMul.weight_scales (f16, [3584]):  46%|████▋     | 526/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.8.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  46%|████▋     | 527/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 528/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 529/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.9.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 530/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 531/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.k_proj.MatMul.weight_scales (f16, [3584]):  47%|████▋     | 532/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.9.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 533/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.v_proj.MatMul.weight_scales (f16, [3584]):  47%|████▋     | 534/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.9.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 535/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 536/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 537/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.10.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  47%|████▋     | 538/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 539/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.k_proj.MatMul.weight_scales (f16, [3584]):  48%|████▊     | 540/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.10.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 541/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.v_proj.MatMul.weight_scales (f16, [3584]):  48%|████▊     | 542/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.10.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 543/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 544/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 545/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.11.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 546/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 547/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.k_proj.MatMul.weight_scales (f16, [3584]):  48%|████▊     | 548/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.11.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  48%|████▊     | 549/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.v_proj.MatMul.weight_scales (f16, [3584]):  49%|████▊     | 550/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.11.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▊     | 551/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▊     | 552/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 553/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.12.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 554/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 555/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.k_proj.MatMul.weight_scales (f16, [3584]):  49%|████▉     | 556/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.12.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 557/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.v_proj.MatMul.weight_scales (f16, [3584]):  49%|████▉     | 558/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.12.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 559/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 560/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  49%|████▉     | 561/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.13.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 562/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 563/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.k_proj.MatMul.weight_scales (f16, [3584]):  50%|████▉     | 564/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.13.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|████▉     | 565/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.v_proj.MatMul.weight_scales (f16, [3584]):  50%|████▉     | 566/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.13.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 567/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 568/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 569/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.14.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 570/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  50%|█████     | 571/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.k_proj.MatMul.weight_scales (f16, [3584]):  50%|█████     | 572/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.14.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 573/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.v_proj.MatMul.weight_scales (f16, [3584]):  51%|█████     | 574/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.14.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 575/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 576/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 577/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.15.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 578/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 579/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.k_proj.MatMul.weight_scales (f16, [3584]):  51%|█████     | 580/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.15.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████     | 581/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.v_proj.MatMul.weight_scales (f16, [3584]):  51%|█████▏    | 582/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.15.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████▏    | 583/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  51%|█████▏    | 584/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 585/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.16.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 586/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 587/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.k_proj.MatMul.weight_scales (f16, [3584]):  52%|█████▏    | 588/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.16.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 589/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.v_proj.MatMul.weight_scales (f16, [3584]):  52%|█████▏    | 590/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.16.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 591/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 592/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 593/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.17.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 594/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  52%|█████▏    | 595/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.k_proj.MatMul.weight_scales (f16, [3584]):  53%|█████▎    | 596/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.17.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 597/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.v_proj.MatMul.weight_scales (f16, [3584]):  53%|█████▎    | 598/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.17.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 599/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 600/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 601/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.18.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 602/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 603/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.k_proj.MatMul.weight_scales (f16, [3584]):  53%|█████▎    | 604/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.18.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  53%|█████▎    | 605/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.v_proj.MatMul.weight_scales (f16, [3584]):  53%|█████▎    | 606/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.18.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▎    | 607/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▎    | 608/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▎    | 609/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.19.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 610/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 611/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.k_proj.MatMul.weight_scales (f16, [3584]):  54%|█████▍    | 612/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.19.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 613/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.v_proj.MatMul.weight_scales (f16, [3584]):  54%|█████▍    | 614/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.19.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 615/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 616/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 617/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.20.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  54%|█████▍    | 618/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▍    | 619/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.k_proj.MatMul.weight_scales (f16, [3584]):  55%|█████▍    | 620/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.20.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▍    | 621/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.v_proj.MatMul.weight_scales (f16, [3584]):  55%|█████▍    | 622/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.20.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▍    | 623/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▌    | 624/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▌    | 625/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.21.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▌    | 626/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▌    | 627/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.k_proj.MatMul.weight_scales (f16, [3584]):  55%|█████▌    | 628/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.21.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  55%|█████▌    | 629/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.v_proj.MatMul.weight_scales (f16, [3584]):  56%|█████▌    | 630/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.21.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 631/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 632/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 633/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.22.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 634/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 635/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.k_proj.MatMul.weight_scales (f16, [3584]):  56%|█████▌    | 636/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.22.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▌    | 637/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.v_proj.MatMul.weight_scales (f16, [3584]):  56%|█████▋    | 638/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.22.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▋    | 639/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  56%|█████▋    | 640/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 641/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.23.attn.q_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 642/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.k_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 643/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.k_proj.MatMul.weight_scales (f16, [3584]):  57%|█████▋    | 644/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.23.attn.v_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 645/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.v_proj.MatMul.weight_scales (f16, [3584]):  57%|█████▋    | 646/1134 [00:00<00:00, 4725.21it/s]      \r",
      "Saving model.layers.23.attn.o_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 647/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 648/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_A.MatMul.weight_Q4 (u8, [16,28,16]):  57%|█████▋    | 649/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  57%|█████▋    | 650/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  57%|█████▋    | 651/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.1.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  57%|█████▋    | 652/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 653/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.2.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 654/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 655/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.3.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 656/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 657/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.4.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 658/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 659/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.5.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 660/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 661/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.6.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 662/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  58%|█████▊    | 663/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.7.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▊    | 664/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▊    | 665/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.8.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▊    | 666/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 667/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.9.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 668/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 669/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.10.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 670/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 671/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.11.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 672/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 673/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.12.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  59%|█████▉    | 674/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 675/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.13.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 676/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 677/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.14.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 678/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 679/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.15.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|█████▉    | 680/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 681/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.16.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 682/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 683/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.17.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 684/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 685/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.18.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  60%|██████    | 686/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 687/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.19.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 688/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 689/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.20.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 690/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 691/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.21.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 692/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 693/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.22.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████    | 694/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████▏   | 695/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.23.mlp.gate_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████▏   | 696/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_B.MatMul.weight_scales (f16, [4864]):  61%|██████▏   | 697/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 698/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 699/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 700/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 701/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.1.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 702/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 703/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 704/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.2.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 705/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 706/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 707/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.3.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  62%|██████▏   | 708/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 709/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 710/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.4.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 711/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 712/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 713/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.5.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 714/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 715/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 716/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.6.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 717/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 718/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 719/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.7.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  63%|██████▎   | 720/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▎   | 721/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▎   | 722/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.8.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 723/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 724/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 725/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.9.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 726/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 727/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 728/1134 [00:00<00:00, 4725.21it/s] \r",
      "Saving model.layers.10.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 729/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 730/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  64%|██████▍   | 731/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.11.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 732/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 733/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 734/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.12.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 735/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 736/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▍   | 737/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.13.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▌   | 738/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▌   | 739/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▌   | 740/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.14.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▌   | 741/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  65%|██████▌   | 742/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 743/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.15.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 744/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 745/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 746/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.16.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 747/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 748/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 749/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.17.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 750/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▌   | 751/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▋   | 752/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.18.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▋   | 753/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  66%|██████▋   | 754/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 755/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.19.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 756/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 757/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 758/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.20.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 759/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 760/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 761/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.21.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 762/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 763/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 764/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.22.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  67%|██████▋   | 765/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  68%|██████▊   | 766/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.q_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  68%|██████▊   | 767/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.23.attn.o_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  68%|██████▊   | 768/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_B.MatMul.weight_Q4 (u8, [896,1,16]):  68%|██████▊   | 769/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 770/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 771/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 772/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 773/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 774/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 775/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  68%|██████▊   | 776/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 777/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 778/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▊   | 779/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 780/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 781/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 782/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 783/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 784/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 785/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 786/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 787/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  69%|██████▉   | 788/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 789/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 790/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 791/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 792/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.down_proj.lora_A.MatMul.weight_Q4 (u8, [16,152,16]):  70%|██████▉   | 793/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.attn.q_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 794/1134 [00:00<00:00, 4725.21it/s]         \r",
      "Saving model.layers.0.attn.o_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 795/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.attn.q_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 796/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.attn.o_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 797/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.q_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 798/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.o_proj.MatMul.weight_scales (f16, [25088]):  70%|███████   | 799/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 800/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.o_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 801/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 802/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.o_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 803/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 804/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.o_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 805/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 806/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.o_proj.MatMul.weight_scales (f16, [25088]):  71%|███████   | 807/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████▏  | 808/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.o_proj.MatMul.weight_scales (f16, [25088]):  71%|███████▏  | 809/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.q_proj.MatMul.weight_scales (f16, [25088]):  71%|███████▏  | 810/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 811/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 812/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 813/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 814/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 815/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 816/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 817/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 818/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 819/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 820/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.o_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 821/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.q_proj.MatMul.weight_scales (f16, [25088]):  72%|███████▏  | 822/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 823/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.q_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 824/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 825/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.q_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 826/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 827/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.q_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 828/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 829/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.q_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 830/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 831/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.q_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 832/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.o_proj.MatMul.weight_scales (f16, [25088]):  73%|███████▎  | 833/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.q_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▎  | 834/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.o_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▎  | 835/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.q_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▎  | 836/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.o_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▍  | 837/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.q_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▍  | 838/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.o_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▍  | 839/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.q_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▍  | 840/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.o_proj.MatMul.weight_scales (f16, [25088]):  74%|███████▍  | 841/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 842/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 843/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  74%|███████▍  | 844/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 845/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 846/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 847/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 848/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 849/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▍  | 850/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 851/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 852/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 853/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 854/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 855/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  75%|███████▌  | 856/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 857/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 858/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 859/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 860/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 861/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 862/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 863/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▌  | 864/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▋  | 865/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▋  | 866/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  76%|███████▋  | 867/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 868/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 869/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 870/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 871/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 872/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 873/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 874/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 875/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 876/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 877/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  77%|███████▋  | 878/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 879/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 880/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 881/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 882/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 883/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 884/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 885/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 886/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 887/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.k_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 888/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.attn.v_proj.MatMul.weight_Q4 (u8, [128,28,16]):  78%|███████▊  | 889/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  78%|███████▊  | 890/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.0.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▊  | 891/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.1.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▊  | 892/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▊  | 893/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.2.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 894/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 895/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.3.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 896/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.3.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 897/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.4.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 898/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.4.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 899/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.5.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 900/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.5.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  79%|███████▉  | 901/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.6.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 902/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.6.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 903/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.7.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 904/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.7.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 905/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.8.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 906/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.8.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|███████▉  | 907/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.9.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|████████  | 908/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.9.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|████████  | 909/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.10.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|████████  | 910/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.10.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|████████  | 911/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.11.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  80%|████████  | 912/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.11.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 913/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.12.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 914/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.12.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 915/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.13.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 916/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.13.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 917/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.14.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 918/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.14.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 919/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.15.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 920/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.15.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████  | 921/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.16.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████▏ | 922/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.16.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████▏ | 923/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.17.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  81%|████████▏ | 924/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.17.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 925/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.18.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 926/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.18.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 927/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.19.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 928/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.19.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 929/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.20.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 930/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.20.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 931/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.21.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 932/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.21.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 933/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.22.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 934/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.22.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  82%|████████▏ | 935/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.23.mlp.gate_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  83%|████████▎ | 936/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.23.mlp.up_proj.lora_B.MatMul.weight_Q4 (u8, [4864,1,16]):  83%|████████▎ | 937/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 938/1134 [00:00<00:00, 4725.21it/s]    \r",
      "Saving model.layers.0.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 939/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 940/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 941/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.1.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 942/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.1.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 943/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 944/1134 [00:00<00:00, 4725.21it/s]\r",
      "Saving model.layers.2.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 945/1134 [00:00<00:00, 4725.21it/s]  \r",
      "Saving model.layers.2.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 946/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.2.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  83%|████████▎ | 946/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▎ | 947/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▎ | 948/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.3.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▎ | 949/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 950/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 951/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.4.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 952/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 953/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 954/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.5.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 955/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 956/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 957/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.6.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  84%|████████▍ | 958/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▍ | 959/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▍ | 960/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.7.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▍ | 961/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▍ | 962/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▍ | 963/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.8.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 964/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 965/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 966/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.9.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 967/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 968/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  85%|████████▌ | 969/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.10.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 970/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 971/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 972/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.11.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 973/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 974/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 975/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.12.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 976/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 977/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▌ | 978/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.13.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▋ | 979/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  86%|████████▋ | 980/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 981/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.14.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 982/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 983/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 984/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.15.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 985/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 986/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 987/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.16.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 988/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 989/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 990/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.17.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 991/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  87%|████████▋ | 992/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 993/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.18.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 994/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 995/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 996/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.19.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 997/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 998/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 999/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.20.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 1000/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 1001/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 1002/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.21.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  88%|████████▊ | 1003/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▊ | 1004/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▊ | 1005/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.22.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▊ | 1006/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▉ | 1007/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.mlp.up_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▉ | 1008/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.23.mlp.down_proj.MatMul.weight_scales (f16, [136192]):  89%|████████▉ | 1009/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.0.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  89%|████████▉ | 1010/1134 [00:00<00:00, 4228.16it/s]     \r",
      "Saving model.layers.0.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  89%|████████▉ | 1011/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.1.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  89%|████████▉ | 1012/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.1.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  89%|████████▉ | 1013/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.2.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  89%|████████▉ | 1014/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.2.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1015/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1016/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1017/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1018/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1019/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|████████▉ | 1020/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1021/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1022/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1023/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1024/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1025/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  90%|█████████ | 1026/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1027/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1028/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1029/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1030/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1031/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1032/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1033/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████ | 1034/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████▏| 1035/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████▏| 1036/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  91%|█████████▏| 1037/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1038/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1039/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1040/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1041/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1042/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1043/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1044/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1045/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1046/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1047/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  92%|█████████▏| 1048/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1049/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1050/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1051/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1052/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1053/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1054/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1055/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.attn.q_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1056/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.attn.o_proj.MatMul.weight_Q4 (u8, [896,28,16]):  93%|█████████▎| 1057/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving cos_cache (f16, [32768,32]):  93%|█████████▎| 1058/1134 [00:00<00:00, 4228.16it/s]                                   \r",
      "Saving sin_cache (f16, [32768,32]):  93%|█████████▎| 1059/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.0.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  93%|█████████▎| 1060/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.0.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▎| 1061/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.0.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▎| 1062/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.1.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▎| 1063/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.1.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1064/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.1.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▍| 1065/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.2.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1066/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.2.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1067/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.2.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▍| 1068/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1069/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.3.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  94%|█████████▍| 1070/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.3.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  94%|█████████▍| 1071/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1072/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.4.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1073/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.4.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▍| 1074/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1075/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.5.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▍| 1076/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.5.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▍| 1077/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1078/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.6.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1079/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.6.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  95%|█████████▌| 1080/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1081/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.7.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  95%|█████████▌| 1082/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.7.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1083/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1084/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.8.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1085/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.8.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1086/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1087/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.9.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1088/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.9.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▌| 1089/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1090/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.10.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▌| 1091/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.10.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  96%|█████████▋| 1092/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▋| 1093/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.11.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  96%|█████████▋| 1094/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.11.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1095/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1096/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.12.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1097/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.12.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1098/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1099/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.13.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1100/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.13.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1101/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1102/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.14.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1103/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.14.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  97%|█████████▋| 1104/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  97%|█████████▋| 1105/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.15.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1106/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.15.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1107/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1108/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.16.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1109/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.16.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1110/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1111/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.17.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1112/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.17.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1113/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1114/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.18.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  98%|█████████▊| 1115/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.18.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  98%|█████████▊| 1116/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▊| 1117/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.19.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▊| 1118/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.19.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▊| 1119/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1120/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.20.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1121/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.20.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1122/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1123/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.21.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1124/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.21.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1125/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1126/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.22.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]):  99%|█████████▉| 1127/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.22.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]):  99%|█████████▉| 1128/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.mlp.gate_proj.MatMul.weight_Q4 (u8, [4864,28,16]): 100%|█████████▉| 1129/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.layers.23.mlp.up_proj.MatMul.weight_Q4 (u8, [4864,28,16]): 100%|█████████▉| 1130/1134 [00:00<00:00, 4228.16it/s]  \r",
      "Saving model.layers.23.mlp.down_proj.MatMul.weight_Q4 (u8, [896,152,16]): 100%|█████████▉| 1131/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving lm_head.MatMul.weight_scales (f16, [4254208]): 100%|█████████▉| 1132/1134 [00:00<00:00, 4228.16it/s]                    \r",
      "Saving lm_head.MatMul.weight_Q4 (u8, [151936,28,16]): 100%|█████████▉| 1133/1134 [00:00<00:00, 4228.16it/s]\r",
      "Saving model.embed_tokens.weight (f16, [151936,896]): 100%|██████████| 1134/1134 [00:02<00:00, 4228.16it/s]\r",
      "Saving model.embed_tokens.weight (f16, [151936,896]): 100%|██████████| 1134/1134 [00:06<00:00, 187.65it/s] \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "MODEL_NAME=\"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "ADAPTER_DIR=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/epicrisis-lora-adapter\"\n",
    "\n",
    "OUT_CPU=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\"\n",
    "OUT_WEBGPU=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4\"\n",
    "\n",
    "python -m onnxruntime_genai.models.builder \\\n",
    "  -m \"${MODEL_NAME}\" \\\n",
    "  -o \"${OUT_CPU}\" \\\n",
    "  -p int4 \\\n",
    "  -e cpu \\\n",
    "  --extra_options hf_remote=true adapter_path=\"${ADAPTER_DIR}\"\n",
    "\n",
    "python -m onnxruntime_genai.models.builder \\\n",
    "  -m \"${MODEL_NAME}\" \\\n",
    "  -o \"${OUT_WEBGPU}\" \\\n",
    "  -p int4 \\\n",
    "  -e webgpu \\\n",
    "  --extra_options hf_remote=true adapter_path=\"${ADAPTER_DIR}\"\n",
    "\n",
    "echo \"✅ ONNX CPU: ${OUT_CPU}\"\n",
    "echo \"✅ ONNX WebGPU: ${OUT_WEBGPU}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "OUT_CPU=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\"\n",
    "OUT_WEBGPU=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-webgpu-int4\"\n",
    "\n",
    "def summarize_dir(p):\n",
    "    if not os.path.isdir(p):\n",
    "        return {\"path\": p, \"exists\": False, \"file_count\": 0, \"sample_files\": []}\n",
    "    files = sorted(os.listdir(p))\n",
    "    onnx_files = glob.glob(p + \"/**/*.onnx\", recursive=True)\n",
    "    return {\n",
    "        \"path\": p,\n",
    "        \"exists\": True,\n",
    "        \"file_count\": len(files),\n",
    "        \"sample_files\": files[:25],\n",
    "        \"onnx_files_found\": onnx_files[:10],\n",
    "    }\n",
    "\n",
    "print(\"CPU package:\", summarize_dir(OUT_CPU))\n",
    "print(\"WebGPU package:\", summarize_dir(OUT_WEBGPU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "MODEL_DIR=\"/content/drive/MyDrive/fine-tuning/epicrisis-qwen25-05b/onnx-cpu-int4\"\n",
    "prompt = \"Redacta una epicrisis breve para un paciente con neumonía adquirida en la comunidad.\"\n",
    "\n",
    "model = og.Model(MODEL_DIR)\n",
    "tok = og.Tokenizer(model)\n",
    "\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(max_length=220)\n",
    "\n",
    "gen = og.Generator(model, params)\n",
    "gen.append_tokens(tok.encode(prompt))\n",
    "\n",
    "while not gen.is_done():\n",
    "    gen.generate_next_token()\n",
    "\n",
    "print(tok.decode(gen.get_sequence(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ac419",
   "metadata": {
    "id": "e25ac419"
   },
   "source": [
    "## Prueba rápida (Python) con onnxruntime-genai (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4735d30",
   "metadata": {
    "id": "e4735d30"
   },
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "model_dir = \"./epicrisis-onnx-webgpu-int4\"  # paquete generado por builder\n",
    "# En Python, el provider real depende de tu instalación; para validar lógica puedes usar CPU si generas paquete CPU.\n",
    "# Si quieres validar en CPU, genera también con: -e cpu -o ./epicrisis-onnx-cpu-int4\n",
    "\n",
    "print(\"Archivos en model_dir:\", model_dir)\n",
    "\n",
    "# Ejemplo mínimo (puede variar según versión). Si falla, usa el ejemplo oficial de ORT GenAI para tu versión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988f900",
   "metadata": {
    "id": "3988f900"
   },
   "source": [
    "## (Opcional) Reorganizar estructura para Transformers.js\n",
    "Si vas a usar Transformers.js, normalmente esperas una carpeta `onnx/` y archivos tokenizer/config en raíz. Con ORT GenAI el paquete es distinto; para Transformers.js puede requerir adaptación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae411e6",
   "metadata": {
    "id": "cae411e6"
   },
   "outputs": [],
   "source": [
    "import os, shutil, json\n",
    "out_dir = './epicrisis-finetuned-onnx'\n",
    "onnx_dir = os.path.join(out_dir, 'onnx')\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "for f in os.listdir(out_dir):\n",
    "    if f.endswith('.onnx') or f.endswith('.onnx_data'):\n",
    "        shutil.move(os.path.join(out_dir, f), os.path.join(onnx_dir, f))\n",
    "\n",
    "cfg_path = os.path.join(out_dir, 'config.json')\n",
    "if os.path.exists(cfg_path):\n",
    "    with open(cfg_path, 'r') as f:\n",
    "        cfg = json.load(f)\n",
    "    cfg['transformers.js_config'] = {\n",
    "        'dtype': 'fp16',\n",
    "        'kv_cache_dtype': {\n",
    "            'fp16': 'float16'\n",
    "        }\n",
    "    }\n",
    "    with open(cfg_path, 'w') as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "print('Listo:', out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ee9bf",
   "metadata": {
    "id": "d26ee9bf"
   },
   "source": [
    "## (Opcional) Export oficial Transformers.js (q4f16)\n",
    "Requiere scripts de Transformers.js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4a52f",
   "metadata": {
    "id": "a2c4a52f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers.js.git -q\n",
    "%cd transformers.js\n",
    "!pip -q install -r requirements.txt\n",
    "!python3 scripts/convert.py \\\n",
    "  --model_id ../epicrisis-merged \\\n",
    "  --task text-generation-with-past \\\n",
    "  --quantize q4f16 \\\n",
    "  --output_dir ../epicrisis-finetuned-tjs\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578310e7",
   "metadata": {
    "id": "578310e7"
   },
   "source": [
    "## Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f2d7",
   "metadata": {
    "id": "2d29f2d7"
   },
   "outputs": [],
   "source": [
    "!zip -r epicrisis-onnx-webgpu-int4.zip epicrisis-onnx-webgpu-int4\n",
    "print('ZIP creado: epicrisis-onnx-webgpu-int4.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0447fd9f67c04cd5aadca1a3f06d98e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_196e59030fe1475eb99a55b67c0b2f58",
       "IPY_MODEL_c90c72559918474fab107f06ad00d1bc",
       "IPY_MODEL_a39a722390664e1fb87b9266c566e041"
      ],
      "layout": "IPY_MODEL_f0de7e74825a40cd90787ba620aeb740"
     }
    },
    "051fd6f6bb4c423eb11db9079005c7be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "082d64e50ac14d95a346299d40c1787f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9997c25c46be4404b423f23b92449351",
      "placeholder": "​",
      "style": "IPY_MODEL_d8c6821c86cb4d7caf410048e03902a0",
      "value": " 321/321 [00:00&lt;00:00, 549.81 examples/s]"
     }
    },
    "0bd71524f0bc4ba283b31d9b82418d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_459fcb162a07456db312a0d79f79a53b",
       "IPY_MODEL_a9fb1808d6ce41e78f262aeccfea8abd",
       "IPY_MODEL_9d73f8f15393447b9d77108ea4cf1ddc"
      ],
      "layout": "IPY_MODEL_d1056bdf56d74acc95c6c0c9f9023de3"
     }
    },
    "0e284328789f4381b77526bd5c131869": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "196e59030fe1475eb99a55b67c0b2f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e284328789f4381b77526bd5c131869",
      "placeholder": "​",
      "style": "IPY_MODEL_f852c4ec4b46478c9cf909d069501151",
      "value": "Map: 100%"
     }
    },
    "1e9d50e9df38443aa17f2d253ec99b7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "213365b419954c19a88b11bb27f21e98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2379f32dda2341a6b218eff4d8fd1e7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "271476620cc6459d977f0fad00290c39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_601fcc12842f4ee1850cde97b24c043a",
      "placeholder": "​",
      "style": "IPY_MODEL_4ceb21c89b1a4411853fce16a22a86f2",
      "value": "Map: 100%"
     }
    },
    "319d445016e946ef8d6eeca9c398e984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "325a3506898d41f2ad7249342daa914d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f8d41f6ca4e4880a76fd04e0f07d76e",
      "max": 321,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d32ee985f2e847f0b990b1c3647de0e2",
      "value": 321
     }
    },
    "32d96ec56a0a4a85b82d0e498df21d20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3448729e8af54acbab460dc3a8657cfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46c110ff2ce54f41b45529c17742dff7",
      "placeholder": "​",
      "style": "IPY_MODEL_1e9d50e9df38443aa17f2d253ec99b7c",
      "value": "Map: 100%"
     }
    },
    "35c392674d3d46cab5e78fb51f75edb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5387efebdff43ce916bbd6f12745de2",
      "placeholder": "​",
      "style": "IPY_MODEL_4c958169c8c44a02889f15fc546146dc",
      "value": " 36/36 [00:00&lt;00:00, 314.29 examples/s]"
     }
    },
    "35ce2b9869a7432c99b5f0f8c7ebaf9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "396caf00ae3d4cf39b543e4980fd065f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e131c16459f41da9597f547269a08c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3448729e8af54acbab460dc3a8657cfa",
       "IPY_MODEL_325a3506898d41f2ad7249342daa914d",
       "IPY_MODEL_082d64e50ac14d95a346299d40c1787f"
      ],
      "layout": "IPY_MODEL_213365b419954c19a88b11bb27f21e98"
     }
    },
    "459fcb162a07456db312a0d79f79a53b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a184c4db4d4540e19513c4d9da445ce4",
      "placeholder": "​",
      "style": "IPY_MODEL_4b54881eb3bb492e890d508a715fd6d8",
      "value": "Map: 100%"
     }
    },
    "46c110ff2ce54f41b45529c17742dff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b54881eb3bb492e890d508a715fd6d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c958169c8c44a02889f15fc546146dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ceb21c89b1a4411853fce16a22a86f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e0520f0b6514d318684310eb31a46f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e08a8fd9682426da01458d15d6e4404": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "601fcc12842f4ee1850cde97b24c043a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7be7054d664a95b704c72f5477b0f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e08a8fd9682426da01458d15d6e4404",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abe6f3f9731443f6a48dfc945fa933fb",
      "value": 0
     }
    },
    "7f2b859e6a854b0dbe61b547e4204dea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e0520f0b6514d318684310eb31a46f0",
      "placeholder": "​",
      "style": "IPY_MODEL_319d445016e946ef8d6eeca9c398e984",
      "value": ""
     }
    },
    "88c167314b864a9ebb8bfb52005692f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9997c25c46be4404b423f23b92449351": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bd3593b11464402a1416a732cb30951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d73f8f15393447b9d77108ea4cf1ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a148cc6f6def45d0bd157f20d30d3998",
      "placeholder": "​",
      "style": "IPY_MODEL_ca710803bbf34ebe98aef4404e17c792",
      "value": " 321/321 [00:00&lt;00:00, 1730.08 examples/s]"
     }
    },
    "9f8d41f6ca4e4880a76fd04e0f07d76e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a148cc6f6def45d0bd157f20d30d3998": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a184c4db4d4540e19513c4d9da445ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a39a722390664e1fb87b9266c566e041": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df010c9d61c7488b8eba24deb2d096b5",
      "placeholder": "​",
      "style": "IPY_MODEL_9bd3593b11464402a1416a732cb30951",
      "value": " 36/36 [00:00&lt;00:00, 927.78 examples/s]"
     }
    },
    "a9fb1808d6ce41e78f262aeccfea8abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_051fd6f6bb4c423eb11db9079005c7be",
      "max": 321,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35ce2b9869a7432c99b5f0f8c7ebaf9b",
      "value": 321
     }
    },
    "abe6f3f9731443f6a48dfc945fa933fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae4221453c8f4d9c939bc659680a7c87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7d8583095045c88b787881c8afc0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3975e41e2a24e4089d9058c7724ba95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_271476620cc6459d977f0fad00290c39",
       "IPY_MODEL_cb6f02cd689d4500896a637b153fb226",
       "IPY_MODEL_35c392674d3d46cab5e78fb51f75edb4"
      ],
      "layout": "IPY_MODEL_396caf00ae3d4cf39b543e4980fd065f"
     }
    },
    "b8175df4fcca4d3aa71afa2ad2ce56d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0fd60027ee34dc49f576fba87f21b22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5387efebdff43ce916bbd6f12745de2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c90c72559918474fab107f06ad00d1bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0fd60027ee34dc49f576fba87f21b22",
      "max": 36,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af7d8583095045c88b787881c8afc0b5",
      "value": 36
     }
    },
    "ca710803bbf34ebe98aef4404e17c792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb6f02cd689d4500896a637b153fb226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88c167314b864a9ebb8bfb52005692f5",
      "max": 36,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8175df4fcca4d3aa71afa2ad2ce56d0",
      "value": 36
     }
    },
    "d1056bdf56d74acc95c6c0c9f9023de3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d32ee985f2e847f0b990b1c3647de0e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8c6821c86cb4d7caf410048e03902a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df010c9d61c7488b8eba24deb2d096b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e11091eaac404b3aa586cfb9b0fc5ca7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32d96ec56a0a4a85b82d0e498df21d20",
      "placeholder": "​",
      "style": "IPY_MODEL_2379f32dda2341a6b218eff4d8fd1e7c",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "f0de7e74825a40cd90787ba620aeb740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f852c4ec4b46478c9cf909d069501151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fae2c963c89a4e5791d4b505083f55ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f2b859e6a854b0dbe61b547e4204dea",
       "IPY_MODEL_7a7be7054d664a95b704c72f5477b0f6",
       "IPY_MODEL_e11091eaac404b3aa586cfb9b0fc5ca7"
      ],
      "layout": "IPY_MODEL_ae4221453c8f4d9c939bc659680a7c87"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
