# ============================================
# CONFIGURACION DE EPICRISIS APP - FRONTEND
# ============================================
# Copie este archivo a .env y configure segun sus necesidades

# ============================================
# MODO DE GENERACION
# ============================================

# Modo inicial de generacion:
# - 'remote': Usa el backend API con LLM servidor
# - 'local': Usa modelos ONNX directamente en el navegador (default)
NG_APP_GENERATION_MODE=local

# ============================================
# CONFIGURACION DE MODELOS ONNX LOCALES
# ============================================

# Usar modelos descargados localmente (true) o desde HuggingFace (false)
NG_APP_USE_LOCAL_MODELS=true

# Ruta base donde estan los modelos ONNX descargados
# Ejecute: ./scripts/download-models.sh ministral-3b embeddings
NG_APP_LOCAL_MODELS_PATH=/assets/models

# Modelo LLM por defecto para modo local
# MODELOS LOCALES (requieren descarga con script):
# - local/Ministral-3-3B-Instruct-2512-ONNX (3.6GB, recomendado, multimodal)
# - local/Llama-3.2-1B-Instruct (1.1GB, rapido)
# - local/SmolLM2-360M-Instruct (200MB, ultra rapido)
#
# MODELOS REMOTOS (descarga automatica desde HuggingFace):
# - onnx-community/Llama-3.2-1B-Instruct (1.1GB)
# - onnx-community/Phi-3.5-mini-instruct-onnx-web (2.2GB, Microsoft)
# - onnx-community/Ministral-3B-Instruct-2412-ONNX (2.4GB)
# - onnx-community/granite-3.0-2b-instruct (1.6GB, IBM)
# - HuggingFaceTB/SmolLM2-360M-Instruct (200MB)
NG_APP_DEFAULT_MODEL=local/Ministral-3-3B-Instruct-2512-ONNX

# Cargar modelos automaticamente al iniciar (true/false)
# Solo tiene efecto si NG_APP_GENERATION_MODE=local
NG_APP_AUTO_LOAD_MODELS=false

# ============================================
# CONFIGURACION DE BACKEND API (modo remoto)
# ============================================

# URL del backend API
NG_APP_API_URL=http://localhost:3000/api

# ============================================
# CONFIGURACION DE GENERACION
# ============================================

# Configuracion de generacion activa:
# - resumen_alta: Optimizado para epicrisis (temp=0.1, tokens=600)
# - rag: Para preguntas RAG (temp=0.2, tokens=512)
# - extraction: Para extraccion precisa (temp=0.1, tokens=256)
NG_APP_GENERATION_CONFIG=resumen_alta

# Maximo de tokens a generar (override)
NG_APP_MAX_NEW_TOKENS=600

# Temperatura (0.0 = determinista, 1.0 = muy creativo)
NG_APP_TEMPERATURE=0.1

# ============================================
# WEBGPU / WASM
# ============================================

# Preferir WebGPU sobre WASM cuando este disponible (true/false)
# WebGPU es mas rapido pero requiere navegador compatible
NG_APP_PREFER_WEBGPU=true

# ============================================
# DEBUG
# ============================================

# Mostrar logs de debug en consola (true/false)
NG_APP_DEBUG_MODE=true

# ============================================
# INSTRUCCIONES DE CONFIGURACION
# ============================================
#
# Para usar modelos LOCALES (recomendado para privacidad):
#
# 1. Descargue los modelos:
#    cd epicrisis-app
#    ./scripts/download-models.sh ministral-3b embeddings
#
# 2. Cree symlink para desarrollo:
#    cd frontend/src/assets
#    ln -s ../../../models models
#
# 3. Configure:
#    NG_APP_USE_LOCAL_MODELS=true
#    NG_APP_LOCAL_MODELS_PATH=/assets/models
#    NG_APP_DEFAULT_MODEL=local/Ministral-3-3B-Instruct-2512-ONNX
#
# Para usar modelos REMOTOS (HuggingFace CDN):
#
# 1. Configure:
#    NG_APP_USE_LOCAL_MODELS=false
#    NG_APP_DEFAULT_MODEL=onnx-community/Llama-3.2-1B-Instruct
#
# Los modelos remotos se descargan automaticamente y se cachean
# en el navegador (IndexedDB/Cache API).
