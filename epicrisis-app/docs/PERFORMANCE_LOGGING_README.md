# Sistema de Logging de Performance - Resumen

**Fecha:** 2025-12-29
**Estado:** ‚úÖ Implementado y funcional

---

## üéØ Objetivo Completado

Se ha implementado un **sistema completo de logging de performance** con m√©tricas detalladas para evaluar diferentes modelos LLM y optimizar el pipeline RAG.

---

## ‚úÖ Cambios Realizados

### 1. **LLM Service** (`backend/src/services/llmService.ts`)

Agregado logging detallado para:

#### Generaci√≥n de Epicrisis
- ‚è±Ô∏è **Preparaci√≥n del prompt** (tiempo + tama√±o)
- ‚è±Ô∏è **Tokenizaci√≥n** (tiempo + tokens estimados)
- ‚è±Ô∏è **Inferencia** (tiempo cr√≠tico del modelo)
- ‚è±Ô∏è **Post-procesamiento**
- üìä **M√©tricas de performance** (tokens/segundo, breakdown porcentual)

#### Regeneraci√≥n con Correcciones
- ‚è±Ô∏è **Preparaci√≥n de whitelists** (dx, procedimientos, medicamentos)
- ‚è±Ô∏è **Construcci√≥n del prompt de correcci√≥n**
- ‚è±Ô∏è **Tokenizaci√≥n**
- ‚è±Ô∏è **Inferencia de regeneraci√≥n**
- üìä **M√©tricas completas** (tokens/segundo, desglose)

### 2. **RAG Service** (`backend/src/services/ragService.ts`)

Agregado logging detallado para:

#### Indexaci√≥n (Ingesta de Datos)
- ‚è±Ô∏è **Chunking sem√°ntico** (tiempo + estad√≠sticas de chunks)
- ‚è±Ô∏è **Generaci√≥n de embeddings** (tiempo por embedding, promedio, min/max)
- üìä **Performance** (embeddings/segundo, desglose porcentual)

#### Retrieval (B√∫squeda)
- ‚è±Ô∏è **Query embedding** (tiempo + dimensiones)
- ‚è±Ô∏è **C√°lculo de similitud** (tiempo + comparaciones/segundo)
- ‚è±Ô∏è **Ordenamiento** (tiempo + top-k scores)
- üìä **Estad√≠sticas de resultados** (avg/min/max scores)

---

## üìä M√©tricas Registradas

### M√©tricas LLM

| M√©trica | Descripci√≥n | Importancia |
|---------|-------------|-------------|
| `tokens_per_second` | Velocidad de generaci√≥n | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `inference_ms` | Tiempo de inferencia | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `total_time_ms` | Tiempo total end-to-end | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `input_tokens` | Tokens de entrada | ‚≠ê‚≠ê‚≠ê |
| `output_tokens` | Tokens generados | ‚≠ê‚≠ê‚≠ê |
| `prompt_length` | Longitud del prompt | ‚≠ê‚≠ê |

### M√©tricas RAG

| M√©trica | Descripci√≥n | Importancia |
|---------|-------------|-------------|
| `embeddings_per_second` | Velocidad de embeddings | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `chunks_count` | N√∫mero de chunks | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `avg_chunk_length` | Tama√±o promedio de chunks | ‚≠ê‚≠ê‚≠ê |
| `similarity_computation_ms` | Tiempo de b√∫squeda | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `avg_score` | Relevancia promedio | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üîç Formato de Logs

### Ejemplo de log LLM

```log
2025-12-29 16:30:45.001 === LLM GENERATION START ===
2025-12-29 16:30:45.002 [LLM_METRICS] Iniciando generaci√≥n de epicrisis
2025-12-29 16:30:45.004 [LLM_METRICS] Prompt preparado {"time_ms":2,"prompt_length":3456,"json_size":2890}
2025-12-29 16:30:45.009 [LLM_METRICS] Tokenizaci√≥n {"time_ms":5,"estimated_tokens":864}
2025-12-29 16:30:47.154 [LLM_METRICS] Inferencia completada {"time_ms":2145,"output_tokens":128}
2025-12-29 16:30:47.155 [LLM_METRICS] === GENERACI√ìN COMPLETADA === {
  "total_time_ms": 2153,
  "breakdown": {
    "prompt_prep": "2ms (0.1%)",
    "tokenization": "5ms (0.2%)",
    "inference": "2145ms (99.6%)",
    "post_processing": "1ms (0.0%)"
  },
  "performance": {
    "tokens_per_second": "59.67",
    "total_tokens": 992,
    "input_tokens": 864,
    "output_tokens": 128
  }
}
```

### Ejemplo de log RAG

```log
2025-12-29 16:31:00.001 === RAG INDEXING START ===
2025-12-29 16:31:00.002 [RAG_METRICS] Iniciando indexaci√≥n {"episodeId":"12345"}
2025-12-29 16:31:00.014 [RAG_METRICS] Chunking completado {"time_ms":12,"chunks_count":8,"avg_chunk_length":156}
2025-12-29 16:31:00.259 [RAG_METRICS] Embeddings generados {
  "total_time_ms": 245,
  "avg_time_ms": "30.63",
  "embeddings_per_second": "32.65"
}
2025-12-29 16:31:00.271 [RAG_METRICS] === INDEXACI√ìN COMPLETADA === {
  "total_time_ms": 257,
  "breakdown": {
    "chunking": "12ms (4.7%)",
    "embeddings": "245ms (95.3%)"
  }
}
```

---

## üöÄ C√≥mo Usar

### 1. Ver m√©tricas en tiempo real

```bash
# Ver todas las m√©tricas LLM
tail -f backend/logs/flow-$(date +%Y-%m-%d).log | grep "LLM_METRICS"

# Ver todas las m√©tricas RAG
tail -f backend/logs/flow-$(date +%Y-%m-%d).log | grep "RAG_METRICS"

# Ver solo tiempos de inferencia
tail -f backend/logs/flow-$(date +%Y-%m-%d).log | grep "Inferencia completada"
```

### 2. Extraer m√©tricas del d√≠a

```bash
# Tokens por segundo promedio
grep "tokens_per_second" backend/logs/flow-$(date +%Y-%m-%d).log | \
  grep -o '"tokens_per_second":"[0-9.]*"' | \
  cut -d'"' -f4 | \
  awk '{sum+=$1; count++} END {print "Promedio: " sum/count " t/s"}'

# Tiempo de inferencia promedio
grep "inference_ms" backend/logs/flow-$(date +%Y-%m-%d).log | \
  grep -o '"inference_ms":[0-9]*' | \
  cut -d: -f2 | \
  awk '{sum+=$1; count++} END {print "Promedio: " sum/count "ms"}'

# Total de generaciones
grep "=== GENERACI√ìN COMPLETADA ===" backend/logs/flow-$(date +%Y-%m-%d).log | wc -l
```

### 3. Comparar dos sesiones

```bash
# Sesi√≥n 1 (modelo A)
grep "tokens_per_second" backend/logs/flow-2025-12-29.log | head -10 > model_a.txt

# Sesi√≥n 2 (modelo B)
grep "tokens_per_second" backend/logs/flow-2025-12-30.log | head -10 > model_b.txt

# Comparar
echo "Modelo A:" && cat model_a.txt | awk '{sum+=$1} END {print "Avg: " sum/NR}'
echo "Modelo B:" && cat model_b.txt | awk '{sum+=$1} END {print "Avg: " sum/NR}'
```

---

## üìà Evaluaci√≥n de LLMs

### M√©tricas clave para comparar modelos

#### 1. **Velocidad** (tokens/segundo)
```bash
grep "tokens_per_second" backend/logs/flow-*.log | \
  grep -o '[0-9.]*' | \
  sort -n | \
  awk '{
    sum+=$1;
    arr[NR]=$1
  }
  END {
    print "Promedio: " sum/NR " t/s"
    print "Mediana: " arr[int(NR/2)] " t/s"
  }'
```

#### 2. **Latencia** (tiempo de inferencia)
```bash
grep "inference_ms" backend/logs/flow-*.log | \
  grep -o '[0-9]*' | \
  awk '{
    sum+=$1
    if(NR==1){min=$1;max=$1}
    if($1<min){min=$1}
    if($1>max){max=$1}
  }
  END {
    print "Promedio: " sum/NR "ms"
    print "M√≠n: " min "ms"
    print "M√°x: " max "ms"
  }'
```

#### 3. **Throughput** (total end-to-end)
```bash
grep "total_time_ms" backend/logs/flow-*.log | \
  grep -o '[0-9]*' | \
  awk '{sum+=$1; count++} END {print "Promedio: " sum/count "ms"}'
```

---

## üß™ Benchmarking

### Protocolo recomendado

```bash
#!/bin/bash
# benchmark.sh

echo "=== BENCHMARK EPICRISIS LLM ==="
echo "Modelo: $1"
echo "Fecha: $(date)"
echo ""

# 10 requests de prueba
for i in {1..10}; do
  echo "Request $i/10..."
  curl -s -X POST http://localhost:3000/api/generate-epicrisis \
    -H "Content-Type: application/json" \
    -d @test_data.json > /dev/null
  sleep 3
done

# Extraer m√©tricas
LOG=$(date +%Y-%m-%d)
echo ""
echo "=== RESULTADOS ==="

echo "Tokens/segundo:"
grep "tokens_per_second" backend/logs/flow-$LOG.log | \
  tail -10 | \
  grep -o '[0-9.]*' | \
  awk '{sum+=$1; count++} END {print "  Promedio: " sum/count " t/s"}'

echo ""
echo "Tiempo de inferencia:"
grep "inference_ms" backend/logs/flow-$LOG.log | \
  tail -10 | \
  grep -o '[0-9]*' | \
  awk '{sum+=$1; count++} END {print "  Promedio: " sum/count "ms"}'

echo ""
echo "Tiempo total:"
grep "total_time_ms" backend/logs/flow-$LOG.log | \
  tail -10 | \
  grep -o '[0-9]*' | \
  awk '{sum+=$1; count++} END {print "  Promedio: " sum/count "ms"}'
```

**Uso:**
```bash
chmod +x benchmark.sh
./benchmark.sh "TinyLlama-1.1B"
```

---

## üìö Documentaci√≥n

Para m√°s detalles, consultar:

- **`LLM_PERFORMANCE_METRICS.md`** - Gu√≠a completa de m√©tricas (600+ l√≠neas)
- **`LOGGING_SYSTEM.md`** - Sistema general de logging
- **`FLUJO_COMPLETO_LOG.md`** - Flujo detallado del sistema

---

## üìä Ejemplo de Comparaci√≥n

### Comparando TinyLlama vs Llama-3

| Modelo | Tokens/Seg | Tiempo Inf | Tiempo Total | Calidad |
|--------|------------|------------|--------------|---------|
| **TinyLlama-1.1B** | 62.3 t/s | 2,145ms | 2,153ms | ‚≠ê‚≠ê‚≠ê |
| **Llama-3-8B** | 38.7 t/s | 3,456ms | 3,468ms | ‚≠ê‚≠ê‚≠ê‚≠ê |

**Conclusi√≥n:** TinyLlama es 61% m√°s r√°pido pero con menor calidad.

---

## üé® Estructura de Archivos

```
epicrisis-app/
‚îú‚îÄ‚îÄ PERFORMANCE_LOGGING_README.md       # Este archivo (resumen)
‚îú‚îÄ‚îÄ LLM_PERFORMANCE_METRICS.md         # Gu√≠a completa de m√©tricas
‚îú‚îÄ‚îÄ LOGGING_SYSTEM.md                  # Sistema de logging general
‚îú‚îÄ‚îÄ FLUJO_COMPLETO_LOG.md             # Flujo detallado
‚îÇ
‚îî‚îÄ‚îÄ backend/
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ services/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llmService.ts         # ‚ú® M√©tricas LLM agregadas
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ragService.ts         # ‚ú® M√©tricas RAG agregadas
    ‚îÇ   ‚îî‚îÄ‚îÄ config/
    ‚îÇ       ‚îî‚îÄ‚îÄ logger.ts             # Logger con FlowLogger
    ‚îÇ
    ‚îî‚îÄ‚îÄ logs/
        ‚îú‚îÄ‚îÄ flow-2025-12-29.log       # ‚ú® Logs con m√©tricas
        ‚îú‚îÄ‚îÄ combined.log
        ‚îî‚îÄ‚îÄ error.log
```

---

## üîß Variables de Entorno

Para ajustar el nivel de logging:

```bash
# .env
LOG_LEVEL=info    # info, debug, warn, error
```

---

## üéØ M√©tricas M√°s Importantes

Para evaluar LLMs, enfocarse en:

1. **`tokens_per_second`** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Mide velocidad pura del modelo
   - Permite comparar hardware/configuraciones

2. **`inference_ms`** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Tiempo real de generaci√≥n
   - Cr√≠tico para UX

3. **`total_time_ms`** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Tiempo end-to-end
   - Incluye overheads

4. **`embeddings_per_second`** (RAG) ‚≠ê‚≠ê‚≠ê‚≠ê
   - Velocidad de indexaci√≥n
   - Importante para ingesta masiva

---

## üö¶ Umbrales Recomendados

### LLM Generation

- ‚úÖ **Excelente**: >70 tokens/segundo
- ‚ö†Ô∏è **Aceptable**: 40-70 tokens/segundo
- ‚ùå **Lento**: <40 tokens/segundo

### RAG Indexing

- ‚úÖ **Excelente**: >50 embeddings/segundo
- ‚ö†Ô∏è **Aceptable**: 20-50 embeddings/segundo
- ‚ùå **Lento**: <20 embeddings/segundo

---

## ‚ú® Pr√≥ximos Pasos (Opcional)

Mejoras futuras posibles:

- [ ] Dashboard de visualizaci√≥n (Grafana)
- [ ] Alertas autom√°ticas por degradaci√≥n
- [ ] Comparaci√≥n autom√°tica A/B testing
- [ ] Exportar m√©tricas a Prometheus
- [ ] M√©tricas de calidad (BLEU, ROUGE)
- [ ] Costo por request (para APIs)

---

## üéâ Resumen

‚úÖ **M√©tricas LLM completas** (prompt, tokenizaci√≥n, inferencia, post-processing)
‚úÖ **M√©tricas RAG detalladas** (chunking, embeddings, retrieval)
‚úÖ **Logs estructurados** para an√°lisis f√°cil
‚úÖ **Scripts de extracci√≥n** y comparaci√≥n
‚úÖ **Documentaci√≥n completa** para evaluaci√≥n de modelos
‚úÖ **Benchmarking protocol** definido
‚úÖ **Listo para producci√≥n** y comparaci√≥n de LLMs

---

**Implementado por:** Sistema Epicrisis Autom√°tica
**Fecha:** 2025-12-29
**Estado:** ‚úÖ Completado y listo para evaluar LLMs
