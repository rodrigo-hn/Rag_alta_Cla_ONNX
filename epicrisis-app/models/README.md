# Modelos Locales para Epicrisis Autom√°tica

Este directorio contiene los modelos de Machine Learning necesarios para ejecutar el sistema de forma 100% local.

## üìÅ Estructura

```
models/
‚îú‚îÄ‚îÄ llm/                      # Modelos de lenguaje (LLM)
‚îÇ   ‚îî‚îÄ‚îÄ tinyllama-1.1b-chat-q4/
‚îÇ       ‚îî‚îÄ‚îÄ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
‚îî‚îÄ‚îÄ embeddings/               # Modelos de embeddings
    ‚îî‚îÄ‚îÄ multilingual-e5-small/
        ‚îú‚îÄ‚îÄ config.json
        ‚îú‚îÄ‚îÄ pytorch_model.bin
        ‚îî‚îÄ‚îÄ tokenizer files...
```

## üöÄ Descarga R√°pida

### Opci√≥n 1: Script Python (Recomendado)

```bash
# Instalar dependencias
pip install tqdm

# Descargar todo (LLM + Embeddings)
python download_models.py --all

# Solo LLM
python download_models.py --llm

# Solo Embeddings
python download_models.py --embeddings

# Modelo alternativo m√°s potente
python download_models.py --alternative-llm mistral-7b
```

### Opci√≥n 2: Script Bash

```bash
# Ejecutar script interactivo
./download-models.sh
```

### Opci√≥n 3: Manual

#### TinyLlama 1.1B Chat (637 MB)

```bash
cd models/llm/tinyllama-1.1b-chat-q4
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

#### Multilingual E5 Small Embeddings (118 MB)

```bash
cd models/embeddings
git lfs install
git clone https://huggingface.co/intfloat/multilingual-e5-small
```

O descarga manual:
```bash
cd models/embeddings/multilingual-e5-small
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/config.json
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/pytorch_model.bin
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/tokenizer.json
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/tokenizer_config.json
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/special_tokens_map.json
wget https://huggingface.co/intfloat/multilingual-e5-small/resolve/main/vocab.txt
```

## ü§ñ Modelos Disponibles

### LLM (Modelos de Lenguaje)

| Modelo | Tama√±o | Cuantizaci√≥n | Calidad | Velocidad | Recomendado Para |
|--------|--------|--------------|---------|-----------|------------------|
| **TinyLlama 1.1B** | 637 MB | Q4_K_M | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Pruebas, desarrollo |
| **Llama 3.2 3B** | 1.9 GB | Q4_K_M | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Balance ideal |
| **Mistral 7B** | 4.1 GB | Q4_K_M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | Producci√≥n |
| **Llama 3.1 8B** | 4.7 GB | Q4_K_M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | M√°xima calidad |

### Embeddings

| Modelo | Tama√±o | Dimensiones | Idiomas | Rendimiento |
|--------|--------|-------------|---------|-------------|
| **E5 Small** | 118 MB | 384 | 100+ | ‚ö°‚ö°‚ö°‚ö° |
| **BGE-M3** | 2.2 GB | 1024 | 100+ | ‚ö°‚ö°‚ö° |
| **E5 Large** | 1.3 GB | 1024 | 100+ | ‚ö°‚ö° |

## üìù Configuraci√≥n

Despu√©s de descargar los modelos, configura `backend/.env`:

```bash
# Tipo de modelo
MODEL_TYPE=local

# Ruta al modelo LLM
LOCAL_LLM_PATH=./models/llm/tinyllama-1.1b-chat-q4/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Ruta a embeddings
EMBEDDINGS_MODEL=./models/embeddings/multilingual-e5-small
EMBEDDINGS_TYPE=local

# Configuraci√≥n de inferencia
MAX_TOKENS=2048
TEMPERATURE=0.3
TOP_P=0.9
```

## üîÑ Modelos Alternativos

### Para mejor calidad en espa√±ol m√©dico

#### Mistral 7B Instruct (Recomendado para Producci√≥n)

```bash
python download_models.py --alternative-llm mistral-7b
```

Actualiza `.env`:
```bash
LOCAL_LLM_PATH=./models/llm/mistral-7b-instruct-q4/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

#### Llama 3.2 3B (Balance entre tama√±o y calidad)

```bash
python download_models.py --alternative-llm llama-3.2-3b
```

Actualiza `.env`:
```bash
LOCAL_LLM_PATH=./models/llm/llama-3.2-3b-instruct-q4/Llama-3.2-3B-Instruct-Q4_K_M.gguf
```

### BGE-M3 Embeddings (Mejor para espa√±ol)

```bash
cd models/embeddings
git lfs install
git clone https://huggingface.co/BAAI/bge-m3
```

Actualiza `.env`:
```bash
EMBEDDINGS_MODEL=./models/embeddings/bge-m3
```

## ‚öôÔ∏è Requisitos del Sistema

### M√≠nimos (TinyLlama + E5 Small)
- **RAM**: 4 GB
- **Disco**: 1 GB libre
- **CPU**: x86_64 o ARM64

### Recomendados (Mistral 7B + E5 Small)
- **RAM**: 8 GB
- **Disco**: 5 GB libre
- **CPU**: 4+ cores
- **GPU** (opcional): CUDA compatible para aceleraci√≥n

### Producci√≥n (Mistral 7B + BGE-M3)
- **RAM**: 16 GB
- **Disco**: 10 GB libre
- **GPU**: NVIDIA con 8+ GB VRAM (recomendado)

## üîç Cuantizaci√≥n Explicada

Las cuantizaciones reducen el tama√±o del modelo con m√≠nima p√©rdida de calidad:

- **Q4_K_M**: 4-bit, balance entre calidad/tama√±o (recomendado)
- **Q5_K_M**: 5-bit, mejor calidad, m√°s grande
- **Q8_0**: 8-bit, casi sin p√©rdida, muy grande
- **F16**: 16-bit, sin cuantizaci√≥n, tama√±o completo

## üåê Uso de APIs Externas (Alternativa)

Si no quieres usar modelos locales, configura APIs externas en `.env`:

```bash
# OpenAI
MODEL_TYPE=openai
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4-turbo-preview

# O Anthropic Claude
MODEL_TYPE=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-opus-20240229
```

## üìä Comparaci√≥n: Local vs API

| Aspecto | Modelos Locales | APIs Externas |
|---------|-----------------|---------------|
| **Privacidad** | ‚úÖ 100% local | ‚ùå Datos salen del servidor |
| **Costo** | ‚úÖ Gratis | üí∞ Por token |
| **Velocidad** | ‚ö° Depende del hardware | üåê Depende de internet |
| **Calidad** | üìà Variable seg√∫n modelo | ‚≠ê Alta (GPT-4, Claude) |
| **Setup** | üîß Requiere descarga | ‚úÖ Solo API key |
| **Escalabilidad** | üíª Limitado por hardware | ‚òÅÔ∏è Ilimitado |

## üõ†Ô∏è Troubleshooting

### Error: "No se puede cargar el modelo"

```bash
# Verificar que el archivo existe
ls -lh models/llm/tinyllama-1.1b-chat-q4/*.gguf

# Verificar permisos
chmod 644 models/llm/tinyllama-1.1b-chat-q4/*.gguf
```

### Error: "Out of memory"

El modelo es muy grande para tu RAM. Opciones:

1. Usar un modelo m√°s peque√±o (TinyLlama)
2. Usar cuantizaci√≥n m√°s agresiva (Q4 en lugar de Q5)
3. Aumentar swap/memoria virtual
4. Usar API externa

### Error de descarga: "Connection timeout"

```bash
# Usar wget con reintentos
wget -c --tries=5 https://huggingface.co/...

# O usar aria2 para descargas m√°s r√°pidas
aria2c -x 16 https://huggingface.co/...
```

## üìö Recursos

- [Hugging Face Model Hub](https://huggingface.co/models)
- [GGUF Format Documentation](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Motor de inferencia
- [Sentence Transformers](https://www.sbert.net/) - Embeddings

## üîê Nota de Privacidad

Los modelos locales garantizan que **NING√öN dato cl√≠nico sale del servidor local**. Esto es cr√≠tico para cumplir con:

- ‚úÖ HIPAA (Health Insurance Portability and Accountability Act)
- ‚úÖ Ley chilena de protecci√≥n de datos m√©dicos
- ‚úÖ Pol√≠ticas internas de privacidad hospitalaria

---

**¬øNecesitas ayuda?** Abre un issue en el repositorio o consulta la documentaci√≥n completa.
